{
  "success": true,
  "processedCount": 66,
  "timestamp": "2026-01-25T20:22:34.787Z",
  "questions": [
    {
      "file": "Screenshot 2026-01-17 124709.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer is launching a Lambda function that requires access to a MySQL RDS instance that is in a private subnet. Which of the following is\nthe MOST secure way to achieve this?\nMove your RDS instance to a public subnet.\nConfigure the Lambda function to connect to your VPC.\n@® Ensure that the Lambda function has proper IAM permission to access RDS.\nExpose an endpoint of your RDS to the Internet using an Elastic IP.\nIncorrect\nYou can configure a Lambda function to connect to a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC)\nto create a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access\nprivate resources during execution.\nAWS Lambda runs your function code securely within a VPC by default. However, to enable your Lambda function to access resources inside your\nprivate VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS\nLambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within\nyour private VPC.\n1 The following diagram guides you through a decision tree as to whether you should use a VPC (Virtual Private Cloud):",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer is launching a Lambda function that requires access to a MySQL RDS instance that is in a private subnet. Which of the following is",
          "c: ure way to achieve this?",
          "D: S instance to a public subnet.",
          "C: onfigure the Lambda function to connect to your VPC.",
          "a: t the Lambda function has proper IAM permission to access RDS.",
          "a: n endpoint of your RDS to the Internet using an Elastic IP.",
          "c: orrect",
          "c: an configure a Lambda function to connect to a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC)",
          "c: reate a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access",
          "a: te resources during execution.",
          "A: WS Lambda runs your function code securely within a VPC by default. However, to enable your Lambda function to access resources inside your",
          "a: te VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS",
          "a: mbda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within",
          "a: te VPC.",
          "d: iagram guides you through a decision tree as to whether you should use a VPC (Virtual Private Cloud):"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "34. QUESTION\nCategory: CDA - Development with AWS Services\nA developer is launching a Lambda function that requires access to a MySQL RDS instance that is in a private subnet. Which of the following is\nthe MOST secure way to achieve this?\nMove your RDS instance to a public subnet.\nConfigure the Lambda function to connect to your VPC.\n@® Ensure that the Lambda function has proper IAM permission to access RDS.\nExpose an endpoint of your RDS to the Internet using an Elastic IP.\nIncorrect\nYou can configure a Lambda function to connect to a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon VPC)\nto create a private network for resources such as databases, cache instances, or internal services. Connect your function to the VPC to access\nprivate resources during execution.\nAWS Lambda runs your function code securely within a VPC by default. However, to enable your Lambda function to access resources inside your\nprivate VPC, you must provide additional VPC-specific configuration information that includes VPC subnet IDs and security group IDs. AWS\nLambda uses this information to set up elastic network interfaces (ENIs) that enable your function to connect securely to other resources within\nyour private VPC.\n1 The following diagram guides you through a decision tree as to whether you should use a VPC (Virtual Private Cloud):"
      },
      "tags": {
        "services": [
          "Lambda",
          "RDS",
          "VPC",
          "IAM",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "security",
          "VPC",
          "subnet",
          "security group"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 124733.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nYou are developing an online game where the app preferences and game state of the player must be synchronized across devices. It should\nalso allow multiple users to synchronize and collaborate shared data in real time.\nWhich of the following is the MOST appropriate solution that you should implement in this scenario?\nIntegrate AWS AppSync to your mobile app.\n® Integrate Amazon Cognito Sync to your mobile app.\nIntegrate Amazon Pinpoint to your mobile app.\nIntegrate AWS Amplify to your mobile app.\nIncorrect\nAWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one\nor more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need.\nWith AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources such as NoSQL data\nstores, relational databases, HTTP APIs, and your custom data sources with AWS Lambda. For mobile and web apps, AppSync additionally\nprovides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online.\n& Amazon\nEnterprise apps | (A DynamoDB (>)\n1 ) EN GraphQL Schema 3 Amazon Aurora «sl ]",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "a: re developing an online game where the app preferences and game state of the player must be synchronized across devices. It should",
          "a: lso allow multiple users to synchronize and collaborate shared data in real time.",
          "c: h of the following is the MOST appropriate solution that you should implement in this scenario?",
          "a: te AWS AppSync to your mobile app.",
          "a: te Amazon Cognito Sync to your mobile app.",
          "a: te Amazon Pinpoint to your mobile app.",
          "a: te AWS Amplify to your mobile app.",
          "c: orrect",
          "A: WS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one",
          "d: ata sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need.",
          "A: ppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources such as NoSQL data",
          "a: tional databases, HTTP APIs, and your custom data sources with AWS Lambda. For mobile and web apps, AppSync additionally",
          "d: es local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online.",
          "A: mazon",
          "a: pps | (A DynamoDB (>)",
          "a: phQL Schema 3 Amazon Aurora «sl ]"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "35. QUESTION\nCategory: CDA - Development with AWS Services\nYou are developing an online game where the app preferences and game state of the player must be synchronized across devices. It should\nalso allow multiple users to synchronize and collaborate shared data in real time.\nWhich of the following is the MOST appropriate solution that you should implement in this scenario?\nIntegrate AWS AppSync to your mobile app.\n® Integrate Amazon Cognito Sync to your mobile app.\nIntegrate Amazon Pinpoint to your mobile app.\nIntegrate AWS Amplify to your mobile app.\nIncorrect\nAWS AppSync simplifies application development by letting you create a flexible API to securely access, manipulate, and combine data from one\nor more data sources. AppSync is a managed service that uses GraphQL to make it easy for applications to get exactly the data they need.\nWith AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources such as NoSQL data\nstores, relational databases, HTTP APIs, and your custom data sources with AWS Lambda. For mobile and web apps, AppSync additionally\nprovides local data access when devices go offline, and data synchronization with customizable conflict resolution, when they are back online.\n& Amazon\nEnterprise apps | (A DynamoDB (>)\n1 ) EN GraphQL Schema 3 Amazon Aurora «sl ]"
      },
      "tags": {
        "services": [
          "Lambda",
          "DynamoDB",
          "Aurora",
          "Cognito",
          "AppSync",
          "Amplify"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 124806.png",
      "parsed": {
        "question": "|\nCategory: CDA - Troubleshooting and Optimization\nA developer is managing a real-time fraud detection system that ingests a stream of data using Amazon Kinesis. The system works well with\nmillisecond end-to-end latency, but the allocated shards are way underutilized based on the performance data in CloudWatch.\n\nWhich of the following is the MOST suitable solution to reduce the cost and capacity of the stream?\nMerge cold shards\nSplit cold shards\n® Merge hot shards\nSplit hot shards\nIncorrect\nThe purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards\nto increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream.\nOne approach to resharding could be to split every shard in the stream—which would double the stream’s capacity. However, this might provide\nmore additional capacity than you actually need and therefore create unnecessary costs.\n(ITT\nEC2\nInstance\nti\nr— N Jy",
        "options": [
          "D: O",
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: developer is managing a real-time fraud detection system that ingests a stream of data using Amazon Kinesis. The system works well with",
          "c: ond end-to-end latency, but the allocated shards are way underutilized based on the performance data in CloudWatch.",
          "c: h of the following is the MOST suitable solution to reduce the cost and capacity of the stream?",
          "c: old shards",
          "c: old shards",
          "a: rds",
          "a: rds",
          "c: orrect",
          "a: rding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards",
          "c: rease the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream.",
          "a: pproach to resharding could be to split every shard in the stream—which would double the stream’s capacity. However, this might provide",
          "a: dditional capacity than you actually need and therefore create unnecessary costs.",
          "C: 2",
          "a: nce"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "GEDO\n36. QUESTION |\nCategory: CDA - Troubleshooting and Optimization\nA developer is managing a real-time fraud detection system that ingests a stream of data using Amazon Kinesis. The system works well with\nmillisecond end-to-end latency, but the allocated shards are way underutilized based on the performance data in CloudWatch.\n\nWhich of the following is the MOST suitable solution to reduce the cost and capacity of the stream?\nMerge cold shards\nSplit cold shards\n® Merge hot shards\nSplit hot shards\nIncorrect\nThe purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards\nto increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream.\nOne approach to resharding could be to split every shard in the stream—which would double the stream’s capacity. However, this might provide\nmore additional capacity than you actually need and therefore create unnecessary costs.\n(ITT\nEC2\nInstance\nti\nr— N Jy"
      },
      "tags": {
        "services": [
          "EC2",
          "RDS",
          "Kinesis",
          "CloudWatch"
        ],
        "domains": [
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "performance"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 124912.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\n\nYou are a developer for a global technology company, which heavily uses AWS with regional offices in San Francisco, Manila, and Bangalore. [\n\nMost of the clients of your company are using serverless computing in which you are responsible for ensuring that their applications are\n\nworking efficiently.\n\nWhich of the following options are valid considerations in improving the performance of your Lambda function? (Select TWO.)\nLambda automatically creates Elastic IP's that enable your function to connect securely to other resources within your private\nVPC.\nAn increase in memory size triggers an equivalent increase in CPU available to your function.\nYou can throttle all incoming executions and stop processing any invocations to your function by setting concurrency to\n\nfalse .\nYou have to install the X-Ray daemon in Lambda to enable active tracing.\nThe concurrent execution limit is enforced against the sum of the concurrent executions of all function.\nIncorrect\nYou can use the AWS Lambda API or console to configure settings on your Lambda functions. Basic function settings include the description, role,\nand runtime that you specify when you create a function in the Lambda console. You can configure more settings after you create a function, or\nuse the API to set things like the handler name, memory allocation, and security groups during creation.\nLambda counts a request each time it starts executing in response to an event notification or invocation call, including test invokes from the\nconsole. You are charged based on the total number of requests processed across all of your Lambda functions.\n1",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "a: re a developer for a global technology company, which heavily uses AWS with regional offices in San Francisco, Manila, and Bangalore. [",
          "c: lients of your company are using serverless computing in which you are responsible for ensuring that their applications are",
          "c: iently.",
          "c: h of the following options are valid considerations in improving the performance of your Lambda function? (Select TWO.)",
          "a: mbda automatically creates Elastic IP's that enable your function to connect securely to other resources within your private",
          "C: An increase in memory size triggers an equivalent increase in CPU available to your function.",
          "c: an throttle all incoming executions and stop processing any invocations to your function by setting concurrency to",
          "a: lse .",
          "a: ve to install the X-Ray daemon in Lambda to enable active tracing.",
          "c: oncurrent execution limit is enforced against the sum of the concurrent executions of all function.",
          "c: orrect",
          "c: an use the AWS Lambda API or console to configure settings on your Lambda functions. Basic function settings include the description, role,",
          "a: nd runtime that you specify when you create a function in the Lambda console. You can configure more settings after you create a function, or",
          "A: PI to set things like the handler name, memory allocation, and security groups during creation.",
          "a: mbda counts a request each time it starts executing in response to an event notification or invocation call, including test invokes from the",
          "c: onsole. You are charged based on the total number of requests processed across all of your Lambda functions."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "39. QUESTION\n\nCategory: CDA - Development with AWS Services\n\nYou are a developer for a global technology company, which heavily uses AWS with regional offices in San Francisco, Manila, and Bangalore. [\n\nMost of the clients of your company are using serverless computing in which you are responsible for ensuring that their applications are\n\nworking efficiently.\n\nWhich of the following options are valid considerations in improving the performance of your Lambda function? (Select TWO.)\nLambda automatically creates Elastic IP's that enable your function to connect securely to other resources within your private\nVPC.\nAn increase in memory size triggers an equivalent increase in CPU available to your function.\nYou can throttle all incoming executions and stop processing any invocations to your function by setting concurrency to\n\nfalse .\nYou have to install the X-Ray daemon in Lambda to enable active tracing.\nThe concurrent execution limit is enforced against the sum of the concurrent executions of all function.\nIncorrect\nYou can use the AWS Lambda API or console to configure settings on your Lambda functions. Basic function settings include the description, role,\nand runtime that you specify when you create a function in the Lambda console. You can configure more settings after you create a function, or\nuse the API to set things like the handler name, memory allocation, and security groups during creation.\nLambda counts a request each time it starts executing in response to an event notification or invocation call, including test invokes from the\nconsole. You are charged based on the total number of requests processed across all of your Lambda functions.\n1"
      },
      "tags": {
        "services": [
          "Lambda",
          "VPC",
          "X-Ray",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "performance",
          "security",
          "VPC",
          "security group"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 125002.png",
      "parsed": {
        "question": "[\nCategory: CDA - Development with AWS Services\nA developer is creating a new global secondary index on a provisioned mode DynamoDB table. Since the application will store large quantities\nof data, the write capacity units must be specified for the expected workload on both the base table and its secondary index.\n\nWhich of the following should the developer do to avoid any potential request throttling?\n\nEnsure that the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.\n® Ensure that the global secondary index's provisioned RCU is equal or greater than the RCU of the base table.\n\nEnsure that the global secondary index's provisioned WCU is equal or less than the WCU of the base table.\n\nEnsure that the global secondary index's provisioned RCU is equal or less than the RCU of the base table.\n\nIncorrect\n\nA global secondary index (GSI) is an index with a partition key and a sort key that can be different from those on the base table. It is considered\n“global” because queries on the index can span all of the data in the base table, across all partitions.\nEvery global secondary index has its own provisioned throughput settings for read and write activity. Queries or scans on a global secondary index\nconsume capacity units from the index, not from the base table. The same holds true for global secondary index updates due to table writes.\nWhen you create a global secondary index on a provisioned mode table, you must specify read and write capacity units for the expected workload\non that index. The provisioned throughput settings of a global secondary index are separate from those of its base table. A Query operation on\na global secondary index consumes read capacity units from the index, not the base table. When you put, update, or delete items in a table, the\nalobal secondary indexes on that table are also undated: these index undates consume write capacitv units from the index not from the base",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer is creating a new global secondary index on a provisioned mode DynamoDB table. Since the application will store large quantities",
          "d: ata, the write capacity units must be specified for the expected workload on both the base table and its secondary index.",
          "c: h of the following should the developer do to avoid any potential request throttling?",
          "a: t the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.",
          "a: t the global secondary index's provisioned RCU is equal or greater than the RCU of the base table.",
          "a: t the global secondary index's provisioned WCU is equal or less than the WCU of the base table.",
          "a: t the global secondary index's provisioned RCU is equal or less than the RCU of the base table.",
          "c: orrect",
          "A: global secondary index (GSI) is an index with a partition key and a sort key that can be different from those on the base table. It is considered",
          "b: al” because queries on the index can span all of the data in the base table, across all partitions.",
          "b: al secondary index has its own provisioned throughput settings for read and write activity. Queries or scans on a global secondary index",
          "c: onsume capacity units from the index, not from the base table. The same holds true for global secondary index updates due to table writes.",
          "c: reate a global secondary index on a provisioned mode table, you must specify read and write capacity units for the expected workload",
          "a: t index. The provisioned throughput settings of a global secondary index are separate from those of its base table. A Query operation on",
          "a: global secondary index consumes read capacity units from the index, not the base table. When you put, update, or delete items in a table, the",
          "a: lobal secondary indexes on that table are also undated: these index undates consume write capacitv units from the index not from the base"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "EO\n42. QUESTION [\nCategory: CDA - Development with AWS Services\nA developer is creating a new global secondary index on a provisioned mode DynamoDB table. Since the application will store large quantities\nof data, the write capacity units must be specified for the expected workload on both the base table and its secondary index.\n\nWhich of the following should the developer do to avoid any potential request throttling?\n\nEnsure that the global secondary index's provisioned WCU is equal or greater than the WCU of the base table.\n® Ensure that the global secondary index's provisioned RCU is equal or greater than the RCU of the base table.\n\nEnsure that the global secondary index's provisioned WCU is equal or less than the WCU of the base table.\n\nEnsure that the global secondary index's provisioned RCU is equal or less than the RCU of the base table.\n\nIncorrect\n\nA global secondary index (GSI) is an index with a partition key and a sort key that can be different from those on the base table. It is considered\n“global” because queries on the index can span all of the data in the base table, across all partitions.\nEvery global secondary index has its own provisioned throughput settings for read and write activity. Queries or scans on a global secondary index\nconsume capacity units from the index, not from the base table. The same holds true for global secondary index updates due to table writes.\nWhen you create a global secondary index on a provisioned mode table, you must specify read and write capacity units for the expected workload\non that index. The provisioned throughput settings of a global secondary index are separate from those of its base table. A Query operation on\na global secondary index consumes read capacity units from the index, not the base table. When you put, update, or delete items in a table, the\nalobal secondary indexes on that table are also undated: these index undates consume write capacitv units from the index not from the base"
      },
      "tags": {
        "services": [
          "DynamoDB",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "throttling"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 125206.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nYou were recently hired by a media company that is planning to build a news portal using Elastic Beanstalk and DynamoDB database, which\nalready contains a few data. There is already an existing DynamoDB Table that has an attribute of ArticleName which acts as the partition\nkey and a category attribute as its sort key. You are instructed to develop a feature that will query the ArticleName attribute but will\nuse a different sort key other than the existing one. The feature also requires strong read consistency to fetch the most up-to-date data.\nWhich of the following solutions should you implement?\nCreate a new DynamoDB table with a Local Secondary Index that uses the ArticleName attribute with a different sort key.\nMigrate the data from the existing table to the new table.\nCreate a Global Secondary Index which uses the Articlename attribute and your alternative sort key as projected\nattributes.\nCreate a Local Secondary Index that uses the ArticleName attribute and a different sort key.\n® Create a Global Secondary Index that uses the Articlename attribute and a different sort key.\nIncorrect\nA local secondary index maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all\nof the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data\nin a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items\nefficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.\nSuppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index,\nthe application would have to Scan the entire Thread table and discard any posts that were not within the specified time frame. With a local\nsecondary index, a Query operation could use LastPostDateTime as a sort key and find the data quickly.\nTo create a Local Secondary Index, make sure that the primary key of the index is the same as the primary key/partition key of the table, just as",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "c: ently hired by a media company that is planning to build a news portal using Elastic Beanstalk and DynamoDB database, which",
          "a: lready contains a few data. There is already an existing DynamoDB Table that has an attribute of ArticleName which acts as the partition",
          "a: nd a category attribute as its sort key. You are instructed to develop a feature that will query the ArticleName attribute but will",
          "a: different sort key other than the existing one. The feature also requires strong read consistency to fetch the most up-to-date data.",
          "c: h of the following solutions should you implement?",
          "C: reate a new DynamoDB table with a Local Secondary Index that uses the ArticleName attribute with a different sort key.",
          "a: te the data from the existing table to the new table.",
          "C: reate a Global Secondary Index which uses the Articlename attribute and your alternative sort key as projected",
          "a: ttributes.",
          "C: reate a Local Secondary Index that uses the ArticleName attribute and a different sort key.",
          "C: reate a Global Secondary Index that uses the Articlename attribute and a different sort key.",
          "c: orrect",
          "A: local secondary index maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all",
          "a: ttributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data",
          "a: local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items",
          "c: iently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.",
          "a: t an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index,",
          "a: pplication would have to Scan the entire Thread table and discard any posts that were not within the specified time frame. With a local",
          "c: ondary index, a Query operation could use LastPostDateTime as a sort key and find the data quickly.",
          "c: reate a Local Secondary Index, make sure that the primary key of the index is the same as the primary key/partition key of the table, just as"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "48. QUESTION\nCategory: CDA - Development with AWS Services\nYou were recently hired by a media company that is planning to build a news portal using Elastic Beanstalk and DynamoDB database, which\nalready contains a few data. There is already an existing DynamoDB Table that has an attribute of ArticleName which acts as the partition\nkey and a category attribute as its sort key. You are instructed to develop a feature that will query the ArticleName attribute but will\nuse a different sort key other than the existing one. The feature also requires strong read consistency to fetch the most up-to-date data.\nWhich of the following solutions should you implement?\nCreate a new DynamoDB table with a Local Secondary Index that uses the ArticleName attribute with a different sort key.\nMigrate the data from the existing table to the new table.\nCreate a Global Secondary Index which uses the Articlename attribute and your alternative sort key as projected\nattributes.\nCreate a Local Secondary Index that uses the ArticleName attribute and a different sort key.\n® Create a Global Secondary Index that uses the Articlename attribute and a different sort key.\nIncorrect\nA local secondary index maintains an alternate sort key for a given partition key value. A local secondary index also contains a copy of some or all\nof the attributes from its base table; you specify which attributes are projected into the local secondary index when you create the table. The data\nin a local secondary index is organized by the same partition key as the base table, but with a different sort key. This lets you access data items\nefficiently across this different dimension. For greater query or scan flexibility, you can create up to five local secondary indexes per table.\nSuppose that an application needs to find all of the threads that have been posted within the last three months. Without a local secondary index,\nthe application would have to Scan the entire Thread table and discard any posts that were not within the specified time frame. With a local\nsecondary index, a Query operation could use LastPostDateTime as a sort key and find the data quickly.\nTo create a Local Secondary Index, make sure that the primary key of the index is the same as the primary key/partition key of the table, just as"
      },
      "tags": {
        "services": [
          "Elastic Beanstalk",
          "DynamoDB",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 125226.png",
      "parsed": {
        "question": "uest a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior\nwrite operations that were successful. A strongly consistent read might not be available if there is a network delay or outage. Strongly consistent\nreads are not supported on global secondary indexes.\n\nThe primary key of a local secondary index must be composite (partition key and sort key). A local secondary index lets you query over a single\npartition, as specified by the partition key value in the query.\n\nLocal secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can\nyou delete any local secondary indexes that currently exist.\n\nHence, the correct answer in this scenario is to create a new DynamoDB table with a Local Secondary Index that uses the ArticleName\nattribute with a different sort key then migrate the data from the existing table to the new table.\n\nCreating a Global Secondary Index that uses the ArticleName attribute and a different sort key is incorrect because it is stated in the\nscenario that you are still using the same partition key, but with an alternate sort key that warrants the use of a local secondary index instead of a\nglobal secondary index.\n\nCreating a Global Secondary Index which uses the ArticleName attribute and your alternative sort key as projected attributes is incorrect\nbecause using a local secondary index is a more appropriate solution to be used in this scenario just as explained above. Moreover, projected\nattributes are just attributes stored in the index that can be returned by queries and scans performed on the index hence, these are not useful in\nsatisfying the provided requirement.\n\nCreating a Local Secondary Index that uses the ArticleName attribute and a different sort key is incorrect. Although it uses the correct\ntype of index, you cannot add a local secondary index to an already existing table.\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Secondarylndexes.html",
        "options": [
          "a: strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior",
          "a: tions that were successful. A strongly consistent read might not be available if there is a network delay or outage. Strongly consistent",
          "a: ds are not supported on global secondary indexes.",
          "a: ry key of a local secondary index must be composite (partition key and sort key). A local secondary index lets you query over a single",
          "a: rtition, as specified by the partition key value in the query.",
          "c: al secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can",
          "d: elete any local secondary indexes that currently exist.",
          "c: e, the correct answer in this scenario is to create a new DynamoDB table with a Local Secondary Index that uses the ArticleName",
          "a: ttribute with a different sort key then migrate the data from the existing table to the new table.",
          "C: reating a Global Secondary Index that uses the ArticleName attribute and a different sort key is incorrect because it is stated in the",
          "c: enario that you are still using the same partition key, but with an alternate sort key that warrants the use of a local secondary index instead of a",
          "b: al secondary index.",
          "C: reating a Global Secondary Index which uses the ArticleName attribute and your alternative sort key as projected attributes is incorrect",
          "b: ecause using a local secondary index is a more appropriate solution to be used in this scenario just as explained above. Moreover, projected",
          "a: ttributes are just attributes stored in the index that can be returned by queries and scans performed on the index hence, these are not useful in",
          "a: tisfying the provided requirement.",
          "C: reating a Local Secondary Index that uses the ArticleName attribute and a different sort key is incorrect. Although it uses the correct",
          "d: ex, you cannot add a local secondary index to an already existing table.",
          "c: es:",
          "d: ocs.aws.amazon.com/amazondynamodb/latest/developerguide/Secondarylndexes.html"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior\nwrite operations that were successful. A strongly consistent read might not be available if there is a network delay or outage. Strongly consistent\nreads are not supported on global secondary indexes.\n\nThe primary key of a local secondary index must be composite (partition key and sort key). A local secondary index lets you query over a single\npartition, as specified by the partition key value in the query.\n\nLocal secondary indexes are created at the same time that you create a table. You cannot add a local secondary index to an existing table, nor can\nyou delete any local secondary indexes that currently exist.\n\nHence, the correct answer in this scenario is to create a new DynamoDB table with a Local Secondary Index that uses the ArticleName\nattribute with a different sort key then migrate the data from the existing table to the new table.\n\nCreating a Global Secondary Index that uses the ArticleName attribute and a different sort key is incorrect because it is stated in the\nscenario that you are still using the same partition key, but with an alternate sort key that warrants the use of a local secondary index instead of a\nglobal secondary index.\n\nCreating a Global Secondary Index which uses the ArticleName attribute and your alternative sort key as projected attributes is incorrect\nbecause using a local secondary index is a more appropriate solution to be used in this scenario just as explained above. Moreover, projected\nattributes are just attributes stored in the index that can be returned by queries and scans performed on the index hence, these are not useful in\nsatisfying the provided requirement.\n\nCreating a Local Secondary Index that uses the ArticleName attribute and a different sort key is incorrect. Although it uses the correct\ntype of index, you cannot add a local secondary index to an already existing table.\n\nReferences:\n\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Secondarylndexes.html"
      },
      "tags": {
        "services": [
          "DynamoDB",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 125556.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services [\nA developer is planning to add a global secondary index in a DynamoDB table. This will allow the application to query a specific index that can\nspan all of the data in the base table, across all partitions.\nWhich of the following should the developer consider when using this type of index? (Select TWO.)\nQueries or scans on this index consume read capacity units from the base table.\nQueries or scans on this index consume capacity units from the index, not from the base table.\nWhen you query this index, you can choose either eventual consistency or strong consistency.\nFor each partition key value, the total size of all indexed items must be 10 GB or less.\nQueries on this index support eventual consistency only.\nIncorrect\nA global secondary index is an index with a partition key and a sort key that can be different from those on the base table. A global secondary\nindex is considered “global” because queries on the index can span all of the data in the base table, across all partitions.\nTo create a table with one or more global secondary indexes, use the CreateTable operation with the GlobalSecondaryindexes parameter. For\nmaximum query flexibility, you can create up to 20 global secondary indexes (default limit) per table. You must specify one attribute to act as the\nindex partition key; you can optionally specify another attribute for the index sort key. It is not necessary for either of these key attributes to be the\nsame as a key attribute in the table. Global secondary indexes inherit the read/write capacity mode from the base table.\nCharacteristic Global Secondary Index Local Secondary Index\nKey Schema | The primary key of a global secondary index can be either simple (partition key) or composite The primary key of a local secondary index must be composite (partition key and\n| (partition kev and <ort kev) cart kev)",
        "options": [
          "C: ategory: CDA - Development with AWS Services [",
          "A: developer is planning to add a global secondary index in a DynamoDB table. This will allow the application to query a specific index that can",
          "a: n all of the data in the base table, across all partitions.",
          "c: h of the following should the developer consider when using this type of index? (Select TWO.)",
          "c: ans on this index consume read capacity units from the base table.",
          "c: ans on this index consume capacity units from the index, not from the base table.",
          "d: ex, you can choose either eventual consistency or strong consistency.",
          "a: ch partition key value, the total size of all indexed items must be 10 GB or less.",
          "d: ex support eventual consistency only.",
          "c: orrect",
          "A: global secondary index is an index with a partition key and a sort key that can be different from those on the base table. A global secondary",
          "d: ex is considered “global” because queries on the index can span all of the data in the base table, across all partitions.",
          "c: reate a table with one or more global secondary indexes, use the CreateTable operation with the GlobalSecondaryindexes parameter. For",
          "a: ximum query flexibility, you can create up to 20 global secondary indexes (default limit) per table. You must specify one attribute to act as the",
          "d: ex partition key; you can optionally specify another attribute for the index sort key. It is not necessary for either of these key attributes to be the",
          "a: me as a key attribute in the table. Global secondary indexes inherit the read/write capacity mode from the base table.",
          "C: haracteristic Global Secondary Index Local Secondary Index",
          "c: hema | The primary key of a global secondary index can be either simple (partition key) or composite The primary key of a local secondary index must be composite (partition key and",
          "a: rtition kev and <ort kev) cart kev)"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "54. QUESTION\nCategory: CDA - Development with AWS Services [\nA developer is planning to add a global secondary index in a DynamoDB table. This will allow the application to query a specific index that can\nspan all of the data in the base table, across all partitions.\nWhich of the following should the developer consider when using this type of index? (Select TWO.)\nQueries or scans on this index consume read capacity units from the base table.\nQueries or scans on this index consume capacity units from the index, not from the base table.\nWhen you query this index, you can choose either eventual consistency or strong consistency.\nFor each partition key value, the total size of all indexed items must be 10 GB or less.\nQueries on this index support eventual consistency only.\nIncorrect\nA global secondary index is an index with a partition key and a sort key that can be different from those on the base table. A global secondary\nindex is considered “global” because queries on the index can span all of the data in the base table, across all partitions.\nTo create a table with one or more global secondary indexes, use the CreateTable operation with the GlobalSecondaryindexes parameter. For\nmaximum query flexibility, you can create up to 20 global secondary indexes (default limit) per table. You must specify one attribute to act as the\nindex partition key; you can optionally specify another attribute for the index sort key. It is not necessary for either of these key attributes to be the\nsame as a key attribute in the table. Global secondary indexes inherit the read/write capacity mode from the base table.\nCharacteristic Global Secondary Index Local Secondary Index\nKey Schema | The primary key of a global secondary index can be either simple (partition key) or composite The primary key of a local secondary index must be composite (partition key and\n| (partition kev and <ort kev) cart kev)"
      },
      "tags": {
        "services": [
          "DynamoDB",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 110603.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nA developer has just finished writing a serverless application using AWS SAM (Serverless Application Model) on a local machine. There is a |\nSAM template ready and the corresponding Lambda function code in a directory. The developer now wants to deploy this application to AWS.\nWhich combination of steps should the developer follow to successfully deploy the SAM application? (Select THREE)\nBuild the SAM template in the local environment\nDeploy the SAM template from an Amazon S3 bucket.\nPackage the SAM application for deployment.\nDeploy the SAM template from AWS CodePipeline.\nBuild the SAM template using the AWS SDK for AWS CodeDeploy.\nBuild the SAM template in an Amazon EC2 instance.\nIncorrect\nAWS SAM uses AWS CloudFormation as the underlying deployment mechanism. You can deploy your application by using AWS SAM command\nline interface (CLI) commands. You can also use other AWS services that integrate with AWS SAM to automate your deployments.\nGENE IEEE IEE EEE EEE EE a\n<P) | —\ni “U | oot] [|",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: developer has just finished writing a serverless application using AWS SAM (Serverless Application Model) on a local machine. There is a |",
          "A: M template ready and the corresponding Lambda function code in a directory. The developer now wants to deploy this application to AWS.",
          "c: h combination of steps should the developer follow to successfully deploy the SAM application? (Select THREE)",
          "B: uild the SAM template in the local environment",
          "D: eploy the SAM template from an Amazon S3 bucket.",
          "a: ckage the SAM application for deployment.",
          "D: eploy the SAM template from AWS CodePipeline.",
          "B: uild the SAM template using the AWS SDK for AWS CodeDeploy.",
          "B: uild the SAM template in an Amazon EC2 instance.",
          "c: orrect",
          "A: WS SAM uses AWS CloudFormation as the underlying deployment mechanism. You can deploy your application by using AWS SAM command",
          "a: ce (CLI) commands. You can also use other AWS services that integrate with AWS SAM to automate your deployments.",
          "a: <P) | —"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "55. QUESTION\nCategory: CDA - Deployment\nA developer has just finished writing a serverless application using AWS SAM (Serverless Application Model) on a local machine. There is a |\nSAM template ready and the corresponding Lambda function code in a directory. The developer now wants to deploy this application to AWS.\nWhich combination of steps should the developer follow to successfully deploy the SAM application? (Select THREE)\nBuild the SAM template in the local environment\nDeploy the SAM template from an Amazon S3 bucket.\nPackage the SAM application for deployment.\nDeploy the SAM template from AWS CodePipeline.\nBuild the SAM template using the AWS SDK for AWS CodeDeploy.\nBuild the SAM template in an Amazon EC2 instance.\nIncorrect\nAWS SAM uses AWS CloudFormation as the underlying deployment mechanism. You can deploy your application by using AWS SAM command\nline interface (CLI) commands. You can also use other AWS services that integrate with AWS SAM to automate your deployments.\nGENE IEEE IEE EEE EEE EE a\n<P) | —\ni “U | oot] [|"
      },
      "tags": {
        "services": [
          "EC2",
          "Lambda",
          "S3",
          "CodeDeploy",
          "CodePipeline",
          "CloudFormation",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 110635.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA company is using a combination of CodeBuild, CodePipeline, and CodeDeploy services for its continuous integration and continuous delivery\n(CI/CD) pipeline on AWS. They want someone to perform a code review before a revision is allowed into the next stage of a pipeline. If the\naction is approved, the pipeline execution resumes, but if it is not, then the pipeline execution will not proceed.\nWhich of the following is the MOST suitable solution to implement in this scenario?\nImplement a manual approval actions configuration in CodePipeline. Send the approval request to an SNS Topic.\nImplement a manual approval actions configuration in CodePipeline. Send the approval request to an SQS Queue.\n® Split the processes into different Task states using Step Functions. Use a Wait state to set a timeout for approval.\nRemodel the pipeline using AWS Serverless Application Model (AWS SAM)\nIncorrect\nIn AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that\nsomeone with the required AWS Identity and Access Management permissions can approve or reject the action.\nIf the action is approved, the pipeline execution resumes. If the action is rejected — or if no one approves or rejects the action within seven days of\nthe pipeline reaching the action and stopping — the result is the same as an action failing, and the pipeline execution does not continue.\nYou might use manual approvals for these reasons:\n— You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.\n— You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: company is using a combination of CodeBuild, CodePipeline, and CodeDeploy services for its continuous integration and continuous delivery",
          "C: I/CD) pipeline on AWS. They want someone to perform a code review before a revision is allowed into the next stage of a pipeline. If the",
          "a: ction is approved, the pipeline execution resumes, but if it is not, then the pipeline execution will not proceed.",
          "c: h of the following is the MOST suitable solution to implement in this scenario?",
          "a: manual approval actions configuration in CodePipeline. Send the approval request to an SNS Topic.",
          "a: manual approval actions configuration in CodePipeline. Send the approval request to an SQS Queue.",
          "c: esses into different Task states using Step Functions. Use a Wait state to set a timeout for approval.",
          "d: el the pipeline using AWS Serverless Application Model (AWS SAM)",
          "c: orrect",
          "A: WS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that",
          "d: AWS Identity and Access Management permissions can approve or reject the action.",
          "a: ction is approved, the pipeline execution resumes. If the action is rejected — or if no one approves or rejects the action within seven days of",
          "a: ching the action and stopping — the result is the same as an action failing, and the pipeline execution does not continue.",
          "a: nual approvals for these reasons:",
          "a: nt someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.",
          "a: nt someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "56. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA company is using a combination of CodeBuild, CodePipeline, and CodeDeploy services for its continuous integration and continuous delivery\n(CI/CD) pipeline on AWS. They want someone to perform a code review before a revision is allowed into the next stage of a pipeline. If the\naction is approved, the pipeline execution resumes, but if it is not, then the pipeline execution will not proceed.\nWhich of the following is the MOST suitable solution to implement in this scenario?\nImplement a manual approval actions configuration in CodePipeline. Send the approval request to an SNS Topic.\nImplement a manual approval actions configuration in CodePipeline. Send the approval request to an SQS Queue.\n® Split the processes into different Task states using Step Functions. Use a Wait state to set a timeout for approval.\nRemodel the pipeline using AWS Serverless Application Model (AWS SAM)\nIncorrect\nIn AWS CodePipeline, you can add an approval action to a stage in a pipeline at the point where you want the pipeline execution to stop so that\nsomeone with the required AWS Identity and Access Management permissions can approve or reject the action.\nIf the action is approved, the pipeline execution resumes. If the action is rejected — or if no one approves or rejects the action within seven days of\nthe pipeline reaching the action and stopping — the result is the same as an action failing, and the pipeline execution does not continue.\nYou might use manual approvals for these reasons:\n— You want someone to perform a code review or change management review before a revision is allowed into the next stage of a pipeline.\n— You want someone to perform manual quality assurance testing on the latest version of an application, or to confirm the integrity of a build"
      },
      "tags": {
        "services": [
          "SQS",
          "SNS",
          "Step Functions",
          "CodeBuild",
          "CodeDeploy",
          "CodePipeline",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment"
        ],
        "keywords": [
          "serverless",
          "CI/CD",
          "queue",
          "topic"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 110809.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services E\nThe company that you are working for recently decided to migrate and transform their monolithic application on-premises to a Lambda\napplication. It is your responsibility to ensure that application works effectively in AWS.\nWhich of the following are the best practices in developing Lambda functions? (Select TWO.)\nInclude the core logic in the Lambda handler.\nUse AWS Lambda Environment Variables to pass operational parameters to your function.\nUse recursive code.\nTake advantage of Execution Context reuse to improve the performance of your function.\nUse Amazon Inspector for troubleshooting.\nIncorrect\nBelow are some of the best practices in working with AWS Lambda Functions:\n— Separate the Lambda handler (entry point) from your core logic.\n— Take advantage of Execution Context reuse to improve the performance of your function\n— Use AWS Lambda Environment Variables to pass operational parameters to your function.\n— Control the dependencies in your function's deployment package.\n— Minimize your deployment package size to its runtime necessities.\n— Reduce the time it takes | ambda to unnack denlovment packaaes",
        "options": [
          "C: ategory: CDA - Development with AWS Services E",
          "c: ompany that you are working for recently decided to migrate and transform their monolithic application on-premises to a Lambda",
          "a: pplication. It is your responsibility to ensure that application works effectively in AWS.",
          "c: h of the following are the best practices in developing Lambda functions? (Select TWO.)",
          "c: lude the core logic in the Lambda handler.",
          "A: WS Lambda Environment Variables to pass operational parameters to your function.",
          "c: ursive code.",
          "a: ke advantage of Execution Context reuse to improve the performance of your function.",
          "A: mazon Inspector for troubleshooting.",
          "c: orrect",
          "B: elow are some of the best practices in working with AWS Lambda Functions:",
          "a: rate the Lambda handler (entry point) from your core logic.",
          "a: ke advantage of Execution Context reuse to improve the performance of your function",
          "A: WS Lambda Environment Variables to pass operational parameters to your function.",
          "C: ontrol the dependencies in your function's deployment package.",
          "d: eployment package size to its runtime necessities.",
          "d: uce the time it takes | ambda to unnack denlovment packaaes"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "59. QUESTION\nCategory: CDA - Development with AWS Services E\nThe company that you are working for recently decided to migrate and transform their monolithic application on-premises to a Lambda\napplication. It is your responsibility to ensure that application works effectively in AWS.\nWhich of the following are the best practices in developing Lambda functions? (Select TWO.)\nInclude the core logic in the Lambda handler.\nUse AWS Lambda Environment Variables to pass operational parameters to your function.\nUse recursive code.\nTake advantage of Execution Context reuse to improve the performance of your function.\nUse Amazon Inspector for troubleshooting.\nIncorrect\nBelow are some of the best practices in working with AWS Lambda Functions:\n— Separate the Lambda handler (entry point) from your core logic.\n— Take advantage of Execution Context reuse to improve the performance of your function\n— Use AWS Lambda Environment Variables to pass operational parameters to your function.\n— Control the dependencies in your function's deployment package.\n— Minimize your deployment package size to its runtime necessities.\n— Reduce the time it takes | ambda to unnack denlovment packaaes"
      },
      "tags": {
        "services": [
          "Lambda",
          "Inspector"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "performance",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 110840.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services Ei\nThe operating cost of a serverless application is quite high and you are instructed to look for ways to lower the costs. As part of its processing,\na Lambda function sends 320 strongly consistent read requests per second to a DynamoDB table which has a provisioned RCU of 5440. The\naverage size of items stored in the database is 17 KB.\nWhich of the following is the MOST suitable action that should you do to make the application more cost-effective while maintaining its\nperformance?\nSet the provisioned RCU to 1600.\nDecrease the provisioned RCU down to 800.\n® Implement exponential backoff.\nSwitch the table from using provisioned mode to on-demand mode.\nIncorrect\nIn this scenario, a Lambda function makes 320 strongly consistent read requests per second against a DynamoDB table which has a provisioned\nRCU of 5440. The average size of items stored in the database is 17 KB.\nIt seems that the RCU is quite high and the calculations to come up with this value are incorrect. If you multiply 320 x 17, then you'll get 5,440\nwhich is a correct calculation for WCU but not for the RCU.\nCapacity calculator\nAvg. item size KB",
        "options": [
          "C: ategory: CDA - Development with AWS Services Ei",
          "a: ting cost of a serverless application is quite high and you are instructed to look for ways to lower the costs. As part of its processing,",
          "a: Lambda function sends 320 strongly consistent read requests per second to a DynamoDB table which has a provisioned RCU of 5440. The",
          "a: verage size of items stored in the database is 17 KB.",
          "c: h of the following is the MOST suitable action that should you do to make the application more cost-effective while maintaining its",
          "a: nce?",
          "d: RCU to 1600.",
          "D: ecrease the provisioned RCU down to 800.",
          "a: l backoff.",
          "c: h the table from using provisioned mode to on-demand mode.",
          "c: orrect",
          "c: enario, a Lambda function makes 320 strongly consistent read requests per second against a DynamoDB table which has a provisioned",
          "C: U of 5440. The average size of items stored in the database is 17 KB.",
          "a: t the RCU is quite high and the calculations to come up with this value are incorrect. If you multiply 320 x 17, then you'll get 5,440",
          "c: h is a correct calculation for WCU but not for the RCU.",
          "C: apacity calculator",
          "A: vg. item size KB"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "61. QUESTION\nCategory: CDA - Development with AWS Services Ei\nThe operating cost of a serverless application is quite high and you are instructed to look for ways to lower the costs. As part of its processing,\na Lambda function sends 320 strongly consistent read requests per second to a DynamoDB table which has a provisioned RCU of 5440. The\naverage size of items stored in the database is 17 KB.\nWhich of the following is the MOST suitable action that should you do to make the application more cost-effective while maintaining its\nperformance?\nSet the provisioned RCU to 1600.\nDecrease the provisioned RCU down to 800.\n® Implement exponential backoff.\nSwitch the table from using provisioned mode to on-demand mode.\nIncorrect\nIn this scenario, a Lambda function makes 320 strongly consistent read requests per second against a DynamoDB table which has a provisioned\nRCU of 5440. The average size of items stored in the database is 17 KB.\nIt seems that the RCU is quite high and the calculations to come up with this value are incorrect. If you multiply 320 x 17, then you'll get 5,440\nwhich is a correct calculation for WCU but not for the RCU.\nCapacity calculator\nAvg. item size KB"
      },
      "tags": {
        "services": [
          "Lambda",
          "DynamoDB",
          "ECR"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "performance"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 115538.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization K\nA developer is hosting a static website from an S3 bucket. The website makes requests to an API Gateway endpoint integrated with a Lambda\nfunction (non-proxy). The developer noticed that the requests were failing. Upon debugging, he found a: \"No 'Access-Control-Allow-\nOrigin' header is present on the requested resource\" error message.\nWhat should the developer do to resolve this issue?\nSet the value of the Access-Control-Max-Age header to 0.\n® Inthe Amazon S3 Console, enable cross-origin resource sharing (CORS) on the S3 bucket where the website is hosted.\nIn the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource.\nSet the value of the Access-Control-Allow-Credentials headerto true .\nIncorrect\nCross-origin resource sharing (CORS) is a browser security feature that restricts cross-origin HTTP requests that are initiated from scripts\nrunning in the browser. If your REST API's resources receive non-simple cross-origin HTTP requests, you need to enable CORS support.\nResources Actions ~ {Enable CORS\n-& Cross-Origin Resource Sharing (CORS) allows browsers to make HTTP requests to servers with a\nv & hello different domain/origin. Specify which methods in the /hello resource are available to CORS requests. To\nGET define static values surround the value in single quotes (eg. 'amazon.com'). To define mappings use the\nsyntax described in the Method Editor (eg. method request querystring. myQueryString)\nMethods* [V]GeT [| oPTIONS ©\nAccess-Control-Allow-Methods GET OPTIONS @",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization K",
          "A: developer is hosting a static website from an S3 bucket. The website makes requests to an API Gateway endpoint integrated with a Lambda",
          "c: tion (non-proxy). The developer noticed that the requests were failing. Upon debugging, he found a: \"No 'Access-Control-Allow-",
          "a: der is present on the requested resource\" error message.",
          "a: t should the developer do to resolve this issue?",
          "a: lue of the Access-Control-Max-Age header to 0.",
          "A: mazon S3 Console, enable cross-origin resource sharing (CORS) on the S3 bucket where the website is hosted.",
          "A: PI Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource.",
          "a: lue of the Access-Control-Allow-Credentials headerto true .",
          "c: orrect",
          "C: ross-origin resource sharing (CORS) is a browser security feature that restricts cross-origin HTTP requests that are initiated from scripts",
          "b: rowser. If your REST API's resources receive non-simple cross-origin HTTP requests, you need to enable CORS support.",
          "c: es Actions ~ {Enable CORS",
          "C: ross-Origin Resource Sharing (CORS) allows browsers to make HTTP requests to servers with a",
          "d: ifferent domain/origin. Specify which methods in the /hello resource are available to CORS requests. To",
          "d: efine static values surround the value in single quotes (eg. 'amazon.com'). To define mappings use the",
          "a: x described in the Method Editor (eg. method request querystring. myQueryString)",
          "d: s* [V]GeT [| oPTIONS ©",
          "A: ccess-Control-Allow-Methods GET OPTIONS @"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "1. QUESTION\nCategory: CDA - Troubleshooting and Optimization K\nA developer is hosting a static website from an S3 bucket. The website makes requests to an API Gateway endpoint integrated with a Lambda\nfunction (non-proxy). The developer noticed that the requests were failing. Upon debugging, he found a: \"No 'Access-Control-Allow-\nOrigin' header is present on the requested resource\" error message.\nWhat should the developer do to resolve this issue?\nSet the value of the Access-Control-Max-Age header to 0.\n® Inthe Amazon S3 Console, enable cross-origin resource sharing (CORS) on the S3 bucket where the website is hosted.\nIn the API Gateway Console, enable cross-origin resource sharing (CORS) for the method in the specified resource.\nSet the value of the Access-Control-Allow-Credentials headerto true .\nIncorrect\nCross-origin resource sharing (CORS) is a browser security feature that restricts cross-origin HTTP requests that are initiated from scripts\nrunning in the browser. If your REST API's resources receive non-simple cross-origin HTTP requests, you need to enable CORS support.\nResources Actions ~ {Enable CORS\n-& Cross-Origin Resource Sharing (CORS) allows browsers to make HTTP requests to servers with a\nv & hello different domain/origin. Specify which methods in the /hello resource are available to CORS requests. To\nGET define static values surround the value in single quotes (eg. 'amazon.com'). To define mappings use the\nsyntax described in the Method Editor (eg. method request querystring. myQueryString)\nMethods* [V]GeT [| oPTIONS ©\nAccess-Control-Allow-Methods GET OPTIONS @"
      },
      "tags": {
        "services": [
          "Lambda",
          "S3",
          "EBS",
          "API Gateway"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "security",
          "CORS"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 115611.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nA team is deploying a serverless app with AWS Cloud Development Kit (CDK), using the Lambda construct library to define Lambda functions. E\nThey've had successful deployments in a test environment using the cdk deploy command. However, when the team attempts their first\ndeployment to a new AWS account, it fails, returning a NoSuchBucket error.\nThe team is confident that they made no alterations to the code or CDK configurations after the last successful deployment.\nWhat AWS CDK CLI command should be run first to fix the problem?\ncdk bootstrap\ncdk synth\ncdk import\n— mm\ncdk context\n_—\nIncorrect\nThe NoSuchBucket error indicates that the AWS CDK is trying to access an S3 bucket that doesn’t exist in the new AWS account. This\nhappens because the CDK uses an S3 bucket to store assets required for deployment, such as Lambda code or Docker images. When you move to\nanew AWS account or a new region within an account, you need to set up the environment for the CDK. This is done using the cdk\nbootstrap command.",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: team is deploying a serverless app with AWS Cloud Development Kit (CDK), using the Lambda construct library to define Lambda functions. E",
          "a: d successful deployments in a test environment using the cdk deploy command. However, when the team attempts their first",
          "d: eployment to a new AWS account, it fails, returning a NoSuchBucket error.",
          "a: m is confident that they made no alterations to the code or CDK configurations after the last successful deployment.",
          "a: t AWS CDK CLI command should be run first to fix the problem?",
          "c: dk bootstrap",
          "c: dk synth",
          "c: dk import",
          "c: dk context",
          "c: orrect",
          "c: hBucket error indicates that the AWS CDK is trying to access an S3 bucket that doesn’t exist in the new AWS account. This",
          "a: ppens because the CDK uses an S3 bucket to store assets required for deployment, such as Lambda code or Docker images. When you move to",
          "a: new AWS account or a new region within an account, you need to set up the environment for the CDK. This is done using the cdk",
          "b: ootstrap command."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "2. QUESTION\nCategory: CDA - Deployment\nA team is deploying a serverless app with AWS Cloud Development Kit (CDK), using the Lambda construct library to define Lambda functions. E\nThey've had successful deployments in a test environment using the cdk deploy command. However, when the team attempts their first\ndeployment to a new AWS account, it fails, returning a NoSuchBucket error.\nThe team is confident that they made no alterations to the code or CDK configurations after the last successful deployment.\nWhat AWS CDK CLI command should be run first to fix the problem?\ncdk bootstrap\ncdk synth\ncdk import\n— mm\ncdk context\n_—\nIncorrect\nThe NoSuchBucket error indicates that the AWS CDK is trying to access an S3 bucket that doesn’t exist in the new AWS account. This\nhappens because the CDK uses an S3 bucket to store assets required for deployment, such as Lambda code or Docker images. When you move to\nanew AWS account or a new region within an account, you need to set up the environment for the CDK. This is done using the cdk\nbootstrap command."
      },
      "tags": {
        "services": [
          "Lambda",
          "S3",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 115646.png",
      "parsed": {
        "question": "®\nCategory: CDA - Troubleshooting and Optimization i:\nA developer is building an AWS Lambda-based Java application that optimizes pictures uploaded to an S3 bucket. Upon running several tests,\nthe Lambda function shows a cold start of about 5 seconds.\nWhich of the following could the developer do to reduce the cold start time? (Select TWO.)\nRun the Lambda function in a VPC to gain access to Amazon's high-end infrastructure.\nReduce the deployment package's size by including only the needed modules from the AWS SDK for Java.\nIncrease the memory allocation setting for the Lambda function.\nAdd the Spring Framework to the project and enable dependency injection.\nIncrease the timeout setting for the Lambda function.\nIncorrect\nA cold start happens when a system needs to create a new resource in response to an event/request. Cold starts are not unique to Lambda. There\nare also cold starts in container orchestration, high-performance computing, or any places where IT resources need to be spun up.\nFunction lifecycle — worker host\nStart new\nDownload Execute Execute\nJ ~~",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization i:",
          "A: developer is building an AWS Lambda-based Java application that optimizes pictures uploaded to an S3 bucket. Upon running several tests,",
          "a: mbda function shows a cold start of about 5 seconds.",
          "c: h of the following could the developer do to reduce the cold start time? (Select TWO.)",
          "a: mbda function in a VPC to gain access to Amazon's high-end infrastructure.",
          "d: uce the deployment package's size by including only the needed modules from the AWS SDK for Java.",
          "c: rease the memory allocation setting for the Lambda function.",
          "A: dd the Spring Framework to the project and enable dependency injection.",
          "c: rease the timeout setting for the Lambda function.",
          "c: orrect",
          "A: cold start happens when a system needs to create a new resource in response to an event/request. Cold starts are not unique to Lambda. There",
          "a: re also cold starts in container orchestration, high-performance computing, or any places where IT resources need to be spun up.",
          "c: tion lifecycle — worker host",
          "a: rt new",
          "D: ownload Execute Execute"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "3. QUESTION\n®\nCategory: CDA - Troubleshooting and Optimization i:\nA developer is building an AWS Lambda-based Java application that optimizes pictures uploaded to an S3 bucket. Upon running several tests,\nthe Lambda function shows a cold start of about 5 seconds.\nWhich of the following could the developer do to reduce the cold start time? (Select TWO.)\nRun the Lambda function in a VPC to gain access to Amazon's high-end infrastructure.\nReduce the deployment package's size by including only the needed modules from the AWS SDK for Java.\nIncrease the memory allocation setting for the Lambda function.\nAdd the Spring Framework to the project and enable dependency injection.\nIncrease the timeout setting for the Lambda function.\nIncorrect\nA cold start happens when a system needs to create a new resource in response to an event/request. Cold starts are not unique to Lambda. There\nare also cold starts in container orchestration, high-performance computing, or any places where IT resources need to be spun up.\nFunction lifecycle — worker host\nStart new\nDownload Execute Execute\nJ ~~"
      },
      "tags": {
        "services": [
          "Lambda",
          "S3",
          "VPC"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "container",
          "performance",
          "deployment",
          "VPC",
          "cold start"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 115728.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization I\nA developer has been instructed to automate the creation of the snapshot of an existing Amazon EC2 instance. The engineer created a script\nthat uses the AWS Command Line Interface (CLI) to run the necessary API call. He is getting an InvalidInstanceID.NotFound error\nwhenever the script is run.\nWhat is the most likely cause of the error?\n® The Image Id used in running the command for creating a snapshot is incorrect.\nThe AWS Region, where the programmatic access for the AWS CLI is created, does not match the region where the instance\nlives.\nThe AWS Access Key Id used to configure the AWS CLI is invalid.\nThe AWS Region name used to configure the AWS CLI does not match the region where the instance lives.\nIncorrect\nFor general use, the aws configure command is the fastest way to set up your AWS CLI installation. When you enter this command, the AWS\nCLI prompts you for four pieces of information:\n— Access Key ID\n— Secret Access Key\n— AWS Region\n— Output format\n~] Access keys consist of an access key ID and secret access keys are used to sign programmatic requests that you make to AWS.",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization I",
          "A: developer has been instructed to automate the creation of the snapshot of an existing Amazon EC2 instance. The engineer created a script",
          "a: t uses the AWS Command Line Interface (CLI) to run the necessary API call. He is getting an InvalidInstanceID.NotFound error",
          "c: ript is run.",
          "a: t is the most likely cause of the error?",
          "a: ge Id used in running the command for creating a snapshot is incorrect.",
          "A: WS Region, where the programmatic access for the AWS CLI is created, does not match the region where the instance",
          "A: WS Access Key Id used to configure the AWS CLI is invalid.",
          "A: WS Region name used to configure the AWS CLI does not match the region where the instance lives.",
          "c: orrect",
          "a: l use, the aws configure command is the fastest way to set up your AWS CLI installation. When you enter this command, the AWS",
          "C: LI prompts you for four pieces of information:",
          "A: ccess Key ID",
          "c: ret Access Key",
          "A: WS Region",
          "a: t",
          "A: ccess keys consist of an access key ID and secret access keys are used to sign programmatic requests that you make to AWS."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "4. QUESTION\nCategory: CDA - Troubleshooting and Optimization I\nA developer has been instructed to automate the creation of the snapshot of an existing Amazon EC2 instance. The engineer created a script\nthat uses the AWS Command Line Interface (CLI) to run the necessary API call. He is getting an InvalidInstanceID.NotFound error\nwhenever the script is run.\nWhat is the most likely cause of the error?\n® The Image Id used in running the command for creating a snapshot is incorrect.\nThe AWS Region, where the programmatic access for the AWS CLI is created, does not match the region where the instance\nlives.\nThe AWS Access Key Id used to configure the AWS CLI is invalid.\nThe AWS Region name used to configure the AWS CLI does not match the region where the instance lives.\nIncorrect\nFor general use, the aws configure command is the fastest way to set up your AWS CLI installation. When you enter this command, the AWS\nCLI prompts you for four pieces of information:\n— Access Key ID\n— Secret Access Key\n— AWS Region\n— Output format\n~] Access keys consist of an access key ID and secret access keys are used to sign programmatic requests that you make to AWS."
      },
      "tags": {
        "services": [
          "EC2",
          "Config",
          "ECR"
        ],
        "domains": [],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 115808.png",
      "parsed": {
        "question": "[\nCategory: CDA - Deployment\nA company uses AWS CodeDeploy in their CI/CD pipeline to handle in-place deployments of their web application on EC2 instances. Recently,\na new version of the application was pushed, which contained a code regression. The deployment group is configured with automatic rollback.\nWhat happens if the deployment of the new version fails?\nCodeDeploy reroutes traffic back to the blue environment and terminates the green environment.\n® CodeDeploy pauses the current deployment, restores the last stable version from S3, and uses the deployment ID of the most\nrecent deployment with a SUCCEEDED status.\nCodeDeploy reverts the EC2 instance to a previous AMI snapshot taken during the last successful deployment.\nCodeDeploy redeploys the last known good version of an application with a new deployment ID.\nIncorrect\nIn AWS CodeDeploy, during an in-place deployment, the previous version of the application on each compute resource is stopped, and then the\nlatest application revision is installed. After installation, the new version of the application is started and validated.\nRollbacks\nEnable deployment rollbacks for this deployment group\nRoll back when a deployment fails\n= Roll back when alarm thresholds are met",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: company uses AWS CodeDeploy in their CI/CD pipeline to handle in-place deployments of their web application on EC2 instances. Recently,",
          "a: new version of the application was pushed, which contained a code regression. The deployment group is configured with automatic rollback.",
          "a: t happens if the deployment of the new version fails?",
          "C: odeDeploy reroutes traffic back to the blue environment and terminates the green environment.",
          "C: odeDeploy pauses the current deployment, restores the last stable version from S3, and uses the deployment ID of the most",
          "c: ent deployment with a SUCCEEDED status.",
          "C: odeDeploy reverts the EC2 instance to a previous AMI snapshot taken during the last successful deployment.",
          "C: odeDeploy redeploys the last known good version of an application with a new deployment ID.",
          "c: orrect",
          "A: WS CodeDeploy, during an in-place deployment, the previous version of the application on each compute resource is stopped, and then the",
          "a: test application revision is installed. After installation, the new version of the application is started and validated.",
          "b: acks",
          "a: ble deployment rollbacks for this deployment group",
          "b: ack when a deployment fails",
          "b: ack when alarm thresholds are met"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "5. QUESTION [\nCategory: CDA - Deployment\nA company uses AWS CodeDeploy in their CI/CD pipeline to handle in-place deployments of their web application on EC2 instances. Recently,\na new version of the application was pushed, which contained a code regression. The deployment group is configured with automatic rollback.\nWhat happens if the deployment of the new version fails?\nCodeDeploy reroutes traffic back to the blue environment and terminates the green environment.\n® CodeDeploy pauses the current deployment, restores the last stable version from S3, and uses the deployment ID of the most\nrecent deployment with a SUCCEEDED status.\nCodeDeploy reverts the EC2 instance to a previous AMI snapshot taken during the last successful deployment.\nCodeDeploy redeploys the last known good version of an application with a new deployment ID.\nIncorrect\nIn AWS CodeDeploy, during an in-place deployment, the previous version of the application on each compute resource is stopped, and then the\nlatest application revision is installed. After installation, the new version of the application is started and validated.\nRollbacks\nEnable deployment rollbacks for this deployment group\nRoll back when a deployment fails\n= Roll back when alarm thresholds are met"
      },
      "tags": {
        "services": [
          "EC2",
          "S3",
          "CodeDeploy",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment"
        ],
        "keywords": [
          "deployment",
          "CI/CD",
          "rollback"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 115932.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services KE\nA developer is debugging an issue in an AWS Lambda-based application. To save time searching through logs, the developer wants the\nfunction to return the corresponding log location of an invocation request.\nWhich approach should the developer take with the least amount of effort?\n® Extract the invocation request id from the Event object of the handler. Call the FilterLogEvents API and use the\nrequest id to filter results.\nExtract the log stream name from the context object of the handler function.\nExtract the log stream name from the Event object of the handler function.\nExtract the invocation request id from the context object of the handler function. Then, call the FilterLogEvents API\nand pass the request id to filter results.\nIncorrect\nWhen Lambda runs your function, it passes a context object to the handler. This object provides methods and properties that provide information\nabout the invocation, function, and execution environment. One of the properties that you can get from the context object is the\nlog_stream_name which gives the log location of a function instance.\nWindow\n= lambda_function.py 8\nlambda_handler(event, context):\n1 Anta",
        "options": [
          "C: ategory: CDA - Development with AWS Services KE",
          "A: developer is debugging an issue in an AWS Lambda-based application. To save time searching through logs, the developer wants the",
          "c: tion to return the corresponding log location of an invocation request.",
          "c: h approach should the developer take with the least amount of effort?",
          "a: ct the invocation request id from the Event object of the handler. Call the FilterLogEvents API and use the",
          "d: to filter results.",
          "a: ct the log stream name from the context object of the handler function.",
          "a: ct the log stream name from the Event object of the handler function.",
          "a: ct the invocation request id from the context object of the handler function. Then, call the FilterLogEvents API",
          "a: nd pass the request id to filter results.",
          "c: orrect",
          "a: mbda runs your function, it passes a context object to the handler. This object provides methods and properties that provide information",
          "a: bout the invocation, function, and execution environment. One of the properties that you can get from the context object is the",
          "a: m_name which gives the log location of a function instance.",
          "d: ow",
          "a: mbda_function.py 8",
          "a: mbda_handler(event, context):",
          "A: nta"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "8. QUESTION\nCategory: CDA - Development with AWS Services KE\nA developer is debugging an issue in an AWS Lambda-based application. To save time searching through logs, the developer wants the\nfunction to return the corresponding log location of an invocation request.\nWhich approach should the developer take with the least amount of effort?\n® Extract the invocation request id from the Event object of the handler. Call the FilterLogEvents API and use the\nrequest id to filter results.\nExtract the log stream name from the context object of the handler function.\nExtract the log stream name from the Event object of the handler function.\nExtract the invocation request id from the context object of the handler function. Then, call the FilterLogEvents API\nand pass the request id to filter results.\nIncorrect\nWhen Lambda runs your function, it passes a context object to the handler. This object provides methods and properties that provide information\nabout the invocation, function, and execution environment. One of the properties that you can get from the context object is the\nlog_stream_name which gives the log location of a function instance.\nWindow\n= lambda_function.py 8\nlambda_handler(event, context):\n1 Anta"
      },
      "tags": {
        "services": [
          "Lambda"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 120141.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA developer is testing a Lambda function that was created from a CloudFormation template. While the function executes without errors, it isn’t E\ngenerating logs in Amazon CloudWatch Logs, and the developer cannot find associated log streams or log groups.\nUpon inspection, the following observations were made:\no The function's code contains appropriate logging statements.\no The Lambda function is associated with an execution role that establishes a trusted relationship with the Lambda service; however, this role\nhas no permissions assigned.\no The Lambda function does not have any resource-based policies.\nWhich configuration must be done to resolve the issue?\n@® Associate the function with a resource-based policy that contains the 1ogs:PutLogEvents permissions.\nUpdate the execution role by adding the CloudWatchLambdaInsightsExecutionRolePolicy managed policy.\nAssociate the function with a resource-based policy that contains the 1logs:CreatelogGroup , logs:CreateLogStream ,\nand logs:PutLogEvents permissions.\nUpdate the execution role by adding the AwsLambdaBasicExecutionRole managed policy.\nIncorrect\nA Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS\nmrt remo mrmed rmmmt trem Foe mares) om vrs vvmtr lod mrt mam rmrrrm rms 1bimrm vem limit lnm rmmrrvmtmemt mem tom mrmrmed Tm 2m A vvmommromrm PO ms sed A Tmt mb mem ed rem] mm od",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: developer is testing a Lambda function that was created from a CloudFormation template. While the function executes without errors, it isn’t E",
          "a: ting logs in Amazon CloudWatch Logs, and the developer cannot find associated log streams or log groups.",
          "c: tion, the following observations were made:",
          "c: tion's code contains appropriate logging statements.",
          "a: mbda function is associated with an execution role that establishes a trusted relationship with the Lambda service; however, this role",
          "a: s no permissions assigned.",
          "a: mbda function does not have any resource-based policies.",
          "c: h configuration must be done to resolve the issue?",
          "A: ssociate the function with a resource-based policy that contains the 1ogs:PutLogEvents permissions.",
          "d: ate the execution role by adding the CloudWatchLambdaInsightsExecutionRolePolicy managed policy.",
          "A: ssociate the function with a resource-based policy that contains the 1logs:CreatelogGroup , logs:CreateLogStream ,",
          "a: nd logs:PutLogEvents permissions.",
          "d: ate the execution role by adding the AwsLambdaBasicExecutionRole managed policy.",
          "c: orrect",
          "A: Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS",
          "d: rmmmt trem Foe mares) om vrs vvmtr lod mrt mam rmrrrm rms 1bimrm vem limit lnm rmmrrvmtmemt mem tom mrmrmed Tm 2m A vvmommromrm PO ms sed A Tmt mb mem ed rem] mm od"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "9. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA developer is testing a Lambda function that was created from a CloudFormation template. While the function executes without errors, it isn’t E\ngenerating logs in Amazon CloudWatch Logs, and the developer cannot find associated log streams or log groups.\nUpon inspection, the following observations were made:\no The function's code contains appropriate logging statements.\no The Lambda function is associated with an execution role that establishes a trusted relationship with the Lambda service; however, this role\nhas no permissions assigned.\no The Lambda function does not have any resource-based policies.\nWhich configuration must be done to resolve the issue?\n@® Associate the function with a resource-based policy that contains the 1ogs:PutLogEvents permissions.\nUpdate the execution role by adding the CloudWatchLambdaInsightsExecutionRolePolicy managed policy.\nAssociate the function with a resource-based policy that contains the 1logs:CreatelogGroup , logs:CreateLogStream ,\nand logs:PutLogEvents permissions.\nUpdate the execution role by adding the AwsLambdaBasicExecutionRole managed policy.\nIncorrect\nA Lambda function's execution role is an AWS Identity and Access Management (IAM) role that grants the function permission to access AWS\nmrt remo mrmed rmmmt trem Foe mares) om vrs vvmtr lod mrt mam rmrrrm rms 1bimrm vem limit lnm rmmrrvmtmemt mem tom mrmrmed Tm 2m A vvmommromrm PO ms sed A Tmt mb mem ed rem] mm od"
      },
      "tags": {
        "services": [
          "Lambda",
          "IAM",
          "CloudFormation",
          "CloudWatch",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "logging",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 120337.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA company wants to know how its monolithic application will perform on a microservice architecture. The Lead Developer has deployed the i\napplication on Amazon ECS using the EC2 launch type. He terminated the container instance after testing; however, the container instance still\nappears as a resource in the ECS cluster.\nWhat is the possible cause of this?\nAfter terminating the container instance in the running state, the container instance must be manually deregistered in the\nAmazon ECS Console.\n® After terminating the container instance in the stopped state, the container instance must be manually deregistered in the\nAmazon EC2 Console since it was launched using the EC2 launch type.\nWhen a container instance is terminated in the running state, the container instance is not automatically deregistered from\nthe cluster.\nWhen a container instance is terminated in the stopped state, the container instance is not automatically deregistered from\nthe cluster.\nIncorrect\nIf you terminate a container instance in the RUNNING state, that container instance is automatically removed or deregistered from the cluster.\nHowever, if you terminate a container instance in the STOPPED state, that container instance isn't automatically removed from the cluster.\nTo deregister your container instance from the cluster, you should deregister it after terminating it in the STOPPED state by using the Amazon ECS\nConsole or AWS Command Line Interface. The deregistered container instance will no longer appear as a resource in your Amazon ECS cluster.\nHence, the correct answer is: When a container instance is terminated in the stopped state, the container instance is not automatically\n- deregistered from the cluster.",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: company wants to know how its monolithic application will perform on a microservice architecture. The Lead Developer has deployed the i",
          "a: pplication on Amazon ECS using the EC2 launch type. He terminated the container instance after testing; however, the container instance still",
          "a: ppears as a resource in the ECS cluster.",
          "a: t is the possible cause of this?",
          "A: fter terminating the container instance in the running state, the container instance must be manually deregistered in the",
          "A: mazon ECS Console.",
          "A: fter terminating the container instance in the stopped state, the container instance must be manually deregistered in the",
          "A: mazon EC2 Console since it was launched using the EC2 launch type.",
          "a: container instance is terminated in the running state, the container instance is not automatically deregistered from",
          "c: luster.",
          "a: container instance is terminated in the stopped state, the container instance is not automatically deregistered from",
          "c: luster.",
          "c: orrect",
          "a: te a container instance in the RUNNING state, that container instance is automatically removed or deregistered from the cluster.",
          "a: te a container instance in the STOPPED state, that container instance isn't automatically removed from the cluster.",
          "d: eregister your container instance from the cluster, you should deregister it after terminating it in the STOPPED state by using the Amazon ECS",
          "C: onsole or AWS Command Line Interface. The deregistered container instance will no longer appear as a resource in your Amazon ECS cluster.",
          "c: e, the correct answer is: When a container instance is terminated in the stopped state, the container instance is not automatically",
          "d: eregistered from the cluster."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "12. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA company wants to know how its monolithic application will perform on a microservice architecture. The Lead Developer has deployed the i\napplication on Amazon ECS using the EC2 launch type. He terminated the container instance after testing; however, the container instance still\nappears as a resource in the ECS cluster.\nWhat is the possible cause of this?\nAfter terminating the container instance in the running state, the container instance must be manually deregistered in the\nAmazon ECS Console.\n® After terminating the container instance in the stopped state, the container instance must be manually deregistered in the\nAmazon EC2 Console since it was launched using the EC2 launch type.\nWhen a container instance is terminated in the running state, the container instance is not automatically deregistered from\nthe cluster.\nWhen a container instance is terminated in the stopped state, the container instance is not automatically deregistered from\nthe cluster.\nIncorrect\nIf you terminate a container instance in the RUNNING state, that container instance is automatically removed or deregistered from the cluster.\nHowever, if you terminate a container instance in the STOPPED state, that container instance isn't automatically removed from the cluster.\nTo deregister your container instance from the cluster, you should deregister it after terminating it in the STOPPED state by using the Amazon ECS\nConsole or AWS Command Line Interface. The deregistered container instance will no longer appear as a resource in your Amazon ECS cluster.\nHence, the correct answer is: When a container instance is terminated in the stopped state, the container instance is not automatically\n- deregistered from the cluster."
      },
      "tags": {
        "services": [
          "EC2",
          "ECS"
        ],
        "domains": [],
        "keywords": [
          "container"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 120502.png",
      "parsed": {
        "question": "KE\nCategory: CDA - Troubleshooting and Optimization\nAn application that runs on a local Linux server is migrated to a Lambda function to save costs. The Lambda function shows an Unable to\nimport module error when invoked.\nAs the developer, how can you fix the error?\nRun a Linux command inside the Lambda function to install the missing modules.\n® Download the missing modules in the lib directory. Package all files under the lib directory into a ZIP file and upload it to AWS\nLambda.\nImport the missing modules in the Lambda code. The Lambda function will automatically install them when invoked.\nInstall the missing modules locally to your application's folder. Package the folder into a ZIP file and upload it to AWS Lambda.\nIncorrect\nModuleNotFoundError and Module cannot be loaded are common errors for Lambda functions. These errors are usually due to an\nincorrect folder structure or file permissions with the deployment package .zip file.\nTo fix this error:\n1. Install all dependency modules local to the function project.\n2. Build the deployment package by zipping up the project folder for deployment to Lambda.\n3. Upload the deployment package.\n1 BYE aco a",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: n application that runs on a local Linux server is migrated to a Lambda function to save costs. The Lambda function shows an Unable to",
          "d: ule error when invoked.",
          "A: s the developer, how can you fix the error?",
          "a: Linux command inside the Lambda function to install the missing modules.",
          "D: ownload the missing modules in the lib directory. Package all files under the lib directory into a ZIP file and upload it to AWS",
          "a: mbda.",
          "d: ules in the Lambda code. The Lambda function will automatically install them when invoked.",
          "a: ll the missing modules locally to your application's folder. Package the folder into a ZIP file and upload it to AWS Lambda.",
          "c: orrect",
          "d: uleNotFoundError and Module cannot be loaded are common errors for Lambda functions. These errors are usually due to an",
          "c: orrect folder structure or file permissions with the deployment package .zip file.",
          "a: ll all dependency modules local to the function project.",
          "B: uild the deployment package by zipping up the project folder for deployment to Lambda.",
          "a: d the deployment package.",
          "B: YE aco a"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "14. QUESTION KE\nCategory: CDA - Troubleshooting and Optimization\nAn application that runs on a local Linux server is migrated to a Lambda function to save costs. The Lambda function shows an Unable to\nimport module error when invoked.\nAs the developer, how can you fix the error?\nRun a Linux command inside the Lambda function to install the missing modules.\n® Download the missing modules in the lib directory. Package all files under the lib directory into a ZIP file and upload it to AWS\nLambda.\nImport the missing modules in the Lambda code. The Lambda function will automatically install them when invoked.\nInstall the missing modules locally to your application's folder. Package the folder into a ZIP file and upload it to AWS Lambda.\nIncorrect\nModuleNotFoundError and Module cannot be loaded are common errors for Lambda functions. These errors are usually due to an\nincorrect folder structure or file permissions with the deployment package .zip file.\nTo fix this error:\n1. Install all dependency modules local to the function project.\n2. Build the deployment package by zipping up the project folder for deployment to Lambda.\n3. Upload the deployment package.\n1 BYE aco a"
      },
      "tags": {
        "services": [
          "Lambda"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 120633.png",
      "parsed": {
        "question": "B\nCategory: CDA - Troubleshooting and Optimization\nAn application has a feature that displays GIFs based on keyword inputs. The code streams random GIF links from an external API to your local\nmachine. When run, the applications process takes longer than expected. You are suspecting that the new function sendRequest() you\nadded is the culprit.\nWhich of the following actions should you do to determine the latency of the function?\n® Using AWS X-Ray, disable sampling to efficiently trace all requests for calls.\nUse CloudTrail to record and store event logs for actions made by your function.\nUsing AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function.\nUsing CloudWatch, troubleshoot the issue by checking the logs.\nIncorrect\nAWS X-ray analyzes and debugs production, distributed applications, such as those built using a microservices architecture. With, X-Ray, you can\nidentify performance bottlenecks, edge case errors, and other hard to detect issues.\nA segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details\nabout downstream calls that your application made to fulfill the original request. A subsegment can contain additional details about a call to an\nAWS service, an external HTTP API, or an SQL database. You can define arbitrary subsegments to instrument specific functions or lines of\ncode in your application.\nSubsegments extend a trace’s segment with details about work done in order to serve a request. Each time you make a call with an instrumented\n- client, the X-Ray SDK records the information generated in a subsegment. You can create additional subsegments to group other subsegments, to",
        "options": [
          "B: Category: CDA - Troubleshooting and Optimization",
          "A: n application has a feature that displays GIFs based on keyword inputs. The code streams random GIF links from an external API to your local",
          "a: chine. When run, the applications process takes longer than expected. You are suspecting that the new function sendRequest() you",
          "a: dded is the culprit.",
          "c: h of the following actions should you do to determine the latency of the function?",
          "A: WS X-Ray, disable sampling to efficiently trace all requests for calls.",
          "C: loudTrail to record and store event logs for actions made by your function.",
          "A: WS X-Ray, define an arbitrary subsegment inside the code to instrument the function.",
          "C: loudWatch, troubleshoot the issue by checking the logs.",
          "c: orrect",
          "A: WS X-ray analyzes and debugs production, distributed applications, such as those built using a microservices architecture. With, X-Ray, you can",
          "d: entify performance bottlenecks, edge case errors, and other hard to detect issues.",
          "A: segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details",
          "a: bout downstream calls that your application made to fulfill the original request. A subsegment can contain additional details about a call to an",
          "A: WS service, an external HTTP API, or an SQL database. You can define arbitrary subsegments to instrument specific functions or lines of",
          "c: ode in your application.",
          "b: segments extend a trace’s segment with details about work done in order to serve a request. Each time you make a call with an instrumented",
          "c: lient, the X-Ray SDK records the information generated in a subsegment. You can create additional subsegments to group other subsegments, to"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "16. QUESTION B\nCategory: CDA - Troubleshooting and Optimization\nAn application has a feature that displays GIFs based on keyword inputs. The code streams random GIF links from an external API to your local\nmachine. When run, the applications process takes longer than expected. You are suspecting that the new function sendRequest() you\nadded is the culprit.\nWhich of the following actions should you do to determine the latency of the function?\n® Using AWS X-Ray, disable sampling to efficiently trace all requests for calls.\nUse CloudTrail to record and store event logs for actions made by your function.\nUsing AWS X-Ray, define an arbitrary subsegment inside the code to instrument the function.\nUsing CloudWatch, troubleshoot the issue by checking the logs.\nIncorrect\nAWS X-ray analyzes and debugs production, distributed applications, such as those built using a microservices architecture. With, X-Ray, you can\nidentify performance bottlenecks, edge case errors, and other hard to detect issues.\nA segment can break down the data about the work done into subsegments. Subsegments provide more granular timing information and details\nabout downstream calls that your application made to fulfill the original request. A subsegment can contain additional details about a call to an\nAWS service, an external HTTP API, or an SQL database. You can define arbitrary subsegments to instrument specific functions or lines of\ncode in your application.\nSubsegments extend a trace’s segment with details about work done in order to serve a request. Each time you make a call with an instrumented\n- client, the X-Ray SDK records the information generated in a subsegment. You can create additional subsegments to group other subsegments, to"
      },
      "tags": {
        "services": [
          "RDS",
          "X-Ray",
          "CloudWatch",
          "CloudTrail",
          "SAM"
        ],
        "domains": [
          "Security",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "microservices",
          "performance"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 120711.png",
      "parsed": {
        "question": "[\nCategory: CDA - Development with AWS Services\nA developer needs to build a queueing mechanism for an application that will run on AWS. The application is expected to consume SQS\nmessages that are larger than 256 KB and up to 1 GB in size.\nHow should the developer manage the SQS messages?\n® Use Amazon S3 and the Amazon SQS Console\nUse Amazon S3 and the Amazon SQS CLI\nUse Amazon S3 and the Amazon SQS HTTP API\nUse Amazon S3 and the Amazon SQS Extended Client Library for Java\nIncorrect\nTo manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the\nAmazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application\nrequires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for\nstoring your data.\nYou can use the Amazon SQS Extended Client Library for Java to do the following:\n— Specify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB\n— Send a message that references a single message object stored in an S3 bucket\n| oo N oo N",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer needs to build a queueing mechanism for an application that will run on AWS. The application is expected to consume SQS",
          "a: ges that are larger than 256 KB and up to 1 GB in size.",
          "d: the developer manage the SQS messages?",
          "A: mazon S3 and the Amazon SQS Console",
          "A: mazon S3 and the Amazon SQS CLI",
          "A: mazon S3 and the Amazon SQS HTTP API",
          "A: mazon S3 and the Amazon SQS Extended Client Library for Java",
          "c: orrect",
          "a: nage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the",
          "A: mazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application",
          "a: tedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for",
          "d: ata.",
          "c: an use the Amazon SQS Extended Client Library for Java to do the following:",
          "c: ify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB",
          "d: a message that references a single message object stored in an S3 bucket"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "i\n17. QUESTION [\nCategory: CDA - Development with AWS Services\nA developer needs to build a queueing mechanism for an application that will run on AWS. The application is expected to consume SQS\nmessages that are larger than 256 KB and up to 1 GB in size.\nHow should the developer manage the SQS messages?\n® Use Amazon S3 and the Amazon SQS Console\nUse Amazon S3 and the Amazon SQS CLI\nUse Amazon S3 and the Amazon SQS HTTP API\nUse Amazon S3 and the Amazon SQS Extended Client Library for Java\nIncorrect\nTo manage large Amazon Simple Queue Service (Amazon SQS) messages, you can use Amazon Simple Storage Service (Amazon S3) and the\nAmazon SQS Extended Client Library for Java. This is especially useful for storing and consuming messages up to 2 GB. Unless your application\nrequires repeatedly creating queues and leaving them inactive or storing large amounts of data in your queues, consider using Amazon S3 for\nstoring your data.\nYou can use the Amazon SQS Extended Client Library for Java to do the following:\n— Specify whether messages are always stored in Amazon S3 or only when the size of a message exceeds 256 KB\n— Send a message that references a single message object stored in an S3 bucket\n| oo N oo N"
      },
      "tags": {
        "services": [
          "S3",
          "SQS"
        ],
        "domains": [
          "Development with AWS Services"
        ],
        "keywords": [
          "queue"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 120841.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nA developer uses AWS Serverless Application Model (SAM) in a local machine to create a serverless Python application. After defining the [\nrequired dependencies in the requirements.txt file, the developer is now ready to test and deploy.\nWhat are the steps to successfully deploy the application?\nUpload and build the SAM template in an EC2 instance. Run the sam deploy command to package and deploy the SAM\ntemplate.\n® Run the sam init command. Build the SAM template in the local machine and call the sam deploy command to package\nand deploy the SAM template from an S3 bucket.\nBuild the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template\nfrom an S3 bucket.\nBuild the SAM template in the local machine. Run the sam deploy command to package and deploy the SAM template from\nAWS CodePipeline.\nIncorrect\nHere are the SAM CLI commands needed to deploy serverless applications:\nsam init - Initializes a serverless application with an AWS SAM template. The template provides a folder structure for your Lambda functions\nand is connected to an event source such as APIs, S3 buckets, or DynamoDB tables. This application includes everything you need to get started\nand to eventually extend it into a production-scale application.\nsam build -The sam build command builds any dependencies that your application has, and copies your application source code to\nfolders under .aws-sam/build to be zipped and uploaded to Lambda.",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: developer uses AWS Serverless Application Model (SAM) in a local machine to create a serverless Python application. After defining the [",
          "d: dependencies in the requirements.txt file, the developer is now ready to test and deploy.",
          "a: t are the steps to successfully deploy the application?",
          "a: d and build the SAM template in an EC2 instance. Run the sam deploy command to package and deploy the SAM",
          "a: te.",
          "a: m init command. Build the SAM template in the local machine and call the sam deploy command to package",
          "a: nd deploy the SAM template from an S3 bucket.",
          "B: uild the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template",
          "a: n S3 bucket.",
          "B: uild the SAM template in the local machine. Run the sam deploy command to package and deploy the SAM template from",
          "A: WS CodePipeline.",
          "c: orrect",
          "a: re the SAM CLI commands needed to deploy serverless applications:",
          "a: m init - Initializes a serverless application with an AWS SAM template. The template provides a folder structure for your Lambda functions",
          "a: nd is connected to an event source such as APIs, S3 buckets, or DynamoDB tables. This application includes everything you need to get started",
          "a: nd to eventually extend it into a production-scale application.",
          "a: m build -The sam build command builds any dependencies that your application has, and copies your application source code to",
          "d: ers under .aws-sam/build to be zipped and uploaded to Lambda."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "19. QUESTION\nCategory: CDA - Deployment\nA developer uses AWS Serverless Application Model (SAM) in a local machine to create a serverless Python application. After defining the [\nrequired dependencies in the requirements.txt file, the developer is now ready to test and deploy.\nWhat are the steps to successfully deploy the application?\nUpload and build the SAM template in an EC2 instance. Run the sam deploy command to package and deploy the SAM\ntemplate.\n® Run the sam init command. Build the SAM template in the local machine and call the sam deploy command to package\nand deploy the SAM template from an S3 bucket.\nBuild the SAM template in the local machine and call the sam deploy command to package and deploy the SAM template\nfrom an S3 bucket.\nBuild the SAM template in the local machine. Run the sam deploy command to package and deploy the SAM template from\nAWS CodePipeline.\nIncorrect\nHere are the SAM CLI commands needed to deploy serverless applications:\nsam init - Initializes a serverless application with an AWS SAM template. The template provides a folder structure for your Lambda functions\nand is connected to an event source such as APIs, S3 buckets, or DynamoDB tables. This application includes everything you need to get started\nand to eventually extend it into a production-scale application.\nsam build -The sam build command builds any dependencies that your application has, and copies your application source code to\nfolders under .aws-sam/build to be zipped and uploaded to Lambda."
      },
      "tags": {
        "services": [
          "EC2",
          "Lambda",
          "S3",
          "DynamoDB",
          "CodePipeline",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 121236.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer is building a serverless URL shortener using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application code as |\nwell as the stack that defines the cloud resources should be written in Python. The code should also be reusable in case an update must be\ndone to the stack.\nWhich of the following actions must be done by the developer to meet the requirements above?\n® Use AWS SDK for Python (boto3) to build the stack. Then, use Python as the runtime environment in writing the application\nlogic on Lambda.\nUse AWS CloudFormation to build the stack. Then, use Python as the runtime environment in writing the application logic on\nLambda.\nUse AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.\nUse AWS CloudShell to build the stack. Then, use Python as the runtime environment in writing the application logic on\nLambda.\nIncorrect\nThe AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to model and provision your cloud application\nresources using familiar programming languages. The AWS CDK has first-class support for TypeScript, JavaScript, Python, Java, and C#. The AWS\nCDK can also update your deployed resources after you modify your app using the appropriate CDK commands.\nConstruct Assembly\nCompiler Processor\nLanguage\nP—",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer is building a serverless URL shortener using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application code as |",
          "a: s the stack that defines the cloud resources should be written in Python. The code should also be reusable in case an update must be",
          "d: one to the stack.",
          "c: h of the following actions must be done by the developer to meet the requirements above?",
          "A: WS SDK for Python (boto3) to build the stack. Then, use Python as the runtime environment in writing the application",
          "c: on Lambda.",
          "A: WS CloudFormation to build the stack. Then, use Python as the runtime environment in writing the application logic on",
          "a: mbda.",
          "A: WS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.",
          "A: WS CloudShell to build the stack. Then, use Python as the runtime environment in writing the application logic on",
          "a: mbda.",
          "c: orrect",
          "A: WS Cloud Development Kit (AWS CDK) is an open-source software development framework to model and provision your cloud application",
          "c: es using familiar programming languages. The AWS CDK has first-class support for TypeScript, JavaScript, Python, Java, and C#. The AWS",
          "C: DK can also update your deployed resources after you modify your app using the appropriate CDK commands.",
          "C: onstruct Assembly",
          "C: ompiler Processor",
          "a: nguage"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "20. QUESTION\nCategory: CDA - Development with AWS Services\nA developer is building a serverless URL shortener using Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. The application code as |\nwell as the stack that defines the cloud resources should be written in Python. The code should also be reusable in case an update must be\ndone to the stack.\nWhich of the following actions must be done by the developer to meet the requirements above?\n® Use AWS SDK for Python (boto3) to build the stack. Then, use Python as the runtime environment in writing the application\nlogic on Lambda.\nUse AWS CloudFormation to build the stack. Then, use Python as the runtime environment in writing the application logic on\nLambda.\nUse AWS CDK to build the stack. Then, use Python as the runtime environment in writing the application logic on Lambda.\nUse AWS CloudShell to build the stack. Then, use Python as the runtime environment in writing the application logic on\nLambda.\nIncorrect\nThe AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to model and provision your cloud application\nresources using familiar programming languages. The AWS CDK has first-class support for TypeScript, JavaScript, Python, Java, and C#. The AWS\nCDK can also update your deployed resources after you modify your app using the appropriate CDK commands.\nConstruct Assembly\nCompiler Processor\nLanguage\nP—"
      },
      "tags": {
        "services": [
          "Lambda",
          "DynamoDB",
          "API Gateway",
          "CloudFormation"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 121354.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA company has a serverless application on AWS. They are using AWS Lambda for business logic and Amazon API Gateway for handling client |\nrequests. They have published a version of the AccountService:Prod function with the alias AccountService:Beta . The internal team\nwants to test these updates before promoting them to production without impacting live users.\nWhich configuration should the company take?\nCreate a ‘Beta’ stage in Amazon API Gateway. In the Lambda function configuration, add a condition to the\n@® AccountService:Prod function that checks for an environment variable and, if set to ‘Beta’, invokes the\nAccountService:Beta alias instead\nModify the existing 'Prod' stage in Amazon API Gateway to create a new stage variable that references the Lambda functions\nin Prod and Beta.\nUpdate the integration request settings in the 'Prod' stage of the Amazon API Gateway to select the Lambda function alias\nbased on a query parameter.\nCreate a new stage named 'Beta' in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod\nand Beta.\nIncorrect\nWith deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage\nvariables you can configure an API deployment stage to interact with different backend endpoints.\nAPI Gateway evaluates which function to invoke\nMyfunction:${stagevariables.lambdaAlias}\n7",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: company has a serverless application on AWS. They are using AWS Lambda for business logic and Amazon API Gateway for handling client |",
          "a: ve published a version of the AccountService:Prod function with the alias AccountService:Beta . The internal team",
          "a: nts to test these updates before promoting them to production without impacting live users.",
          "c: h configuration should the company take?",
          "C: reate a ‘Beta’ stage in Amazon API Gateway. In the Lambda function configuration, add a condition to the",
          "A: ccountService:Prod function that checks for an environment variable and, if set to ‘Beta’, invokes the",
          "A: ccountService:Beta alias instead",
          "d: ify the existing 'Prod' stage in Amazon API Gateway to create a new stage variable that references the Lambda functions",
          "d: and Beta.",
          "d: ate the integration request settings in the 'Prod' stage of the Amazon API Gateway to select the Lambda function alias",
          "b: ased on a query parameter.",
          "C: reate a new stage named 'Beta' in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod",
          "a: nd Beta.",
          "c: orrect",
          "d: eployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage",
          "a: riables you can configure an API deployment stage to interact with different backend endpoints.",
          "A: PI Gateway evaluates which function to invoke",
          "c: tion:${stagevariables.lambdaAlias}"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "22. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA company has a serverless application on AWS. They are using AWS Lambda for business logic and Amazon API Gateway for handling client |\nrequests. They have published a version of the AccountService:Prod function with the alias AccountService:Beta . The internal team\nwants to test these updates before promoting them to production without impacting live users.\nWhich configuration should the company take?\nCreate a ‘Beta’ stage in Amazon API Gateway. In the Lambda function configuration, add a condition to the\n@® AccountService:Prod function that checks for an environment variable and, if set to ‘Beta’, invokes the\nAccountService:Beta alias instead\nModify the existing 'Prod' stage in Amazon API Gateway to create a new stage variable that references the Lambda functions\nin Prod and Beta.\nUpdate the integration request settings in the 'Prod' stage of the Amazon API Gateway to select the Lambda function alias\nbased on a query parameter.\nCreate a new stage named 'Beta' in Amazon API Gateway and use stage variables to reference the Lambda functions in Prod\nand Beta.\nIncorrect\nWith deployment stages in API Gateway, you can manage multiple release stages for each API, such as alpha, beta, and production. Using stage\nvariables you can configure an API deployment stage to interact with different backend endpoints.\nAPI Gateway evaluates which function to invoke\nMyfunction:${stagevariables.lambdaAlias}\n7"
      },
      "tags": {
        "services": [
          "Lambda",
          "API Gateway",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 121607.png",
      "parsed": {
        "question": "[\nCategory: CDA - Troubleshooting and Optimization\nA developer is building a serverless workflow using Step Functions. The developer has to implement a solution that will help the State Machine\nhandle and recover from state exception errors.\nWhich of the following actions should the developer do?\nUse a Catch field inside the application code to capture the exception error. Then, use a Retry field in the state definition.\nUse Catch and Retry fields in the state machine definition.\nUse Catch and Retry fields inside the application code.\n® Use a try and catch logic inside the application code to capture the exception error. Then, use a Retry field in the state\ndefinition.\nIncorrect\nAWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into\nbusiness-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that\nmaintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by\nyour business logic.\n©) (©) (©) (©)\n& & o> &\n| o, | TOD HEB &0\nC._ 1 31 1] 31",
        "options": [
          "a: 4 4 A 4",
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: developer is building a serverless workflow using Step Functions. The developer has to implement a solution that will help the State Machine",
          "a: ndle and recover from state exception errors.",
          "c: h of the following actions should the developer do?",
          "a: Catch field inside the application code to capture the exception error. Then, use a Retry field in the state definition.",
          "C: atch and Retry fields in the state machine definition.",
          "C: atch and Retry fields inside the application code.",
          "a: try and catch logic inside the application code to capture the exception error. Then, use a Retry field in the state",
          "d: efinition.",
          "c: orrect",
          "A: WS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into",
          "b: usiness-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that",
          "a: intain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by",
          "b: usiness logic.",
          "D: HEB &0",
          "C: _ 1 31 1] 31"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "a4 4 A 4\n23. QUESTION [\nCategory: CDA - Troubleshooting and Optimization\nA developer is building a serverless workflow using Step Functions. The developer has to implement a solution that will help the State Machine\nhandle and recover from state exception errors.\nWhich of the following actions should the developer do?\nUse a Catch field inside the application code to capture the exception error. Then, use a Retry field in the state definition.\nUse Catch and Retry fields in the state machine definition.\nUse Catch and Retry fields inside the application code.\n® Use a try and catch logic inside the application code to capture the exception error. Then, use a Retry field in the state\ndefinition.\nIncorrect\nAWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into\nbusiness-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that\nmaintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by\nyour business logic.\n©) (©) (©) (©)\n& & o> &\n| o, | TOD HEB &0\nC._ 1 31 1] 31"
      },
      "tags": {
        "services": [
          "Lambda",
          "Step Functions"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "event-driven"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 121908.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA company plans to conduct an online survey to distinguish the users who bought its product from those who didn’t. The survey will be |\nprocessed by Step Functions which comprises four states that will manage the application logic and error handling of the state machine. It is\nrequired to aggregate all the data that passes through the nodes if the process fails.\nWhat should the company do to meet the requirements?\n® Include a Parameters field in the state machine definition to capture the errors. Then, use TtemsPath to include each\nnode's input data with its output.\nInclude a catch field in the state machine definition to capture the errors. Then, use TtemsPath to include each node's\ninput data with its output.\nInclude a catch field in the state machine definition to capture the error. Then, use ResultPath to include each node's\ninput data with its output.\nInclude a Parameters field in the state machine definition to capture the errors. Then, use ResultPath to include each\nnode's input data with its output.\nIncorrect\nThe output of a state can be a copy of its input, the result it produces (for example, the output from a Task state's Lambda function), or a\ncombination of its input and result. Use ResultPath to control which combination of these is passed to the state output.\nYou can use a Catch field to capture the error in a Task and Parallel State. This field's value must be an array of objects, known as catchers.\nA catcher contains the following fields:\nErrorEquals — A non-empty array of strings that match error names.\n]",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: company plans to conduct an online survey to distinguish the users who bought its product from those who didn’t. The survey will be |",
          "c: essed by Step Functions which comprises four states that will manage the application logic and error handling of the state machine. It is",
          "d: to aggregate all the data that passes through the nodes if the process fails.",
          "a: t should the company do to meet the requirements?",
          "c: lude a Parameters field in the state machine definition to capture the errors. Then, use TtemsPath to include each",
          "d: e's input data with its output.",
          "c: lude a catch field in the state machine definition to capture the errors. Then, use TtemsPath to include each node's",
          "d: ata with its output.",
          "c: lude a catch field in the state machine definition to capture the error. Then, use ResultPath to include each node's",
          "d: ata with its output.",
          "c: lude a Parameters field in the state machine definition to capture the errors. Then, use ResultPath to include each",
          "d: e's input data with its output.",
          "c: orrect",
          "a: state can be a copy of its input, the result it produces (for example, the output from a Task state's Lambda function), or a",
          "c: ombination of its input and result. Use ResultPath to control which combination of these is passed to the state output.",
          "c: an use a Catch field to capture the error in a Task and Parallel State. This field's value must be an array of objects, known as catchers.",
          "A: catcher contains the following fields:",
          "a: ls — A non-empty array of strings that match error names."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "24. QUESTION\nCategory: CDA - Development with AWS Services\nA company plans to conduct an online survey to distinguish the users who bought its product from those who didn’t. The survey will be |\nprocessed by Step Functions which comprises four states that will manage the application logic and error handling of the state machine. It is\nrequired to aggregate all the data that passes through the nodes if the process fails.\nWhat should the company do to meet the requirements?\n® Include a Parameters field in the state machine definition to capture the errors. Then, use TtemsPath to include each\nnode's input data with its output.\nInclude a catch field in the state machine definition to capture the errors. Then, use TtemsPath to include each node's\ninput data with its output.\nInclude a catch field in the state machine definition to capture the error. Then, use ResultPath to include each node's\ninput data with its output.\nInclude a Parameters field in the state machine definition to capture the errors. Then, use ResultPath to include each\nnode's input data with its output.\nIncorrect\nThe output of a state can be a copy of its input, the result it produces (for example, the output from a Task state's Lambda function), or a\ncombination of its input and result. Use ResultPath to control which combination of these is passed to the state output.\nYou can use a Catch field to capture the error in a Task and Parallel State. This field's value must be an array of objects, known as catchers.\nA catcher contains the following fields:\nErrorEquals — A non-empty array of strings that match error names.\n]"
      },
      "tags": {
        "services": [
          "Lambda",
          "Step Functions"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 122356.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer is building a ReactJS application that will be hosted on Amazon S3. Amazon Cognito handles the registration and signing of users |\nusing the AWS Software Development Kit (SDK) for JavaScript. The JSON Web Token (JWT) received upon authentication will be stored on the\nbrowser's local storage. After signing in, the application will use the JWT as an authorizer to access an API Gateway endpoint.\nWhat are the steps needed to implement the scenario above? (Select THREE.)\nOn the API Gateway Console, create an authorizer using the Cognito User Pool ID.\nCreate an Amazon Cognito User Pool.\nChoose and set the authentication provider for your website.\nSet the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization.\nSet the name of the header that will be used from the request to the Cognito Identity Pool as a token source for authorization.\nCreate an Amazon Cognito Identity Pool.\nIncorrect\nAs an alternative to using IAM roles and policies or Lambda Authorizers (formerly known as custom authorizers), you can use an Amazon Cognito\nUser Pool to control who can access your API in Amazon API Gateway.\nTo use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO USER POOLS type and then configure\nan API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access\ntoken for the user, and then call the API method with one of the tokens, which are typically set to the request’s Authorization header. The API call\nsucceeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn’t authorized to make the call because the",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer is building a ReactJS application that will be hosted on Amazon S3. Amazon Cognito handles the registration and signing of users |",
          "A: WS Software Development Kit (SDK) for JavaScript. The JSON Web Token (JWT) received upon authentication will be stored on the",
          "b: rowser's local storage. After signing in, the application will use the JWT as an authorizer to access an API Gateway endpoint.",
          "a: t are the steps needed to implement the scenario above? (Select THREE.)",
          "A: PI Gateway Console, create an authorizer using the Cognito User Pool ID.",
          "C: reate an Amazon Cognito User Pool.",
          "C: hoose and set the authentication provider for your website.",
          "a: me of the header that will be used from the request to the Cognito User Pool as a token source for authorization.",
          "a: me of the header that will be used from the request to the Cognito Identity Pool as a token source for authorization.",
          "C: reate an Amazon Cognito Identity Pool.",
          "c: orrect",
          "A: s an alternative to using IAM roles and policies or Lambda Authorizers (formerly known as custom authorizers), you can use an Amazon Cognito",
          "c: ontrol who can access your API in Amazon API Gateway.",
          "a: n Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO USER POOLS type and then configure",
          "a: n API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access",
          "a: nd then call the API method with one of the tokens, which are typically set to the request’s Authorization header. The API call",
          "c: ceeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn’t authorized to make the call because the"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "25. QUESTION\nCategory: CDA - Development with AWS Services\nA developer is building a ReactJS application that will be hosted on Amazon S3. Amazon Cognito handles the registration and signing of users |\nusing the AWS Software Development Kit (SDK) for JavaScript. The JSON Web Token (JWT) received upon authentication will be stored on the\nbrowser's local storage. After signing in, the application will use the JWT as an authorizer to access an API Gateway endpoint.\nWhat are the steps needed to implement the scenario above? (Select THREE.)\nOn the API Gateway Console, create an authorizer using the Cognito User Pool ID.\nCreate an Amazon Cognito User Pool.\nChoose and set the authentication provider for your website.\nSet the name of the header that will be used from the request to the Cognito User Pool as a token source for authorization.\nSet the name of the header that will be used from the request to the Cognito Identity Pool as a token source for authorization.\nCreate an Amazon Cognito Identity Pool.\nIncorrect\nAs an alternative to using IAM roles and policies or Lambda Authorizers (formerly known as custom authorizers), you can use an Amazon Cognito\nUser Pool to control who can access your API in Amazon API Gateway.\nTo use an Amazon Cognito user pool with your API, you must first create an authorizer of the COGNITO USER POOLS type and then configure\nan API method to use that authorizer. After the API is deployed, the client must first sign the user into the user pool, obtain an identity or access\ntoken for the user, and then call the API method with one of the tokens, which are typically set to the request’s Authorization header. The API call\nsucceeds only if the required token is supplied and the supplied token is valid, otherwise, the client isn’t authorized to make the call because the"
      },
      "tags": {
        "services": [
          "Lambda",
          "S3",
          "EBS",
          "API Gateway",
          "IAM",
          "Cognito",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "authentication",
          "authorization",
          "IAM role"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 122723.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services E\nA developer has a Python script that relies on the low-level BatchGetItem API to fetch large amounts of data from a DynamoDB table. The\nscript often encounters responses with partial results. A significant portion of the data appears under UnprocessedKeys .\nWhich approaches can the developer implement to handle data retrieval MOST reliably? (Select TWO.)\nCreate a Global Secondary Index (GSI) with its own read capacity settings.\nIncrease the read capacity units (RCUs) for the DynamoDB table and enable Auto Scaling.\nUse the AWS software development kit (AWS SDK) to send batch requests.\nImplement a logic that immediately retries the batch request.\nImplement an exponential backoff algorithm with a randomized delay between retries of the batch request.\nIncorrect\nA single BatchGetItem operation can retrieve up to 16 MB of data, which can contain as many as 100 items.\nBatchGetItem returns a partial result if:\n— The response size limit is exceeded\n— The table's provisioned throughput is exceeded\n— More than 1MB per partition is requested\n— An internal processing failure occurs.\n_| For marooned om EE tims mel $m vrmdrtimism AON tmrnm brid momembe mdi sid vm] men 2m ONO IVD tom mim tlm tsetse vrmdt trv ED mmm (mmm mem vmod dm mirrored lam 10",
        "options": [
          "C: ategory: CDA - Development with AWS Services E",
          "A: developer has a Python script that relies on the low-level BatchGetItem API to fetch large amounts of data from a DynamoDB table. The",
          "c: ript often encounters responses with partial results. A significant portion of the data appears under UnprocessedKeys .",
          "c: h approaches can the developer implement to handle data retrieval MOST reliably? (Select TWO.)",
          "C: reate a Global Secondary Index (GSI) with its own read capacity settings.",
          "c: rease the read capacity units (RCUs) for the DynamoDB table and enable Auto Scaling.",
          "A: WS software development kit (AWS SDK) to send batch requests.",
          "a: logic that immediately retries the batch request.",
          "a: n exponential backoff algorithm with a randomized delay between retries of the batch request.",
          "c: orrect",
          "A: single BatchGetItem operation can retrieve up to 16 MB of data, which can contain as many as 100 items.",
          "B: atchGetItem returns a partial result if:",
          "c: eeded",
          "a: ble's provisioned throughput is exceeded",
          "a: n 1MB per partition is requested",
          "A: n internal processing failure occurs.",
          "a: rooned om EE tims mel $m vrmdrtimism AON tmrnm brid momembe mdi sid vm] men 2m ONO IVD tom mim tlm tsetse vrmdt trv ED mmm (mmm mem vmod dm mirrored lam 10"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "26. QUESTION\nCategory: CDA - Development with AWS Services E\nA developer has a Python script that relies on the low-level BatchGetItem API to fetch large amounts of data from a DynamoDB table. The\nscript often encounters responses with partial results. A significant portion of the data appears under UnprocessedKeys .\nWhich approaches can the developer implement to handle data retrieval MOST reliably? (Select TWO.)\nCreate a Global Secondary Index (GSI) with its own read capacity settings.\nIncrease the read capacity units (RCUs) for the DynamoDB table and enable Auto Scaling.\nUse the AWS software development kit (AWS SDK) to send batch requests.\nImplement a logic that immediately retries the batch request.\nImplement an exponential backoff algorithm with a randomized delay between retries of the batch request.\nIncorrect\nA single BatchGetItem operation can retrieve up to 16 MB of data, which can contain as many as 100 items.\nBatchGetItem returns a partial result if:\n— The response size limit is exceeded\n— The table's provisioned throughput is exceeded\n— More than 1MB per partition is requested\n— An internal processing failure occurs.\n_| For marooned om EE tims mel $m vrmdrtimism AON tmrnm brid momembe mdi sid vm] men 2m ONO IVD tom mim tlm tsetse vrmdt trv ED mmm (mmm mem vmod dm mirrored lam 10"
      },
      "tags": {
        "services": [
          "DynamoDB"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "scaling"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 122822.png",
      "parsed": {
        "question": "Category: CDA - Security\nA developer is building a serverless API composed of an API Gateway and several Lambda functions. All resources are defined using Cloud B\nDevelopment Kit (CDK) L2 constructs. The developer wants to test some of the Lambda functions in their local environment.\nGiven that AWS SAM and AWS CDK are already configured locally, what combination of actions must the developer do? (Select TWO.)\nExecute the sam package and specify the S3 bucket where the Lambda code will be uploaded for local testing.\nRun the cdk synth command and indicate the stack name of Lambda functions to be tested.\nRun the cdk bootstrap command to prepare the staging of CDK assets.\nExecute the sam local invoke command and specify the location of the synthesized CloudFormation template and\nidentifier of each function.\nExecute the sam local start-lambda and specify the location of the synthesized CloudFormation template along with\nfunction identifiers.\nIncorrect\nThe sam local invoke command allows you to test AWS Lambda functions locally by emulating the Lambda execution environment.\nHowever, to test resources defined in AWS CDK, you must first convert the CDK constructs into a format that SAM can understand. This is where\nthe cdk synth command comes into play. It synthesizes or “compiles” your CDK application into an AWS CloudFormation template.\nmm\nAWS CDK application |\nBn",
        "options": [
          "C: ategory: CDA - Security",
          "A: developer is building a serverless API composed of an API Gateway and several Lambda functions. All resources are defined using Cloud B",
          "D: evelopment Kit (CDK) L2 constructs. The developer wants to test some of the Lambda functions in their local environment.",
          "a: t AWS SAM and AWS CDK are already configured locally, what combination of actions must the developer do? (Select TWO.)",
          "c: ute the sam package and specify the S3 bucket where the Lambda code will be uploaded for local testing.",
          "c: dk synth command and indicate the stack name of Lambda functions to be tested.",
          "c: dk bootstrap command to prepare the staging of CDK assets.",
          "c: ute the sam local invoke command and specify the location of the synthesized CloudFormation template and",
          "d: entifier of each function.",
          "c: ute the sam local start-lambda and specify the location of the synthesized CloudFormation template along with",
          "c: tion identifiers.",
          "c: orrect",
          "a: m local invoke command allows you to test AWS Lambda functions locally by emulating the Lambda execution environment.",
          "c: es defined in AWS CDK, you must first convert the CDK constructs into a format that SAM can understand. This is where",
          "c: dk synth command comes into play. It synthesizes or “compiles” your CDK application into an AWS CloudFormation template.",
          "A: WS CDK application |",
          "B: n"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "27. QUESTION\nCategory: CDA - Security\nA developer is building a serverless API composed of an API Gateway and several Lambda functions. All resources are defined using Cloud B\nDevelopment Kit (CDK) L2 constructs. The developer wants to test some of the Lambda functions in their local environment.\nGiven that AWS SAM and AWS CDK are already configured locally, what combination of actions must the developer do? (Select TWO.)\nExecute the sam package and specify the S3 bucket where the Lambda code will be uploaded for local testing.\nRun the cdk synth command and indicate the stack name of Lambda functions to be tested.\nRun the cdk bootstrap command to prepare the staging of CDK assets.\nExecute the sam local invoke command and specify the location of the synthesized CloudFormation template and\nidentifier of each function.\nExecute the sam local start-lambda and specify the location of the synthesized CloudFormation template along with\nfunction identifiers.\nIncorrect\nThe sam local invoke command allows you to test AWS Lambda functions locally by emulating the Lambda execution environment.\nHowever, to test resources defined in AWS CDK, you must first convert the CDK constructs into a format that SAM can understand. This is where\nthe cdk synth command comes into play. It synthesizes or “compiles” your CDK application into an AWS CloudFormation template.\nmm\nAWS CDK application |\nBn"
      },
      "tags": {
        "services": [
          "Lambda",
          "S3",
          "API Gateway",
          "CloudFormation",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "security"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 123320.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nA development team uses AWS Step Functions to orchestrate a serverless workflow that processes incoming data streams with AWS Lambda. E\nThe team requires a solution to extract custom metrics, such as processing times, directly from the logs generated by a Lambda function.\nThese metrics must be analyzed for operational insights, with alarms set up to detect anomalies in real-time.\nWhich approach should be used to meet this requirement?\n® Send custom metric data directly to Amazon EventBridge using the PutMetricData API operation. Create EventBridge\nrules to trigger actions based on the metrics.\nUse Amazon CloudWatch Lambda Insights to extract and monitor custom metrics for processing times. Set up alarms and\ndashboards in Lambda Insights for real-time analysis.\nConfigure Amazon Data Firehose to stream logs from the Lambda function to an Amazon Redshift cluster. Use SQL queries to\nanalyze metrics and set alerts for anomalies.\nUse Amazon's open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use\nCloudWatch to monitor, view, and set alarms for the custom metrics.\nIncorrect\nThe Amazon CloudWatch embedded metric format allows you to generate custom metrics asynchronously in the form of logs written to\nCloudWatch Logs. You can embed custom metrics alongside detailed log event data, and CloudWatch automatically extracts the custom metrics\nso that you can visualize and alarm on them, for real-time incident detection. Additionally, the detailed log events associated with the extracted\nmetrics can be queried using CloudWatch Logs Insights to provide deep insights into the root causes of operational events.\nEmbedded metric format helps you generate actionable custom metrics from ephemeral resources such as Lambda functions and containers. By\nusing the embedded metric format to send logs from these ephemeral resources, you can now easily create custom metrics without having to\nn",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: development team uses AWS Step Functions to orchestrate a serverless workflow that processes incoming data streams with AWS Lambda. E",
          "a: m requires a solution to extract custom metrics, such as processing times, directly from the logs generated by a Lambda function.",
          "c: s must be analyzed for operational insights, with alarms set up to detect anomalies in real-time.",
          "c: h approach should be used to meet this requirement?",
          "d: custom metric data directly to Amazon EventBridge using the PutMetricData API operation. Create EventBridge",
          "a: ctions based on the metrics.",
          "A: mazon CloudWatch Lambda Insights to extract and monitor custom metrics for processing times. Set up alarms and",
          "d: ashboards in Lambda Insights for real-time analysis.",
          "C: onfigure Amazon Data Firehose to stream logs from the Lambda function to an Amazon Redshift cluster. Use SQL queries to",
          "a: nalyze metrics and set alerts for anomalies.",
          "A: mazon's open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use",
          "C: loudWatch to monitor, view, and set alarms for the custom metrics.",
          "c: orrect",
          "A: mazon CloudWatch embedded metric format allows you to generate custom metrics asynchronously in the form of logs written to",
          "C: loudWatch Logs. You can embed custom metrics alongside detailed log event data, and CloudWatch automatically extracts the custom metrics",
          "a: t you can visualize and alarm on them, for real-time incident detection. Additionally, the detailed log events associated with the extracted",
          "c: s can be queried using CloudWatch Logs Insights to provide deep insights into the root causes of operational events.",
          "b: edded metric format helps you generate actionable custom metrics from ephemeral resources such as Lambda functions and containers. By",
          "b: edded metric format to send logs from these ephemeral resources, you can now easily create custom metrics without having to"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "29. QUESTION\nCategory: CDA - Deployment\nA development team uses AWS Step Functions to orchestrate a serverless workflow that processes incoming data streams with AWS Lambda. E\nThe team requires a solution to extract custom metrics, such as processing times, directly from the logs generated by a Lambda function.\nThese metrics must be analyzed for operational insights, with alarms set up to detect anomalies in real-time.\nWhich approach should be used to meet this requirement?\n® Send custom metric data directly to Amazon EventBridge using the PutMetricData API operation. Create EventBridge\nrules to trigger actions based on the metrics.\nUse Amazon CloudWatch Lambda Insights to extract and monitor custom metrics for processing times. Set up alarms and\ndashboards in Lambda Insights for real-time analysis.\nConfigure Amazon Data Firehose to stream logs from the Lambda function to an Amazon Redshift cluster. Use SQL queries to\nanalyze metrics and set alerts for anomalies.\nUse Amazon's open-source libraries to format logs in the Amazon CloudWatch embedded metric format. Then, use\nCloudWatch to monitor, view, and set alarms for the custom metrics.\nIncorrect\nThe Amazon CloudWatch embedded metric format allows you to generate custom metrics asynchronously in the form of logs written to\nCloudWatch Logs. You can embed custom metrics alongside detailed log event data, and CloudWatch automatically extracts the custom metrics\nso that you can visualize and alarm on them, for real-time incident detection. Additionally, the detailed log events associated with the extracted\nmetrics can be queried using CloudWatch Logs Insights to provide deep insights into the root causes of operational events.\nEmbedded metric format helps you generate actionable custom metrics from ephemeral resources such as Lambda functions and containers. By\nusing the embedded metric format to send logs from these ephemeral resources, you can now easily create custom metrics without having to\nn"
      },
      "tags": {
        "services": [
          "Lambda",
          "RDS",
          "Redshift",
          "EventBridge",
          "Step Functions",
          "CloudWatch",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "container",
          "deployment",
          "asynchronous",
          "synchronous"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 123340.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA developer wants to cut down the execution time of the scan operation on a DynamoDB table during periods of low demand without\ninterfering with typical workloads. The operation consumes half of the strongly consistent read capacity units within regular operating hours.\nHow can the developer improve this scan operation?\n® Useaparallel scan operation.\nPerform a rate-limited sequential scan operation.\nUse eventually consistent reads for the scan operation instead of strongly consistent reads.\nPerform a rate-limited parallel scan operation.\nIncorrect\nBy default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an\napplication performs additional Scan operations to retrieve the next 1 MB of data.\nThe larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be\nable to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table's data across multiple physical\npartitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum\nthroughput of a single partition.\nTo address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application\nworkers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating",
        "options": [
          "A: ___ 4A 4 N 4",
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: developer wants to cut down the execution time of the scan operation on a DynamoDB table during periods of low demand without",
          "c: al workloads. The operation consumes half of the strongly consistent read capacity units within regular operating hours.",
          "c: an the developer improve this scan operation?",
          "a: parallel scan operation.",
          "a: rate-limited sequential scan operation.",
          "a: lly consistent reads for the scan operation instead of strongly consistent reads.",
          "a: rate-limited parallel scan operation.",
          "c: orrect",
          "B: y default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an",
          "a: pplication performs additional Scan operations to retrieve the next 1 MB of data.",
          "a: rger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be",
          "a: ble to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table's data across multiple physical",
          "a: rtitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum",
          "a: single partition.",
          "a: ddress these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application",
          "c: anning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "A___ 4A 4 N 4\n30. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA developer wants to cut down the execution time of the scan operation on a DynamoDB table during periods of low demand without\ninterfering with typical workloads. The operation consumes half of the strongly consistent read capacity units within regular operating hours.\nHow can the developer improve this scan operation?\n® Useaparallel scan operation.\nPerform a rate-limited sequential scan operation.\nUse eventually consistent reads for the scan operation instead of strongly consistent reads.\nPerform a rate-limited parallel scan operation.\nIncorrect\nBy default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an\napplication performs additional Scan operations to retrieve the next 1 MB of data.\nThe larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be\nable to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table's data across multiple physical\npartitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum\nthroughput of a single partition.\nTo address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application\nworkers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating"
      },
      "tags": {
        "services": [
          "DynamoDB"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 123410.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nA developer plans to use AWS Elastic Beanstalk to deploy a microservice application. The application will be implemented in a multi-container\nDocker environment.\nHow should the developer configure the container definitions in the environment?\nConfigure the container definitions in the Amazon ECS Console when building the Docker environment.\nConfigure the container definitions in the Dockerrun.aws.json file.\nConfigure the container definitions in the Dockerrun.aws.json.config and put it inside the .ebextensions folder.\n® Usethe eb config command to configure the container definitions.\nIncorrect\nStandard generic and preconfigured Docker platforms on Elastic Beanstalk support only a single Docker container per Elastic Beanstalk\nenvironment. In order to get the most out of Docker, Elastic Beanstalk lets you create an environment where your Amazon EC2 instances run\nmultiple Docker containers side by side.\nThe following diagram shows an example Elastic Beanstalk environment configured with three Docker containers running on each Amazon EC2\ninstance in an Auto Scaling group:\nEE inci I a",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: developer plans to use AWS Elastic Beanstalk to deploy a microservice application. The application will be implemented in a multi-container",
          "D: ocker environment.",
          "d: the developer configure the container definitions in the environment?",
          "C: onfigure the container definitions in the Amazon ECS Console when building the Docker environment.",
          "C: onfigure the container definitions in the Dockerrun.aws.json file.",
          "C: onfigure the container definitions in the Dockerrun.aws.json.config and put it inside the .ebextensions folder.",
          "b: config command to configure the container definitions.",
          "c: orrect",
          "a: ndard generic and preconfigured Docker platforms on Elastic Beanstalk support only a single Docker container per Elastic Beanstalk",
          "d: er to get the most out of Docker, Elastic Beanstalk lets you create an environment where your Amazon EC2 instances run",
          "D: ocker containers side by side.",
          "d: iagram shows an example Elastic Beanstalk environment configured with three Docker containers running on each Amazon EC2",
          "a: nce in an Auto Scaling group:",
          "c: i I a"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "EGO\n31. QUESTION\nCategory: CDA - Deployment\nA developer plans to use AWS Elastic Beanstalk to deploy a microservice application. The application will be implemented in a multi-container\nDocker environment.\nHow should the developer configure the container definitions in the environment?\nConfigure the container definitions in the Amazon ECS Console when building the Docker environment.\nConfigure the container definitions in the Dockerrun.aws.json file.\nConfigure the container definitions in the Dockerrun.aws.json.config and put it inside the .ebextensions folder.\n® Usethe eb config command to configure the container definitions.\nIncorrect\nStandard generic and preconfigured Docker platforms on Elastic Beanstalk support only a single Docker container per Elastic Beanstalk\nenvironment. In order to get the most out of Docker, Elastic Beanstalk lets you create an environment where your Amazon EC2 instances run\nmultiple Docker containers side by side.\nThe following diagram shows an example Elastic Beanstalk environment configured with three Docker containers running on each Amazon EC2\ninstance in an Auto Scaling group:\nEE inci I a"
      },
      "tags": {
        "services": [
          "EC2",
          "Elastic Beanstalk",
          "ECS",
          "Config"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": [
          "container",
          "scaling",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 124127.png",
      "parsed": {
        "question": "E\nCategory: CDA - Security\nAn application uses the PutObject operation in parallel to upload hundreds of thousands of objects per second to an S3 bucket. To meet\nsecurity compliance, the developer uses the server-side encryption in AWS KMS (SSE-KMS) to encrypt objects as they get stored in the S3\nbucket. There is a noticeable performance degradation after making the change.\nWhich of the following is the most likely cause of the problem?\n® The Amazon S3 throttles the pPutobject operation for objects encrypted with SSE-KMS.\nThe AWK KMS key uses an AES 256 algorithm, which makes the encryption process slower. AES 128 should be used instead.\nThe KMS key does not use an alias required for the server-side encryption with AWS KMS (SSE-KMS).\nThe API request rate has exceeded the quota for AWS KMS API operations.\nIncorrect\nAWS KMS establishes quotas for the number of API operations requested in each second.\nYou can make API requests directly or by using an integrated AWS service that makes API requests to AWS KMS on your behalf. The quota applies\nto both kinds of requests.\nService Quotas xX Service Quotas >» AWS services > AWS Key Management Service (AWS KMS)\nDashboard AWS Key Management Service (AWS KMS) [4\nAWS services Centralized Generation and Management of Encryption Keys to Control Access to Stored Data Across AWS.\nQuota request history\n: w Organization Service quotas info",
        "options": [
          "C: ategory: CDA - Security",
          "A: n application uses the PutObject operation in parallel to upload hundreds of thousands of objects per second to an S3 bucket. To meet",
          "c: urity compliance, the developer uses the server-side encryption in AWS KMS (SSE-KMS) to encrypt objects as they get stored in the S3",
          "b: ucket. There is a noticeable performance degradation after making the change.",
          "c: h of the following is the most likely cause of the problem?",
          "A: mazon S3 throttles the pPutobject operation for objects encrypted with SSE-KMS.",
          "A: WK KMS key uses an AES 256 algorithm, which makes the encryption process slower. AES 128 should be used instead.",
          "d: oes not use an alias required for the server-side encryption with AWS KMS (SSE-KMS).",
          "A: PI request rate has exceeded the quota for AWS KMS API operations.",
          "c: orrect",
          "A: WS KMS establishes quotas for the number of API operations requested in each second.",
          "c: an make API requests directly or by using an integrated AWS service that makes API requests to AWS KMS on your behalf. The quota applies",
          "b: oth kinds of requests.",
          "c: e Quotas xX Service Quotas >» AWS services > AWS Key Management Service (AWS KMS)",
          "D: ashboard AWS Key Management Service (AWS KMS) [4",
          "A: WS services Centralized Generation and Management of Encryption Keys to Control Access to Stored Data Across AWS.",
          "a: request history",
          "a: nization Service quotas info"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "33. QUESTION E\nCategory: CDA - Security\nAn application uses the PutObject operation in parallel to upload hundreds of thousands of objects per second to an S3 bucket. To meet\nsecurity compliance, the developer uses the server-side encryption in AWS KMS (SSE-KMS) to encrypt objects as they get stored in the S3\nbucket. There is a noticeable performance degradation after making the change.\nWhich of the following is the most likely cause of the problem?\n® The Amazon S3 throttles the pPutobject operation for objects encrypted with SSE-KMS.\nThe AWK KMS key uses an AES 256 algorithm, which makes the encryption process slower. AES 128 should be used instead.\nThe KMS key does not use an alias required for the server-side encryption with AWS KMS (SSE-KMS).\nThe API request rate has exceeded the quota for AWS KMS API operations.\nIncorrect\nAWS KMS establishes quotas for the number of API operations requested in each second.\nYou can make API requests directly or by using an integrated AWS service that makes API requests to AWS KMS on your behalf. The quota applies\nto both kinds of requests.\nService Quotas xX Service Quotas >» AWS services > AWS Key Management Service (AWS KMS)\nDashboard AWS Key Management Service (AWS KMS) [4\nAWS services Centralized Generation and Management of Encryption Keys to Control Access to Stored Data Across AWS.\nQuota request history\n: w Organization Service quotas info"
      },
      "tags": {
        "services": [
          "S3",
          "KMS"
        ],
        "domains": [
          "Development with AWS Services",
          "Security"
        ],
        "keywords": [
          "performance",
          "security",
          "encryption"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 124215.png",
      "parsed": {
        "question": "®\nCategory: CDA - Security ki\nA startup has recently opened an AWS account to develop a cloud-native web application. The CEO wants to improve the security of the\naccount by implementing the best practices in managing access keys in AWS.\nWhich actions follow the security best practices in IAM? (Select TWO.)\nSave the access key in your application code for convenience.\nUse IAM roles for applications that need access to AWS services.\nDelete any access keys to your AWS account root user.\nRegularly rotate the credentials for all the account users except for the administrator user for tracking purposes.\nMaintain at least one access key for your AWS account root user.\nIncorrect\nAWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control\nwho is authenticated (signed in) and authorized (has permissions) to use resources.\nAccount ID 123456789012\nAWS\nPrincipal Request\nAN\n= Actions/Operations Account ID 012345012345\nere ~© Resouces Gis\n7) - 3 Request\nG \\— information 8\na | Authentication — ; —\n) he BN oe",
        "options": [
          "C: ategory: CDA - Security ki",
          "A: startup has recently opened an AWS account to develop a cloud-native web application. The CEO wants to improve the security of the",
          "a: ccount by implementing the best practices in managing access keys in AWS.",
          "c: h actions follow the security best practices in IAM? (Select TWO.)",
          "a: ve the access key in your application code for convenience.",
          "A: M roles for applications that need access to AWS services.",
          "D: elete any access keys to your AWS account root user.",
          "a: rly rotate the credentials for all the account users except for the administrator user for tracking purposes.",
          "a: intain at least one access key for your AWS account root user.",
          "c: orrect",
          "A: WS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control",
          "a: uthenticated (signed in) and authorized (has permissions) to use resources.",
          "A: ccount ID 123456789012",
          "A: WS",
          "c: ipal Request",
          "A: N",
          "A: ctions/Operations Account ID 012345012345",
          "c: es Gis",
          "a: tion 8",
          "a: | Authentication — ; —",
          "B: N oe"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "35. QUESTION\n®\nCategory: CDA - Security ki\nA startup has recently opened an AWS account to develop a cloud-native web application. The CEO wants to improve the security of the\naccount by implementing the best practices in managing access keys in AWS.\nWhich actions follow the security best practices in IAM? (Select TWO.)\nSave the access key in your application code for convenience.\nUse IAM roles for applications that need access to AWS services.\nDelete any access keys to your AWS account root user.\nRegularly rotate the credentials for all the account users except for the administrator user for tracking purposes.\nMaintain at least one access key for your AWS account root user.\nIncorrect\nAWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. You use IAM to control\nwho is authenticated (signed in) and authorized (has permissions) to use resources.\nAccount ID 123456789012\nAWS\nPrincipal Request\nAN\n= Actions/Operations Account ID 012345012345\nere ~© Resouces Gis\n7) - 3 Request\nG \\— information 8\na | Authentication — ; —\n) he BN oe"
      },
      "tags": {
        "services": [
          "IAM"
        ],
        "domains": [
          "Security"
        ],
        "keywords": [
          "security",
          "authentication",
          "IAM role"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 124458.png",
      "parsed": {
        "question": "E\nCategory: CDA - Security\nA San Francisco-based tech startup is building a cross-platform mobile app that can notify the user of upcoming astronomical events. Your\nmobile app authenticates with the Identity Provider (IdP) using the provider's SDK and Amazon Cognito. Once the end-user is authenticated\nwith the IdP, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito.\nWhich of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?\nCognito Key Pair\n@® Cognito API\nCognito SDK\nCognito ID\nIncorrect\nYou can use Amazon Cognito to deliver temporary, limited-privilege credentials to your application so that your users can access AWS resources.\nAmazon Cognito identity pools support both authenticated and unauthenticated identities. You can retrieve a unique Amazon Cognito identifier\n(identity ID) for your end-user immediately if you're allowing unauthenticated users or after you've set the login tokens in the credentials provider\nif you're authenticating users.\n= Amazon Cognito » Identity pools » Create identity pool\nStep 1 i i i TUTORIALS\ncon foure dentitypoot tt Configure identity pool trust we O) JToRIAl\n1",
        "options": [
          "C: ategory: CDA - Security",
          "A: San Francisco-based tech startup is building a cross-platform mobile app that can notify the user of upcoming astronomical events. Your",
          "b: ile app authenticates with the Identity Provider (IdP) using the provider's SDK and Amazon Cognito. Once the end-user is authenticated",
          "d: P, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito.",
          "c: h of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?",
          "C: ognito Key Pair",
          "C: ognito API",
          "C: ognito SDK",
          "C: ognito ID",
          "c: orrect",
          "c: an use Amazon Cognito to deliver temporary, limited-privilege credentials to your application so that your users can access AWS resources.",
          "A: mazon Cognito identity pools support both authenticated and unauthenticated identities. You can retrieve a unique Amazon Cognito identifier",
          "d: entity ID) for your end-user immediately if you're allowing unauthenticated users or after you've set the login tokens in the credentials provider",
          "a: uthenticating users.",
          "A: mazon Cognito » Identity pools » Create identity pool",
          "A: LS",
          "c: on foure dentitypoot tt Configure identity pool trust we O) JToRIAl"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "37. QUESTION E\nCategory: CDA - Security\nA San Francisco-based tech startup is building a cross-platform mobile app that can notify the user of upcoming astronomical events. Your\nmobile app authenticates with the Identity Provider (IdP) using the provider's SDK and Amazon Cognito. Once the end-user is authenticated\nwith the IdP, the OAuth or OpenID Connect token returned from the IdP is passed by your app to Amazon Cognito.\nWhich of the following is returned for the user to provide a set of temporary, limited-privilege AWS credentials?\nCognito Key Pair\n@® Cognito API\nCognito SDK\nCognito ID\nIncorrect\nYou can use Amazon Cognito to deliver temporary, limited-privilege credentials to your application so that your users can access AWS resources.\nAmazon Cognito identity pools support both authenticated and unauthenticated identities. You can retrieve a unique Amazon Cognito identifier\n(identity ID) for your end-user immediately if you're allowing unauthenticated users or after you've set the login tokens in the credentials provider\nif you're authenticating users.\n= Amazon Cognito » Identity pools » Create identity pool\nStep 1 i i i TUTORIALS\ncon foure dentitypoot tt Configure identity pool trust we O) JToRIAl\n1"
      },
      "tags": {
        "services": [
          "Cognito",
          "Config"
        ],
        "domains": [
          "Security"
        ],
        "keywords": [
          "security"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 124518.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer is building an application with Amazon DynamoDB as its database. The application needs to group the PUT , UPDATE , and\nDELETE actions into a single all-or-nothing operation to make changes against multiple items in the DynamoDB table.\nWhich DynamoDB operation should the developer use?\nTransactWriteItems\n@® BatchWriteItem\nQuery\nScan\nIncorrect\nWith Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-\nnothing TransactWriteItems or TransactGetItems operation.\n\nTransactWriteItems isa synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing\noperation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same\nRegion. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them\nsucceed or none of them succeeds.\n\nYou can add the following types of actions to a transaction:\nJ Put Initiates a Put Item operation to create a new item or replace an old item with a new item. conditionally or without specifying anv",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer is building an application with Amazon DynamoDB as its database. The application needs to group the PUT , UPDATE , and",
          "D: ELETE actions into a single all-or-nothing operation to make changes against multiple items in the DynamoDB table.",
          "c: h DynamoDB operation should the developer use?",
          "a: nsactWriteItems",
          "B: atchWriteItem",
          "c: an",
          "c: orrect",
          "A: mazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-",
          "a: nsactWriteItems or TransactGetItems operation.",
          "a: nsactWriteItems isa synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing",
          "a: tion. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same",
          "a: ggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them",
          "c: ceed or none of them succeeds.",
          "c: an add the following types of actions to a transaction:",
          "a: tes a Put Item operation to create a new item or replace an old item with a new item. conditionally or without specifying anv"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "38. QUESTION\nCategory: CDA - Development with AWS Services\nA developer is building an application with Amazon DynamoDB as its database. The application needs to group the PUT , UPDATE , and\nDELETE actions into a single all-or-nothing operation to make changes against multiple items in the DynamoDB table.\nWhich DynamoDB operation should the developer use?\nTransactWriteItems\n@® BatchWriteItem\nQuery\nScan\nIncorrect\nWith Amazon DynamoDB transactions, you can group multiple actions together and submit them as a single all-or-\nnothing TransactWriteItems or TransactGetItems operation.\n\nTransactWriteItems isa synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing\noperation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same\nRegion. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them\nsucceed or none of them succeeds.\n\nYou can add the following types of actions to a transaction:\nJ Put Initiates a Put Item operation to create a new item or replace an old item with a new item. conditionally or without specifying anv"
      },
      "tags": {
        "services": [
          "DynamoDB",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "synchronous",
          "idempotent"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 124715.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA serverless application is created to process numerous files. Each invocation takes 5 minutes to complete. The Lambda function’s execution\ntime is too slow for the application.\nConsidering that the Lambda function does not return any important data, which method will accelerate data processing the most?\nMerge all of the files into a single file then process them all at once with an asynchronous Event Lambda invocation.\n® Compress the files to reduce their size, then process them with synchronous RequestResponse Lambda invocations.\nUse synchronous RequestResponse Lambda invocations. Process the files one by one.\nUse asynchronous Event Lambda invocations. Configure the function to process the files in parallel.\nIncorrect\nSeveral AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Simple Notification Service (Amazon SNS), invoke\nfunctions asynchronously to process events. When you invoke a function asynchronously, you don’t wait for a response from the function code.\nYou hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to\na downstream resource to chain together components of your application.\nAsynchronous Invocation\nLambda function\nEvents Event queue = 7 \\\n| [@) (®) — PEE a",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: serverless application is created to process numerous files. Each invocation takes 5 minutes to complete. The Lambda function’s execution",
          "a: pplication.",
          "C: onsidering that the Lambda function does not return any important data, which method will accelerate data processing the most?",
          "a: ll of the files into a single file then process them all at once with an asynchronous Event Lambda invocation.",
          "C: ompress the files to reduce their size, then process them with synchronous RequestResponse Lambda invocations.",
          "c: hronous RequestResponse Lambda invocations. Process the files one by one.",
          "a: synchronous Event Lambda invocations. Configure the function to process the files in parallel.",
          "c: orrect",
          "a: l AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Simple Notification Service (Amazon SNS), invoke",
          "c: tions asynchronously to process events. When you invoke a function asynchronously, you don’t wait for a response from the function code.",
          "a: nd off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to",
          "a: downstream resource to chain together components of your application.",
          "A: synchronous Invocation",
          "a: mbda function"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "hie)\n39. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA serverless application is created to process numerous files. Each invocation takes 5 minutes to complete. The Lambda function’s execution\ntime is too slow for the application.\nConsidering that the Lambda function does not return any important data, which method will accelerate data processing the most?\nMerge all of the files into a single file then process them all at once with an asynchronous Event Lambda invocation.\n® Compress the files to reduce their size, then process them with synchronous RequestResponse Lambda invocations.\nUse synchronous RequestResponse Lambda invocations. Process the files one by one.\nUse asynchronous Event Lambda invocations. Configure the function to process the files in parallel.\nIncorrect\nSeveral AWS services, such as Amazon Simple Storage Service (Amazon S3) and Amazon Simple Notification Service (Amazon SNS), invoke\nfunctions asynchronously to process events. When you invoke a function asynchronously, you don’t wait for a response from the function code.\nYou hand off the event to Lambda and Lambda handles the rest. You can configure how Lambda handles errors and can send invocation records to\na downstream resource to chain together components of your application.\nAsynchronous Invocation\nLambda function\nEvents Event queue = 7 \\\n| [@) (®) — PEE a"
      },
      "tags": {
        "services": [
          "Lambda",
          "S3",
          "RDS",
          "SNS",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "queue",
          "asynchronous",
          "synchronous"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 125423.png",
      "parsed": {
        "question": "Category: CDA - Security\nSome static assets stored in an S3 bucket need to be accessed by a user on the development account. The S3 bucket is in the production |\naccount. According to the company policy, the sharing of full credentials between accounts is prohibited.\nWhat steps should be done to delegate access across the two accounts? (Select THREE.)\nLog in to the production account and create a policy that will use STS to assume the IAM role in the development account.\nAttach the policy to the IAM users.\nOn the production account, create an IAM role and specify the development account as a trusted entity.\nSet the policy that will grant access to S3 for the IAM role created in the production account.\nSet the policy that will grant access to S3 for the IAM role created in the development account.\nOn the development account, create an IAM role and specify the production account as a trusted entity.\nLog in to the development account and create a policy that will use STS to assume the IAM role in the production account.\nAttach the policy to the IAM users.\nIncorrect\nThe problem is about delegating access to the development account to use the S3 Bucket on the production account.\nez",
        "options": [
          "C: ategory: CDA - Security",
          "a: tic assets stored in an S3 bucket need to be accessed by a user on the development account. The S3 bucket is in the production |",
          "a: ccount. According to the company policy, the sharing of full credentials between accounts is prohibited.",
          "a: t steps should be done to delegate access across the two accounts? (Select THREE.)",
          "d: uction account and create a policy that will use STS to assume the IAM role in the development account.",
          "A: ttach the policy to the IAM users.",
          "d: uction account, create an IAM role and specify the development account as a trusted entity.",
          "c: y that will grant access to S3 for the IAM role created in the production account.",
          "c: y that will grant access to S3 for the IAM role created in the development account.",
          "d: evelopment account, create an IAM role and specify the production account as a trusted entity.",
          "d: evelopment account and create a policy that will use STS to assume the IAM role in the production account.",
          "A: ttach the policy to the IAM users.",
          "c: orrect",
          "b: lem is about delegating access to the development account to use the S3 Bucket on the production account."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "40. QUESTION\nCategory: CDA - Security\nSome static assets stored in an S3 bucket need to be accessed by a user on the development account. The S3 bucket is in the production |\naccount. According to the company policy, the sharing of full credentials between accounts is prohibited.\nWhat steps should be done to delegate access across the two accounts? (Select THREE.)\nLog in to the production account and create a policy that will use STS to assume the IAM role in the development account.\nAttach the policy to the IAM users.\nOn the production account, create an IAM role and specify the development account as a trusted entity.\nSet the policy that will grant access to S3 for the IAM role created in the production account.\nSet the policy that will grant access to S3 for the IAM role created in the development account.\nOn the development account, create an IAM role and specify the production account as a trusted entity.\nLog in to the development account and create a policy that will use STS to assume the IAM role in the production account.\nAttach the policy to the IAM users.\nIncorrect\nThe problem is about delegating access to the development account to use the S3 Bucket on the production account.\nez"
      },
      "tags": {
        "services": [
          "S3",
          "IAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Security"
        ],
        "keywords": [
          "security",
          "IAM role",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 125455.png",
      "parsed": {
        "question": "|\nCategory: CDA - Troubleshooting and Optimization\nA company uses a Linux, Apache, MySQL, and PHP (LAMP) web service stack to host an on-premises application for its car rental business.\nThe manager wants to move its operation into the Cloud using Amazon Web Services.\nWhich combination of services could be used to run the application that will require the least amount of configuration?\nAmazon API Gateway and Amazon RDS\nAmazon EC2 and Amazon Aurora\nAmazon ECS and Amazon EFS\n® Amazon S3 and Amazon CloudFront\nIncorrect\nYou can install an Apache web server with PHP and MySQL support on your Amazon Linux instance (sometimes called a LAMP web server or LAMP\nstack). You can use this server to host a static website or deploy a dynamic PHP application that reads and writes information to a database.\nApplications\n< 7 -\n1",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: company uses a Linux, Apache, MySQL, and PHP (LAMP) web service stack to host an on-premises application for its car rental business.",
          "a: nager wants to move its operation into the Cloud using Amazon Web Services.",
          "c: h combination of services could be used to run the application that will require the least amount of configuration?",
          "A: mazon API Gateway and Amazon RDS",
          "A: mazon EC2 and Amazon Aurora",
          "A: mazon ECS and Amazon EFS",
          "A: mazon S3 and Amazon CloudFront",
          "c: orrect",
          "c: an install an Apache web server with PHP and MySQL support on your Amazon Linux instance (sometimes called a LAMP web server or LAMP",
          "a: ck). You can use this server to host a static website or deploy a dynamic PHP application that reads and writes information to a database.",
          "A: pplications"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "Gis)\n41. QUESTION |\nCategory: CDA - Troubleshooting and Optimization\nA company uses a Linux, Apache, MySQL, and PHP (LAMP) web service stack to host an on-premises application for its car rental business.\nThe manager wants to move its operation into the Cloud using Amazon Web Services.\nWhich combination of services could be used to run the application that will require the least amount of configuration?\nAmazon API Gateway and Amazon RDS\nAmazon EC2 and Amazon Aurora\nAmazon ECS and Amazon EFS\n® Amazon S3 and Amazon CloudFront\nIncorrect\nYou can install an Apache web server with PHP and MySQL support on your Amazon Linux instance (sometimes called a LAMP web server or LAMP\nstack). You can use this server to host a static website or deploy a dynamic PHP application that reads and writes information to a database.\nApplications\n< 7 -\n1"
      },
      "tags": {
        "services": [
          "EC2",
          "ECS",
          "S3",
          "EBS",
          "EFS",
          "RDS",
          "Aurora",
          "CloudFront",
          "API Gateway",
          "Config"
        ],
        "domains": [
          "Development with AWS Services"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 131236.png",
      "parsed": {
        "question": "Category: CDA - Security KE\nA developer has an application that stores sensitive data to an Amazon DynamoDB table. AWS KMS must be used to encrypt the data before\nsending it to the table and to manage the encryption keys.\nWhich of the following features are supported when using KMS? (Select TWO.)\nRe-enabling disabled keys\nCreation of symmetric encryption and asymmetric KMS keys\nAutomatic key rotation for KMS keys in custom key stores\nImporting a custom key material to an asymmetric KMS key\nUsing AWS Certificate Manager as a custom key store\nIncorrect\nAWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control KMS keys, the encryption keys\nused to encrypt your data. KMS keys are protected by hardware security modules (HSMs) that are validated by the FIPS 140-2 Cryptographic\nModule Validation Program except in the China (Beijing) and China (Ningxi",
        "options": [
          "C: ategory: CDA - Security KE",
          "A: developer has an application that stores sensitive data to an Amazon DynamoDB table. AWS KMS must be used to encrypt the data before",
          "d: ing it to the table and to manage the encryption keys.",
          "c: h of the following features are supported when using KMS? (Select TWO.)",
          "a: bling disabled keys",
          "C: reation of symmetric encryption and asymmetric KMS keys",
          "A: utomatic key rotation for KMS keys in custom key stores",
          "a: custom key material to an asymmetric KMS key",
          "A: WS Certificate Manager as a custom key store",
          "c: orrect",
          "A: WS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control KMS keys, the encryption keys",
          "d: to encrypt your data. KMS keys are protected by hardware security modules (HSMs) that are validated by the FIPS 140-2 Cryptographic",
          "d: ule Validation Program except in the China (Beijing) and China (Ningxia) Regions. AWS KMS is integrated with most other AWS services that",
          "c: rypt your data. AWS KMS is also integrated with AWS CloudTrail to log the use of your KMS keys for auditing, regulatory, and compliance",
          "d: s."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "42. QUESTION\nCategory: CDA - Security KE\nA developer has an application that stores sensitive data to an Amazon DynamoDB table. AWS KMS must be used to encrypt the data before\nsending it to the table and to manage the encryption keys.\nWhich of the following features are supported when using KMS? (Select TWO.)\nRe-enabling disabled keys\nCreation of symmetric encryption and asymmetric KMS keys\nAutomatic key rotation for KMS keys in custom key stores\nImporting a custom key material to an asymmetric KMS key\nUsing AWS Certificate Manager as a custom key store\nIncorrect\nAWS Key Management Service (AWS KMS) is a managed service that makes it easy for you to create and control KMS keys, the encryption keys\nused to encrypt your data. KMS keys are protected by hardware security modules (HSMs) that are validated by the FIPS 140-2 Cryptographic\nModule Validation Program except in the China (Beijing) and China (Ningxia) Regions. AWS KMS is integrated with most other AWS services that\nencrypt your data. AWS KMS is also integrated with AWS CloudTrail to log the use of your KMS keys for auditing, regulatory, and compliance\nneeds.\n- / | KMS \\"
      },
      "tags": {
        "services": [
          "DynamoDB",
          "KMS",
          "CloudTrail"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "security",
          "encryption"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 131330.png",
      "parsed": {
        "question": "|\nCategory: CDA - Troubleshooting and Optimization\nA development team is building a website that displays an analytics dashboard. The team uses AWS CodeBuild to compile the website from a\nsource code residing on Github. A member was instructed to configure CodeBuild to run with a proxy server for privacy and security reasons. A\nRequestError timeout error appears on CloudWatch whenever CodeBuild is accessed.\nWhich is a possible solution to resolve the issue?\nModify the proxy element of the buildspec.yml file on the source code root directory.\nModify the proxy element of the Appspec.yml file on the source code root directory.\n® Modify the artifacts element of the buildspec.yml file on the source code root directory.\nModify the phases element of the appspec.yml file on the source code root directory.\nIncorrect\nYou can use AWS CodeBuild with a proxy server to regulate HTTP and HTTPS traffic to and from the Internet. To run CodeBuild with a proxy\nserver, you install a proxy server in a public subnet and CodeBuild in a private subnet in a VPC.\nBelow are possible causes of error when running CodeBuild with a proxy server:\n1. ssl-bump is not configured properly.\n2. Your organization's security policy does not allow you to use ssl-bump .\n3. Your buildspec.yml file does not have proxy settings specified usinga proxy element.\n= If you do not use ssl-bump for an explicit proxy server, add a proxy configuration to your buildspec.yml usinga proxy element.",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: development team is building a website that displays an analytics dashboard. The team uses AWS CodeBuild to compile the website from a",
          "c: e code residing on Github. A member was instructed to configure CodeBuild to run with a proxy server for privacy and security reasons. A",
          "a: ppears on CloudWatch whenever CodeBuild is accessed.",
          "c: h is a possible solution to resolve the issue?",
          "d: ify the proxy element of the buildspec.yml file on the source code root directory.",
          "d: ify the proxy element of the Appspec.yml file on the source code root directory.",
          "d: ify the artifacts element of the buildspec.yml file on the source code root directory.",
          "d: ify the phases element of the appspec.yml file on the source code root directory.",
          "c: orrect",
          "c: an use AWS CodeBuild with a proxy server to regulate HTTP and HTTPS traffic to and from the Internet. To run CodeBuild with a proxy",
          "a: ll a proxy server in a public subnet and CodeBuild in a private subnet in a VPC.",
          "B: elow are possible causes of error when running CodeBuild with a proxy server:",
          "b: ump is not configured properly.",
          "a: nization's security policy does not allow you to use ssl-bump .",
          "b: uildspec.yml file does not have proxy settings specified usinga proxy element.",
          "d: o not use ssl-bump for an explicit proxy server, add a proxy configuration to your buildspec.yml usinga proxy element."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "43. QUESTION |\nCategory: CDA - Troubleshooting and Optimization\nA development team is building a website that displays an analytics dashboard. The team uses AWS CodeBuild to compile the website from a\nsource code residing on Github. A member was instructed to configure CodeBuild to run with a proxy server for privacy and security reasons. A\nRequestError timeout error appears on CloudWatch whenever CodeBuild is accessed.\nWhich is a possible solution to resolve the issue?\nModify the proxy element of the buildspec.yml file on the source code root directory.\nModify the proxy element of the Appspec.yml file on the source code root directory.\n® Modify the artifacts element of the buildspec.yml file on the source code root directory.\nModify the phases element of the appspec.yml file on the source code root directory.\nIncorrect\nYou can use AWS CodeBuild with a proxy server to regulate HTTP and HTTPS traffic to and from the Internet. To run CodeBuild with a proxy\nserver, you install a proxy server in a public subnet and CodeBuild in a private subnet in a VPC.\nBelow are possible causes of error when running CodeBuild with a proxy server:\n1. ssl-bump is not configured properly.\n2. Your organization's security policy does not allow you to use ssl-bump .\n3. Your buildspec.yml file does not have proxy settings specified usinga proxy element.\n= If you do not use ssl-bump for an explicit proxy server, add a proxy configuration to your buildspec.yml usinga proxy element."
      },
      "tags": {
        "services": [
          "EBS",
          "VPC",
          "CodeBuild",
          "CloudWatch",
          "Config"
        ],
        "domains": [
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "security",
          "policy",
          "VPC",
          "subnet"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 131354.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\n\nA mobile game developer is using DynamoDB as a data store and a Web Identity Federation for authorization and authentication. Each item in E\n\nthe DynamoDB table contains the attributes for individual user's game data such as user ID, game scores, and top score where the user ID is\n\nthe partition key. The developer must control user access to specific data items based on their IDs. In doing so, users will only be able to\n\nobtain items that they own.\n\nWhich of the following solutions must be implemented by the developer?\n\n® Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Attributes\ncondition key to the user IDs.\nModify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:LeadingKeys\ncondition key to the user IDs.\nModify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Returnvalues\ncondition key to the user IDs.\nModify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:select condition\nkey to the user IDs.\nIncorrect\nIn DynamoDB, you can control access to individual data items and attributes in a table. For example, you can do the following:\n— Grant permissions on a table, but restrict access to specific items in that table based on certain primary key values. An example might be a\nsocial networking app for games, where all users’ saved game data is stored in a single table, but no users can access data items that they do\nnot own, as shown in the following illustration:\n]",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: mobile game developer is using DynamoDB as a data store and a Web Identity Federation for authorization and authentication. Each item in E",
          "D: ynamoDB table contains the attributes for individual user's game data such as user ID, game scores, and top score where the user ID is",
          "a: rtition key. The developer must control user access to specific data items based on their IDs. In doing so, users will only be able to",
          "b: tain items that they own.",
          "c: h of the following solutions must be implemented by the developer?",
          "d: ify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Attributes",
          "c: ondition key to the user IDs.",
          "d: ify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:LeadingKeys",
          "c: ondition key to the user IDs.",
          "d: ify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Returnvalues",
          "c: ondition key to the user IDs.",
          "d: ify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:select condition",
          "D: s.",
          "c: orrect",
          "D: ynamoDB, you can control access to individual data items and attributes in a table. For example, you can do the following:",
          "a: nt permissions on a table, but restrict access to specific items in that table based on certain primary key values. An example might be a",
          "c: ial networking app for games, where all users’ saved game data is stored in a single table, but no users can access data items that they do",
          "a: s shown in the following illustration:"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "44. QUESTION\n\nCategory: CDA - Development with AWS Services\n\nA mobile game developer is using DynamoDB as a data store and a Web Identity Federation for authorization and authentication. Each item in E\n\nthe DynamoDB table contains the attributes for individual user's game data such as user ID, game scores, and top score where the user ID is\n\nthe partition key. The developer must control user access to specific data items based on their IDs. In doing so, users will only be able to\n\nobtain items that they own.\n\nWhich of the following solutions must be implemented by the developer?\n\n® Modify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Attributes\ncondition key to the user IDs.\nModify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:LeadingKeys\ncondition key to the user IDs.\nModify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:Returnvalues\ncondition key to the user IDs.\nModify the IAM Policy associated with the Identity provider's role by setting the value of the dynamodb:select condition\nkey to the user IDs.\nIncorrect\nIn DynamoDB, you can control access to individual data items and attributes in a table. For example, you can do the following:\n— Grant permissions on a table, but restrict access to specific items in that table based on certain primary key values. An example might be a\nsocial networking app for games, where all users’ saved game data is stored in a single table, but no users can access data items that they do\nnot own, as shown in the following illustration:\n]"
      },
      "tags": {
        "services": [
          "DynamoDB",
          "IAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "authentication",
          "authorization",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 131417.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA startup is integrating an event-driven alerting tool with a third-party platform. The platform requires a publicly accessible HTTPS endpoint to E\nreceive webhook requests, which will be processed by a Lambda function.\nGiven that the platform signs each request with a secret key and includes it in the headers, the developer must ensure that the Lambda\nfunction executes the domain logic only when a webhook request comes from a valid user.\nWhich action would satisfy the requirement with the least amount of development effort?\nCreate a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the\n\"lambda: FunctionUrlAuthType\": \"AWS IAM\" condition is present.\n® Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the\n\"lambda:CodeSigningConfigArn\": \"arn:aws:lambda:::code-signing-config:csc-\" condition is present.\nConfigure API Gateway to connect with the Lambda function using a Lambda proxy integration. Create a Lambda function\nauthorizer to validate incoming requests based on a signature provided in the HTTP headers.\nCreate a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the\n\"lambda: FunctionUrlAuthType\": \"NONE\" condition is present. Write a custom authorization logic based on a signature\nprovided in the HTTP headers.\nIncorrect\nIf you need a simple way to configure an HTTPS endpoint in front of your Lambda function without having to learn and configure additional\nservices besides Lambda, you can use Lambda function URLs. This can be useful in cases where you need to implement a simple webhook\nhandler or form validator that runs within an individual Lambda function and does not require additional functionality beyond processing incoming\n|\nrealiecte",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: startup is integrating an event-driven alerting tool with a third-party platform. The platform requires a publicly accessible HTTPS endpoint to E",
          "c: eive webhook requests, which will be processed by a Lambda function.",
          "a: t the platform signs each request with a secret key and includes it in the headers, the developer must ensure that the Lambda",
          "c: tion executes the domain logic only when a webhook request comes from a valid user.",
          "c: h action would satisfy the requirement with the least amount of development effort?",
          "C: reate a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the",
          "a: mbda: FunctionUrlAuthType\": \"AWS IAM\" condition is present.",
          "C: reate a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the",
          "a: mbda:CodeSigningConfigArn\": \"arn:aws:lambda:::code-signing-config:csc-\" condition is present.",
          "C: onfigure API Gateway to connect with the Lambda function using a Lambda proxy integration. Create a Lambda function",
          "a: uthorizer to validate incoming requests based on a signature provided in the HTTP headers.",
          "C: reate a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the",
          "a: mbda: FunctionUrlAuthType\": \"NONE\" condition is present. Write a custom authorization logic based on a signature",
          "d: ed in the HTTP headers.",
          "c: orrect",
          "d: a simple way to configure an HTTPS endpoint in front of your Lambda function without having to learn and configure additional",
          "c: es besides Lambda, you can use Lambda function URLs. This can be useful in cases where you need to implement a simple webhook",
          "a: ndler or form validator that runs within an individual Lambda function and does not require additional functionality beyond processing incoming",
          "a: liecte"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "45. QUESTION\nCategory: CDA - Development with AWS Services\nA startup is integrating an event-driven alerting tool with a third-party platform. The platform requires a publicly accessible HTTPS endpoint to E\nreceive webhook requests, which will be processed by a Lambda function.\nGiven that the platform signs each request with a secret key and includes it in the headers, the developer must ensure that the Lambda\nfunction executes the domain logic only when a webhook request comes from a valid user.\nWhich action would satisfy the requirement with the least amount of development effort?\nCreate a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the\n\"lambda: FunctionUrlAuthType\": \"AWS IAM\" condition is present.\n® Create a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the\n\"lambda:CodeSigningConfigArn\": \"arn:aws:lambda:::code-signing-config:csc-\" condition is present.\nConfigure API Gateway to connect with the Lambda function using a Lambda proxy integration. Create a Lambda function\nauthorizer to validate incoming requests based on a signature provided in the HTTP headers.\nCreate a Lambda function URL. Attach a resource-based policy to the function allowing anyone to invoke it only if the\n\"lambda: FunctionUrlAuthType\": \"NONE\" condition is present. Write a custom authorization logic based on a signature\nprovided in the HTTP headers.\nIncorrect\nIf you need a simple way to configure an HTTPS endpoint in front of your Lambda function without having to learn and configure additional\nservices besides Lambda, you can use Lambda function URLs. This can be useful in cases where you need to implement a simple webhook\nhandler or form validator that runs within an individual Lambda function and does not require additional functionality beyond processing incoming\n|\nrealiecte"
      },
      "tags": {
        "services": [
          "Lambda",
          "API Gateway",
          "IAM",
          "Config",
          "ECR"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "authorization",
          "policy",
          "event-driven"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 131438.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer plans to launch an EC2 instance, with Amazon Linux 2 as its AMI, using the AWS Console. A security group with port 80 that is |\nopen to public access will be associated with the instance. He wants to quickly build and test an Apache webserver with an index.html\ndisplaying a hello world message.\nWhich of the following should the developer do?\nConfigure the metadata at the creation of the EC2 instance to run a script that will install the Apache webserver after the\ninstance starts.\nConnect to the instance via port 22. Run the commands that will install and create the Apache webserver.\nConfigure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver\nafter the instance starts.\n@® Connect to the instance via port 80. Run the commands that will install and create the Apache webserver.\nIncorrect\nWhen you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common\nautomated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts\nand cloud-init directives. You can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the\ncommand line tools), or as base64-encoded text (for API calls).\n1.ChooseAMI 2. Choose Instance Type 3. Configure Instance 4. Add Storage 5.AddTags 6. Configure Security Group 7. Review\nStep 3: Configure Instance Details\nElastic Inference (i (J Add an Elastic Inference accelerator\nAdditional charges apply.\n. Credit specification (i OJ Unlimited",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer plans to launch an EC2 instance, with Amazon Linux 2 as its AMI, using the AWS Console. A security group with port 80 that is |",
          "b: lic access will be associated with the instance. He wants to quickly build and test an Apache webserver with an index.html",
          "d: isplaying a hello world message.",
          "c: h of the following should the developer do?",
          "C: onfigure the metadata at the creation of the EC2 instance to run a script that will install the Apache webserver after the",
          "a: nce starts.",
          "C: onnect to the instance via port 22. Run the commands that will install and create the Apache webserver.",
          "C: onfigure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver",
          "a: fter the instance starts.",
          "C: onnect to the instance via port 80. Run the commands that will install and create the Apache webserver.",
          "c: orrect",
          "a: unch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common",
          "a: utomated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts",
          "a: nd cloud-init directives. You can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the",
          "c: ommand line tools), or as base64-encoded text (for API calls).",
          "C: hooseAMI 2. Choose Instance Type 3. Configure Instance 4. Add Storage 5.AddTags 6. Configure Security Group 7. Review",
          "C: onfigure Instance Details",
          "a: stic Inference (i (J Add an Elastic Inference accelerator",
          "A: dditional charges apply.",
          "C: redit specification (i OJ Unlimited"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "46. QUESTION\nCategory: CDA - Development with AWS Services\nA developer plans to launch an EC2 instance, with Amazon Linux 2 as its AMI, using the AWS Console. A security group with port 80 that is |\nopen to public access will be associated with the instance. He wants to quickly build and test an Apache webserver with an index.html\ndisplaying a hello world message.\nWhich of the following should the developer do?\nConfigure the metadata at the creation of the EC2 instance to run a script that will install the Apache webserver after the\ninstance starts.\nConnect to the instance via port 22. Run the commands that will install and create the Apache webserver.\nConfigure the user data at the creation of the EC2 instance to run a script that will install and create the Apache webserver\nafter the instance starts.\n@® Connect to the instance via port 80. Run the commands that will install and create the Apache webserver.\nIncorrect\nWhen you launch an instance in Amazon EC2, you have the option of passing user data to the instance that can be used to perform common\nautomated configuration tasks and even run scripts after the instance starts. You can pass two types of user data to Amazon EC2: shell scripts\nand cloud-init directives. You can also pass this data into the launch wizard as plain text, as a file (this is useful for launching instances using the\ncommand line tools), or as base64-encoded text (for API calls).\n1.ChooseAMI 2. Choose Instance Type 3. Configure Instance 4. Add Storage 5.AddTags 6. Configure Security Group 7. Review\nStep 3: Configure Instance Details\nElastic Inference (i (J Add an Elastic Inference accelerator\nAdditional charges apply.\n. Credit specification (i OJ Unlimited"
      },
      "tags": {
        "services": [
          "EC2",
          "EBS",
          "Config"
        ],
        "domains": [],
        "keywords": [
          "security",
          "security group"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 131757.png",
      "parsed": {
        "question": "ki\nCategory: CDA - Security\nA developer needs to use IAM roles to list all EC2 instances that belong to the development environment in an AWS account.\nWhich methods could be done to verify IAM access to describe instances? (Select TWO.)\nRun the get-group-policy command.\nRun the get-role command.\nValidate the IAM role’s permission by querying the in-line policies within the EC2 instance metadata.\nUse the IAM Policy Simulator to validate the permission for the IAM role.\nRun the describe-instances command with the --dry-run parameter.\nIncorrect\nThe --dry-run parameter checks whether you have the required permissions for the action, without actually making the request, and provides\nan error response. If you have the required permissions, the error response is DryRun-Operation . Otherwise, itis\nUnauthorizedOperation .\nD:\\>aws ec2 describe-instances --dry-ru\nAn error occurred (DryRunOperation) when calling the Describelnstances operation: Request would have succeeded, but DryRun flag is set",
        "options": [
          "C: ategory: CDA - Security",
          "A: developer needs to use IAM roles to list all EC2 instances that belong to the development environment in an AWS account.",
          "c: h methods could be done to verify IAM access to describe instances? (Select TWO.)",
          "c: y command.",
          "c: ommand.",
          "a: lidate the IAM role’s permission by querying the in-line policies within the EC2 instance metadata.",
          "A: M Policy Simulator to validate the permission for the IAM role.",
          "d: escribe-instances command with the --dry-run parameter.",
          "c: orrect",
          "d: ry-run parameter checks whether you have the required permissions for the action, without actually making the request, and provides",
          "a: n error response. If you have the required permissions, the error response is DryRun-Operation . Otherwise, itis",
          "a: uthorizedOperation .",
          "D: \\>aws ec2 describe-instances --dry-ru",
          "A: n error occurred (DryRunOperation) when calling the Describelnstances operation: Request would have succeeded, but DryRun flag is set"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "47. QUESTION ki\nCategory: CDA - Security\nA developer needs to use IAM roles to list all EC2 instances that belong to the development environment in an AWS account.\nWhich methods could be done to verify IAM access to describe instances? (Select TWO.)\nRun the get-group-policy command.\nRun the get-role command.\nValidate the IAM role’s permission by querying the in-line policies within the EC2 instance metadata.\nUse the IAM Policy Simulator to validate the permission for the IAM role.\nRun the describe-instances command with the --dry-run parameter.\nIncorrect\nThe --dry-run parameter checks whether you have the required permissions for the action, without actually making the request, and provides\nan error response. If you have the required permissions, the error response is DryRun-Operation . Otherwise, itis\nUnauthorizedOperation .\nD:\\>aws ec2 describe-instances --dry-ru\nAn error occurred (DryRunOperation) when calling the Describelnstances operation: Request would have succeeded, but DryRun flag is set"
      },
      "tags": {
        "services": [
          "EC2",
          "IAM"
        ],
        "domains": [
          "Security"
        ],
        "keywords": [
          "security",
          "IAM role",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 131824.png",
      "parsed": {
        "question": "6PgG_0aRTFGZ9ZHTSufueBPzI0aa0S6751s5QbsCovir7gD7IIHTOWNHYXK_87ROsHfA@3xD\nDSUkHzyhkwZvmu_6Ny1BeFJRuBwdbIRqL-Ugh88UVg5zv2Bhw3zRDh5j8qWSMé6NeNal13cWn5_HgNwM4HNhxQOS2h4cz4Rx7t1IWFqT4Uyzxn6IK3dsbn3JuQ4aVd_jPLKW4_OWg\nh_KZCtWbBtpQE-qzL3171Ljqzz9zoU5qz-S8E1d1zDLwZgDi6tnQsWs-1_Kt7-oL6Ut9Ktk2h-DjAdkwLdmIbPjbKpJc7L19qd8Mf4SXzABeUX-AS5vIjvTGLGMdF7HI_wVéMj3\nwjtzHS6JIzC81wyAcTteGYA7sPTO5eYPT4JI9BF9DO4WAOVUN-VFUDBK704MQ5AGSEQYpX1x1beo0-Bm50wqTtKs9SIfuspbN22V1xxbIP9i0ON27UMULcgzUMo5yEJAwWajmM1tdZ\na1BH-TcQKCnptYdNUMIU6_15anyKEGv8V053hj6daWwQ501SA3k0X807 qUsmzarrZetAuR_KT9QEqOoK40WUgrvLIEPbagr—e—Anyal41uzZvsY84171-z_M11P3I3pvIWUIXL5\n6QaVq_1hoC9bSLOHK6LZm4KtoCXahq7gsS2_RtxMdHWDRWPXqTANVUBTILKQF6UIaTLY1LV_1reV4zeY_@3gtAXvHbYzLv6H7yg\nAlong with the response is an additional failure message displayed in ciphertext format.\nHow can the developer decode the message?\n@® Decode the message by calling the AWS KMS decrypt command.\nDecode the message by calling the AWS IAM decode-authorization-message command.\nDecode the message using an external cryptography library.\nDecode the message by calling the AWS STS decode-authorization-message command.\nIncorrect\nThe AWS STS DecodeAuthorizationMessage API decodes additional information about the authorization status of a request from an",
        "options": [
          "C: ategory: CDA - Security",
          "A: developer is using the AWS CLI to interact with different AWS services. An UnauthorizedOperation error, as shown below, is received",
          "a: fter running the stop-instance command: B",
          "a: ls Dojo Philippines > aws ec2 stop-instances --instance-ids i-@flcfffal9d96H7yg",
          "A: n error occurred (UnauthorizedOperation) when calling the StopInstances operation: You are not authorized to perform this operation.",
          "c: oded authorization failure message: SbwU2Mc9-D-Yb2S7STN3JPQ6PgG_0aRTFGZ9ZHTSufueBPzI0aa0S6751s5QbsCovir7gD7IIHTOWNHYXK_87ROsHfA@3xD",
          "D: SUkHzyhkwZvmu_6Ny1BeFJRuBwdbIRqL-Ugh88UVg5zv2Bhw3zRDh5j8qWSMé6NeNal13cWn5_HgNwM4HNhxQOS2h4cz4Rx7t1IWFqT4Uyzxn6IK3dsbn3JuQ4aVd_jPLKW4_OWg",
          "C: tWbBtpQE-qzL3171Ljqzz9zoU5qz-S8E1d1zDLwZgDi6tnQsWs-1_Kt7-oL6Ut9Ktk2h-DjAdkwLdmIbPjbKpJc7L19qd8Mf4SXzABeUX-AS5vIjvTGLGMdF7HI_wVéMj3",
          "C: 81wyAcTteGYA7sPTO5eYPT4JI9BF9DO4WAOVUN-VFUDBK704MQ5AGSEQYpX1x1beo0-Bm50wqTtKs9SIfuspbN22V1xxbIP9i0ON27UMULcgzUMo5yEJAwWajmM1tdZ",
          "a: 1BH-TcQKCnptYdNUMIU6_15anyKEGv8V053hj6daWwQ501SA3k0X807 qUsmzarrZetAuR_KT9QEqOoK40WUgrvLIEPbagr—e—Anyal41uzZvsY84171-z_M11P3I3pvIWUIXL5",
          "a: Vq_1hoC9bSLOHK6LZm4KtoCXahq7gsS2_RtxMdHWDRWPXqTANVUBTILKQF6UIaTLY1LV_1reV4zeY_@3gtAXvHbYzLv6H7yg",
          "A: long with the response is an additional failure message displayed in ciphertext format.",
          "c: an the developer decode the message?",
          "D: ecode the message by calling the AWS KMS decrypt command.",
          "D: ecode the message by calling the AWS IAM decode-authorization-message command.",
          "D: ecode the message using an external cryptography library.",
          "D: ecode the message by calling the AWS STS decode-authorization-message command.",
          "c: orrect",
          "A: WS STS DecodeAuthorizationMessage API decodes additional information about the authorization status of a request from an"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "Category: CDA - Security\nA developer is using the AWS CLI to interact with different AWS services. An UnauthorizedOperation error, as shown below, is received\nafter running the stop-instance command: B\njon@Tutorials Dojo Philippines > aws ec2 stop-instances --instance-ids i-@flcfffal9d96H7yg\nAn error occurred (UnauthorizedOperation) when calling the StopInstances operation: You are not authorized to perform this operation.\nEncoded authorization failure message: SbwU2Mc9-D-Yb2S7STN3JPQ6PgG_0aRTFGZ9ZHTSufueBPzI0aa0S6751s5QbsCovir7gD7IIHTOWNHYXK_87ROsHfA@3xD\nDSUkHzyhkwZvmu_6Ny1BeFJRuBwdbIRqL-Ugh88UVg5zv2Bhw3zRDh5j8qWSMé6NeNal13cWn5_HgNwM4HNhxQOS2h4cz4Rx7t1IWFqT4Uyzxn6IK3dsbn3JuQ4aVd_jPLKW4_OWg\nh_KZCtWbBtpQE-qzL3171Ljqzz9zoU5qz-S8E1d1zDLwZgDi6tnQsWs-1_Kt7-oL6Ut9Ktk2h-DjAdkwLdmIbPjbKpJc7L19qd8Mf4SXzABeUX-AS5vIjvTGLGMdF7HI_wVéMj3\nwjtzHS6JIzC81wyAcTteGYA7sPTO5eYPT4JI9BF9DO4WAOVUN-VFUDBK704MQ5AGSEQYpX1x1beo0-Bm50wqTtKs9SIfuspbN22V1xxbIP9i0ON27UMULcgzUMo5yEJAwWajmM1tdZ\na1BH-TcQKCnptYdNUMIU6_15anyKEGv8V053hj6daWwQ501SA3k0X807 qUsmzarrZetAuR_KT9QEqOoK40WUgrvLIEPbagr—e—Anyal41uzZvsY84171-z_M11P3I3pvIWUIXL5\n6QaVq_1hoC9bSLOHK6LZm4KtoCXahq7gsS2_RtxMdHWDRWPXqTANVUBTILKQF6UIaTLY1LV_1reV4zeY_@3gtAXvHbYzLv6H7yg\nAlong with the response is an additional failure message displayed in ciphertext format.\nHow can the developer decode the message?\n@® Decode the message by calling the AWS KMS decrypt command.\nDecode the message by calling the AWS IAM decode-authorization-message command.\nDecode the message using an external cryptography library.\nDecode the message by calling the AWS STS decode-authorization-message command.\nIncorrect\nThe AWS STS DecodeAuthorizationMessage API decodes additional information about the authorization status of a request from an"
      },
      "tags": {
        "services": [
          "EC2",
          "IAM",
          "KMS",
          "ECR"
        ],
        "domains": [
          "Security"
        ],
        "keywords": [
          "security",
          "authorization"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 132126.png",
      "parsed": {
        "question": "[\nCategory: CDA - Development with AWS Services\nA company is planning to launch an online cross-platform game that expects millions of users. The developer wants to use an in-house\nauthentication system for user identification. Each user identifier must be kept consistent across devices and platforms.\nHow can the developer achieve this?\nGenerate a unique IAM access key for each user and use the access key ID as the unique identifier.\nUse developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users.\n@® Create an IAM Role for each user and use its Amazon Resource Name (ARN) as unique identifiers.\nGenerate a universally unique identifier (UUID) for each device. Store the UUID with the user in a DynamoDB table.\nIncorrect\nAmazon Cognito supports developer authenticated identities, in addition to web identity federation through Facebook (Identity Pools), Google\n(Identity Pools), Login with Amazon (Identity Pools), and Sign in with Apple (Identity Pools). With developer authenticated identities, you can\nregister and authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access\nAWS resources. Using developer authenticated identities involves interaction between the end-user device, your backend for authentication, and\nAmazon Cognito.\nDevelopers can use their own authentication system with Cognito. What this means is that your app can benefit from all of the features of Amazon\nCognito while utilizing your own authentication system. This works by your app requesting a unique identity ID for your end-users based on the\nidentifier you use in your own authentication system. You can use the Cognito identity ID to save and synchronize user data across devices with\nny",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: company is planning to launch an online cross-platform game that expects millions of users. The developer wants to use an in-house",
          "a: uthentication system for user identification. Each user identifier must be kept consistent across devices and platforms.",
          "c: an the developer achieve this?",
          "a: te a unique IAM access key for each user and use the access key ID as the unique identifier.",
          "d: eveloper-authenticated identities in Amazon Cognito to generate unique identifiers for the users.",
          "C: reate an IAM Role for each user and use its Amazon Resource Name (ARN) as unique identifiers.",
          "a: te a universally unique identifier (UUID) for each device. Store the UUID with the user in a DynamoDB table.",
          "c: orrect",
          "A: mazon Cognito supports developer authenticated identities, in addition to web identity federation through Facebook (Identity Pools), Google",
          "d: entity Pools), Login with Amazon (Identity Pools), and Sign in with Apple (Identity Pools). With developer authenticated identities, you can",
          "a: nd authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access",
          "A: WS resources. Using developer authenticated identities involves interaction between the end-user device, your backend for authentication, and",
          "A: mazon Cognito.",
          "D: evelopers can use their own authentication system with Cognito. What this means is that your app can benefit from all of the features of Amazon",
          "C: ognito while utilizing your own authentication system. This works by your app requesting a unique identity ID for your end-users based on the",
          "d: entifier you use in your own authentication system. You can use the Cognito identity ID to save and synchronize user data across devices with"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "Gis)\n51. QUESTION [\nCategory: CDA - Development with AWS Services\nA company is planning to launch an online cross-platform game that expects millions of users. The developer wants to use an in-house\nauthentication system for user identification. Each user identifier must be kept consistent across devices and platforms.\nHow can the developer achieve this?\nGenerate a unique IAM access key for each user and use the access key ID as the unique identifier.\nUse developer-authenticated identities in Amazon Cognito to generate unique identifiers for the users.\n@® Create an IAM Role for each user and use its Amazon Resource Name (ARN) as unique identifiers.\nGenerate a universally unique identifier (UUID) for each device. Store the UUID with the user in a DynamoDB table.\nIncorrect\nAmazon Cognito supports developer authenticated identities, in addition to web identity federation through Facebook (Identity Pools), Google\n(Identity Pools), Login with Amazon (Identity Pools), and Sign in with Apple (Identity Pools). With developer authenticated identities, you can\nregister and authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access\nAWS resources. Using developer authenticated identities involves interaction between the end-user device, your backend for authentication, and\nAmazon Cognito.\nDevelopers can use their own authentication system with Cognito. What this means is that your app can benefit from all of the features of Amazon\nCognito while utilizing your own authentication system. This works by your app requesting a unique identity ID for your end-users based on the\nidentifier you use in your own authentication system. You can use the Cognito identity ID to save and synchronize user data across devices with\nny"
      },
      "tags": {
        "services": [
          "DynamoDB",
          "IAM",
          "Cognito"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "authentication",
          "IAM role"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 132213.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA microservices application's Customer and Payment service components have two separate DynamoDB tables. New items inserted into the [\nCustomer service table must be dynamically updated in the Payment service table.\nHow can the Payment service get near real-time update\nEnable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to\nupdate the Payment service table.\nUse a Kinesis data stream to stream all the changes from the Customer service database directly into the Payment service\ntable.\nCreate a Firehose stream to stream all the changes from the Customer service table and trigger a Lambda function to update\nthe Payment service table.\n® Create a scheduled Amazon EventBridge (Amazon CloudWatch Events) rule that invokes a Lambda function every minute to\nupdate the changes from the Customer service table into the Payment service table.\nIncorrect\nDynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for\nup to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real-time.\nAmazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in\nDynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.\nIf you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you\nwrite. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes\nyour Lambda function synchronously when it detects new stream records.",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: microservices application's Customer and Payment service components have two separate DynamoDB tables. New items inserted into the [",
          "C: ustomer service table must be dynamically updated in the Payment service table.",
          "c: an the Payment service get near real-time update",
          "a: ble DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to",
          "d: ate the Payment service table.",
          "a: Kinesis data stream to stream all the changes from the Customer service database directly into the Payment service",
          "a: ble.",
          "C: reate a Firehose stream to stream all the changes from the Customer service table and trigger a Lambda function to update",
          "a: yment service table.",
          "C: reate a scheduled Amazon EventBridge (Amazon CloudWatch Events) rule that invokes a Lambda function every minute to",
          "d: ate the changes from the Customer service table into the Payment service table.",
          "c: orrect",
          "D: ynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for",
          "A: pplications can access this log and view the data items as they appeared before and after they were modified, in near-real-time.",
          "A: mazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in",
          "D: ynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.",
          "a: ble DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you",
          "d: iately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes",
          "a: mbda function synchronously when it detects new stream records."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "53. QUESTION\nCategory: CDA - Development with AWS Services\nA microservices application's Customer and Payment service components have two separate DynamoDB tables. New items inserted into the [\nCustomer service table must be dynamically updated in the Payment service table.\nHow can the Payment service get near real-time update\nEnable DynamoDB Streams to stream all the changes from the Customer service table and trigger a Lambda function to\nupdate the Payment service table.\nUse a Kinesis data stream to stream all the changes from the Customer service database directly into the Payment service\ntable.\nCreate a Firehose stream to stream all the changes from the Customer service table and trigger a Lambda function to update\nthe Payment service table.\n® Create a scheduled Amazon EventBridge (Amazon CloudWatch Events) rule that invokes a Lambda function every minute to\nupdate the changes from the Customer service table into the Payment service table.\nIncorrect\nDynamoDB Streams captures a time-ordered sequence of item-level modifications in any DynamoDB table and stores this information in a log for\nup to 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near-real-time.\nAmazon DynamoDB is integrated with AWS Lambda so that you can create triggers—pieces of code that automatically respond to events in\nDynamoDB Streams. With triggers, you can build applications that react to data modifications in DynamoDB tables.\nIf you enable DynamoDB Streams on a table, you can associate the stream Amazon Resource Name (ARN) with an AWS Lambda function that you\nwrite. Immediately after an item in the table is modified, a new record appears in the table's stream. AWS Lambda polls the stream and invokes\nyour Lambda function synchronously when it detects new stream records."
      },
      "tags": {
        "services": [
          "Lambda",
          "RDS",
          "DynamoDB",
          "EventBridge",
          "Kinesis",
          "CloudWatch"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "microservices",
          "synchronous"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 132441.png",
      "parsed": {
        "question": "E\nCategory: CDA - Troubleshooting and Optimization\nA developer has enabled the lifecycle policy of an application deployed in Elastic Beanstalk. The lifecycle is set to limit the application version\nto 15 versions. The developer wants to keep the source code in an S3 bucket, yet, it gets deleted.\nWhat change should the developer do?\nConfigure the Retention setting to retain the source bundle in the S3 bucket.\n® Modify the value of the set the application versions limit by age option to zero.\nModify the value of the set application versions limit by the total count option to zero.\nTrigger a Lambda function to copy the source code to another S3 bucket.\nIncorrect\nEach time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application\nversion. If you don’t delete versions that you no longer use, you will eventually reach the application version quota and be unable to create new\nversions of that application.\nYou can avoid hitting the quota by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to\ndelete application versions that are old or to delete application versions when the total number of versions for an application exceeds a specified\nnumber.\nElastic Beanstalk applies an application's lifecycle policy each time you create a new application version and deletes up to 100 versions each time\n. the lifecycle policy is applied. Elastic Beanstalk deletes old versions after creating the new version and does not count the new version towards the",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: developer has enabled the lifecycle policy of an application deployed in Elastic Beanstalk. The lifecycle is set to limit the application version",
          "d: eveloper wants to keep the source code in an S3 bucket, yet, it gets deleted.",
          "a: t change should the developer do?",
          "C: onfigure the Retention setting to retain the source bundle in the S3 bucket.",
          "d: ify the value of the set the application versions limit by age option to zero.",
          "d: ify the value of the set application versions limit by the total count option to zero.",
          "a: Lambda function to copy the source code to another S3 bucket.",
          "c: orrect",
          "a: ch time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application",
          "d: on’t delete versions that you no longer use, you will eventually reach the application version quota and be unable to create new",
          "a: t application.",
          "c: an avoid hitting the quota by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to",
          "d: elete application versions that are old or to delete application versions when the total number of versions for an application exceeds a specified",
          "b: er.",
          "a: stic Beanstalk applies an application's lifecycle policy each time you create a new application version and deletes up to 100 versions each time",
          "c: ycle policy is applied. Elastic Beanstalk deletes old versions after creating the new version and does not count the new version towards the"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "hil\n55. QUESTION E\nCategory: CDA - Troubleshooting and Optimization\nA developer has enabled the lifecycle policy of an application deployed in Elastic Beanstalk. The lifecycle is set to limit the application version\nto 15 versions. The developer wants to keep the source code in an S3 bucket, yet, it gets deleted.\nWhat change should the developer do?\nConfigure the Retention setting to retain the source bundle in the S3 bucket.\n® Modify the value of the set the application versions limit by age option to zero.\nModify the value of the set application versions limit by the total count option to zero.\nTrigger a Lambda function to copy the source code to another S3 bucket.\nIncorrect\nEach time you upload a new version of your application with the Elastic Beanstalk console or the EB CLI, Elastic Beanstalk creates an application\nversion. If you don’t delete versions that you no longer use, you will eventually reach the application version quota and be unable to create new\nversions of that application.\nYou can avoid hitting the quota by applying an application version lifecycle policy to your applications. A lifecycle policy tells Elastic Beanstalk to\ndelete application versions that are old or to delete application versions when the total number of versions for an application exceeds a specified\nnumber.\nElastic Beanstalk applies an application's lifecycle policy each time you create a new application version and deletes up to 100 versions each time\n. the lifecycle policy is applied. Elastic Beanstalk deletes old versions after creating the new version and does not count the new version towards the"
      },
      "tags": {
        "services": [
          "Lambda",
          "Elastic Beanstalk",
          "S3",
          "RDS",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 132502.png",
      "parsed": {
        "question": "E\nCategory: CDA - Development with AWS Services\nA Lamba function has multiple sub-functions that are chained together to process large data synchronously. When invoked, the function tends\nto exceed its maximum timeout limit. This has prompted the developer to break the Lambda function into manageable coordinated states using\nStep Functions, enabling each sub-function to run in separate processes.\nWhich of the following type of states should the developer use to run processes?\nWait State\nPass State\n® Parallel State\nTask State\nIncorrect\nAWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into\nbusiness-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that\nmaintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by\nyour business logic.\nStep Functions can help solve the problem of timeout errors of Lambda functions. Imagine a Lambda function that has four utility functions that\nare run sequentially. Each of those functions takes 5 minutes to finish which translates to a total execution time of 20 minutes. This is a problem\nsince Lambda can only run for a maximum of 15 minutes. To solve this, we can refactor the functions inside the Lambda function into individual\nStep Functions states. This way, each function is contained in a separate Lambda function, which has its own execution timeout.\n]",
        "options": [
          "a: A 4 a4",
          "C: ategory: CDA - Development with AWS Services",
          "A: Lamba function has multiple sub-functions that are chained together to process large data synchronously. When invoked, the function tends",
          "c: eed its maximum timeout limit. This has prompted the developer to break the Lambda function into manageable coordinated states using",
          "c: tions, enabling each sub-function to run in separate processes.",
          "c: h of the following type of states should the developer use to run processes?",
          "a: it State",
          "a: ss State",
          "a: rallel State",
          "a: sk State",
          "c: orrect",
          "A: WS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into",
          "b: usiness-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that",
          "a: intain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by",
          "b: usiness logic.",
          "c: tions can help solve the problem of timeout errors of Lambda functions. Imagine a Lambda function that has four utility functions that",
          "a: re run sequentially. Each of those functions takes 5 minutes to finish which translates to a total execution time of 20 minutes. This is a problem",
          "c: e Lambda can only run for a maximum of 15 minutes. To solve this, we can refactor the functions inside the Lambda function into individual",
          "c: tions states. This way, each function is contained in a separate Lambda function, which has its own execution timeout."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "aA 4 a4\n56. QUESTION E\nCategory: CDA - Development with AWS Services\nA Lamba function has multiple sub-functions that are chained together to process large data synchronously. When invoked, the function tends\nto exceed its maximum timeout limit. This has prompted the developer to break the Lambda function into manageable coordinated states using\nStep Functions, enabling each sub-function to run in separate processes.\nWhich of the following type of states should the developer use to run processes?\nWait State\nPass State\n® Parallel State\nTask State\nIncorrect\nAWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into\nbusiness-critical applications. Through its visual interface, you can create and run a series of checkpointed and event-driven workflows that\nmaintain the application state. The output of one step acts as an input to the next. Each step in your application executes in order, as defined by\nyour business logic.\nStep Functions can help solve the problem of timeout errors of Lambda functions. Imagine a Lambda function that has four utility functions that\nare run sequentially. Each of those functions takes 5 minutes to finish which translates to a total execution time of 20 minutes. This is a problem\nsince Lambda can only run for a maximum of 15 minutes. To solve this, we can refactor the functions inside the Lambda function into individual\nStep Functions states. This way, each function is contained in a separate Lambda function, which has its own execution timeout.\n]"
      },
      "tags": {
        "services": [
          "Lambda",
          "Step Functions"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "event-driven",
          "synchronous"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 133219.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\n\nAn application is hosted in the us-east-1 region. The app needs to be recreated on the us-east-2, ap-northeast-1, and ap-southeast-1 region E\n\nusing the same Amazon Machine Image (AMI). As the developer, you have to use AWS CloudFormation to rebuild the application using a\n\ntemplate.\n\nWhich of the following actions is the most suitable way to configure the CloudFormation template for the scenario?\nCopy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,\nadd a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in\nmapping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region\nkey.\nCopy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,\nadd a Parameters section wherein you will define the different Image Id for the three regions. Use the region name as the key\nin mapping to its correct Image Id. Lastly, use the ref function to retrieve the desired Image Id from the region key.\nCopy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,\n\n® add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in\nmapping to its correct Image Id. Lastly, use the Fn::Importvalue function to retrieve the desired Image Id from the region\nkey.\nCopy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,\nadd a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in\nmapping to its correct Image Id. Lastly, use the Fn::Getatt function to retrieve the desired Image Id from the region key.\nIncorrect",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: n application is hosted in the us-east-1 region. The app needs to be recreated on the us-east-2, ap-northeast-1, and ap-southeast-1 region E",
          "a: me Amazon Machine Image (AMI). As the developer, you have to use AWS CloudFormation to rebuild the application using a",
          "a: te.",
          "c: h of the following actions is the most suitable way to configure the CloudFormation template for the scenario?",
          "C: opy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,",
          "a: dd a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in",
          "a: pping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region",
          "C: opy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,",
          "a: dd a Parameters section wherein you will define the different Image Id for the three regions. Use the region name as the key",
          "a: pping to its correct Image Id. Lastly, use the ref function to retrieve the desired Image Id from the region key.",
          "C: opy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,",
          "a: dd a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in",
          "a: pping to its correct Image Id. Lastly, use the Fn::Importvalue function to retrieve the desired Image Id from the region",
          "C: opy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,",
          "a: dd a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in",
          "a: pping to its correct Image Id. Lastly, use the Fn::Getatt function to retrieve the desired Image Id from the region key.",
          "c: orrect"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "59. QUESTION\n\nCategory: CDA - Development with AWS Services\n\nAn application is hosted in the us-east-1 region. The app needs to be recreated on the us-east-2, ap-northeast-1, and ap-southeast-1 region E\n\nusing the same Amazon Machine Image (AMI). As the developer, you have to use AWS CloudFormation to rebuild the application using a\n\ntemplate.\n\nWhich of the following actions is the most suitable way to configure the CloudFormation template for the scenario?\nCopy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,\nadd a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in\nmapping to its correct Image Id. Lastly, use the Fn::FindInMap function to retrieve the desired Image Id from the region\nkey.\nCopy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,\nadd a Parameters section wherein you will define the different Image Id for the three regions. Use the region name as the key\nin mapping to its correct Image Id. Lastly, use the ref function to retrieve the desired Image Id from the region key.\nCopy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,\n\n® add a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in\nmapping to its correct Image Id. Lastly, use the Fn::Importvalue function to retrieve the desired Image Id from the region\nkey.\nCopy the AMI of the instance from the us-east-1 region to the us-east-2, ap-northeast-1, and ap-southeast-1 region. Then,\nadd a Mappings section wherein you will define the different Image Id for the three regions. Use the region name as the key in\nmapping to its correct Image Id. Lastly, use the Fn::Getatt function to retrieve the desired Image Id from the region key.\nIncorrect"
      },
      "tags": {
        "services": [
          "CloudFormation",
          "Config",
          "ECR",
          "SAM"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 133310.png",
      "parsed": {
        "question": "+\nCategory: CDA - Development with AWS Services\nA developer is building a serverless application that will send out a newsletter to customers using AWS Lambda. The Lambda function will be\ninvoked at a 7-day interval.\nWhich method will provide an automated and serverless approach to trigger the function?\nAdd an environment variable named DAYS for the Lambda function and set its value to 7.\n® Implement a task timer using Step Functions that will send a newsletter every week.\nRun a cron job in an Amazon EC2 instance that will trigger the Lambda function every week.\nConfigure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda\nfunction.\nIncorrect\nAmazon EventBridge (Amazon CloudWatch Events) help you to respond to state changes in your AWS resources. When your resources change\nstate, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your\nAWS Lambda function to take action.\nEvent Source Targets\nBuld or customize an Event Pate or set a Schedule o invoke Targets. ‘Select Target 0 invoke when an event matches your Event Pate or when schedule fs\ntriggered.\nEventPatem © @ Schedic @\n‘Lambda function ~ o\nCEra—— Ce f— :\nCron expression [0/5 = = # 7 +\nLear more about Cloudalch Events scheduiss. . > Configure version/alias\n» Show sample event(s) * Configure input",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer is building a serverless application that will send out a newsletter to customers using AWS Lambda. The Lambda function will be",
          "d: at a 7-day interval.",
          "c: h method will provide an automated and serverless approach to trigger the function?",
          "A: dd an environment variable named DAYS for the Lambda function and set its value to 7.",
          "a: task timer using Step Functions that will send a newsletter every week.",
          "a: cron job in an Amazon EC2 instance that will trigger the Lambda function every week.",
          "C: onfigure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda",
          "c: tion.",
          "c: orrect",
          "A: mazon EventBridge (Amazon CloudWatch Events) help you to respond to state changes in your AWS resources. When your resources change",
          "a: te, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your",
          "A: WS Lambda function to take action.",
          "c: e Targets",
          "B: uld or customize an Event Pate or set a Schedule o invoke Targets. ‘Select Target 0 invoke when an event matches your Event Pate or when schedule fs",
          "d: EventPatem © @ Schedic @",
          "a: mbda function ~ o",
          "C: Era—— Ce f— :",
          "C: ron expression [0/5 = = # 7 +",
          "a: r more about Cloudalch Events scheduiss. . > Configure version/alias",
          "a: mple event(s) * Configure input"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "60. QUESTION +\nCategory: CDA - Development with AWS Services\nA developer is building a serverless application that will send out a newsletter to customers using AWS Lambda. The Lambda function will be\ninvoked at a 7-day interval.\nWhich method will provide an automated and serverless approach to trigger the function?\nAdd an environment variable named DAYS for the Lambda function and set its value to 7.\n® Implement a task timer using Step Functions that will send a newsletter every week.\nRun a cron job in an Amazon EC2 instance that will trigger the Lambda function every week.\nConfigure a scheduled Amazon EventBridge (Amazon CloudWatch Events) that triggers every week to invoke the Lambda\nfunction.\nIncorrect\nAmazon EventBridge (Amazon CloudWatch Events) help you to respond to state changes in your AWS resources. When your resources change\nstate, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your\nAWS Lambda function to take action.\nEvent Source Targets\nBuld or customize an Event Pate or set a Schedule o invoke Targets. ‘Select Target 0 invoke when an event matches your Event Pate or when schedule fs\ntriggered.\nEventPatem © @ Schedic @\n‘Lambda function ~ o\nCEra—— Ce f— :\nCron expression [0/5 = = # 7 +\nLear more about Cloudalch Events scheduiss. . > Configure version/alias\n» Show sample event(s) * Configure input"
      },
      "tags": {
        "services": [
          "EC2",
          "Lambda",
          "EventBridge",
          "Step Functions",
          "CloudWatch",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 133420.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services E\nA Ruby developer is looking to offload some of the processing on his application to the AWS cloud without managing any servers. The\nsubmodules must be written in Ruby, which mainly invokes API calls to an external web service. The response from the API call is parsed and\nstored in a MongoDB database.\nWhat should he do to develop the Lambda function in his preferred programming language?\n® Create a Lambda function on Ruby with a custom runtime and use the AWS SDK for Ruby.\nCreate a Lambda function with a supported runtime version for Ruby.\nCreate a Lambda function using the AWS SDK for Ruby.\nCreate a Lambda function with a custom runtime to use Ruby. Then include the runtime in the function's deployment\npackage. Migrate it to a layer that you manage independently from the function.\nIncorrect\nAWS Lambda natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code, and provides a Runtime API, which allows you to use\nany additional programming languages to author your functions.\n= E from scratch ° Use a blueprint Browse ¢\n‘Start with a simple Hello World example. Build a Lambda application from sample code and Deploy as:\nLatest supported a\nNET Core 3.1 (C#/PowerShell)\nGo1x\nJava 11 (Corretto)\nNodejs 12x\ni Python 3.8",
        "options": [
          "C: ategory: CDA - Development with AWS Services E",
          "A: Ruby developer is looking to offload some of the processing on his application to the AWS cloud without managing any servers. The",
          "b: modules must be written in Ruby, which mainly invokes API calls to an external web service. The response from the API call is parsed and",
          "d: in a MongoDB database.",
          "a: t should he do to develop the Lambda function in his preferred programming language?",
          "C: reate a Lambda function on Ruby with a custom runtime and use the AWS SDK for Ruby.",
          "C: reate a Lambda function with a supported runtime version for Ruby.",
          "C: reate a Lambda function using the AWS SDK for Ruby.",
          "C: reate a Lambda function with a custom runtime to use Ruby. Then include the runtime in the function's deployment",
          "a: ckage. Migrate it to a layer that you manage independently from the function.",
          "c: orrect",
          "A: WS Lambda natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code, and provides a Runtime API, which allows you to use",
          "a: ny additional programming languages to author your functions.",
          "c: ratch ° Use a blueprint Browse ¢",
          "a: rt with a simple Hello World example. Build a Lambda application from sample code and Deploy as:",
          "a: test supported a",
          "C: ore 3.1 (C#/PowerShell)",
          "a: va 11 (Corretto)",
          "d: ejs 12x"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "62. QUESTION\nCategory: CDA - Development with AWS Services E\nA Ruby developer is looking to offload some of the processing on his application to the AWS cloud without managing any servers. The\nsubmodules must be written in Ruby, which mainly invokes API calls to an external web service. The response from the API call is parsed and\nstored in a MongoDB database.\nWhat should he do to develop the Lambda function in his preferred programming language?\n® Create a Lambda function on Ruby with a custom runtime and use the AWS SDK for Ruby.\nCreate a Lambda function with a supported runtime version for Ruby.\nCreate a Lambda function using the AWS SDK for Ruby.\nCreate a Lambda function with a custom runtime to use Ruby. Then include the runtime in the function's deployment\npackage. Migrate it to a layer that you manage independently from the function.\nIncorrect\nAWS Lambda natively supports Java, Go, PowerShell, Node.js, C#, Python, and Ruby code, and provides a Runtime API, which allows you to use\nany additional programming languages to author your functions.\n= E from scratch ° Use a blueprint Browse ¢\n‘Start with a simple Hello World example. Build a Lambda application from sample code and Deploy as:\nLatest supported a\nNET Core 3.1 (C#/PowerShell)\nGo1x\nJava 11 (Corretto)\nNodejs 12x\ni Python 3.8"
      },
      "tags": {
        "services": [
          "Lambda",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 133448.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services .\nA developer is managing several microservices built using API Gateway and AWS Lambda. The Developer wants to deploy new updates to one ii\nof the APIs. He wants to ensure a smooth transition between the versions by giving users enough time to migrate to the new version before\nretiring the previous one.\nWhich solution should the developer implement?\nImplement the updates on the Lambda function. Create a CloudFront distribution and use the function as the origin.\nImplement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target\nresource then redeploy it to the same stage.\n® Implement the updates and publish a new version of the Lambda function. Then, issue the new Lambda invocation URL.\nImplement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target\nresource then redeploy it to a new stage.\nIncorrect\nTo deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for\nexample, dev , prod , beta , v2 ).API stages are identified by the API ID and stage name. They're included in the URL that you use to\ninvoke the API. Each stage is a named reference to an API deployment and is made available for client applications to call.\nos Amazon API Gateway | APIs > test poudmenD) > Stages > v2\nAPs sto v2 Stage Editor\nCustom Domain Names LE Wl\nVPC Links\nSettings Logs/Tracing Stage Variables ~~ SDK Generation Export Deployment History ~~ Documentatio\nAptest Cache Settings.",
        "options": [
          "C: ategory: CDA - Development with AWS Services .",
          "A: developer is managing several microservices built using API Gateway and AWS Lambda. The Developer wants to deploy new updates to one ii",
          "A: PIs. He wants to ensure a smooth transition between the versions by giving users enough time to migrate to the new version before",
          "c: h solution should the developer implement?",
          "d: ates on the Lambda function. Create a CloudFront distribution and use the function as the origin.",
          "d: ates and publish a new version of the Lambda function. Specify the new version in the API Gateway target",
          "c: e then redeploy it to the same stage.",
          "d: ates and publish a new version of the Lambda function. Then, issue the new Lambda invocation URL.",
          "d: ates and publish a new version of the Lambda function. Specify the new version in the API Gateway target",
          "c: e then redeploy it to a new stage.",
          "c: orrect",
          "d: eploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for",
          "a: mple, dev , prod , beta , v2 ).API stages are identified by the API ID and stage name. They're included in the URL that you use to",
          "A: PI. Each stage is a named reference to an API deployment and is made available for client applications to call.",
          "A: mazon API Gateway | APIs > test poudmenD) > Stages > v2",
          "A: Ps sto v2 Stage Editor",
          "C: ustom Domain Names LE Wl",
          "C: Links",
          "a: cing Stage Variables ~~ SDK Generation Export Deployment History ~~ Documentatio",
          "A: ptest Cache Settings."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "63. QUESTION\nCategory: CDA - Development with AWS Services .\nA developer is managing several microservices built using API Gateway and AWS Lambda. The Developer wants to deploy new updates to one ii\nof the APIs. He wants to ensure a smooth transition between the versions by giving users enough time to migrate to the new version before\nretiring the previous one.\nWhich solution should the developer implement?\nImplement the updates on the Lambda function. Create a CloudFront distribution and use the function as the origin.\nImplement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target\nresource then redeploy it to the same stage.\n® Implement the updates and publish a new version of the Lambda function. Then, issue the new Lambda invocation URL.\nImplement the updates and publish a new version of the Lambda function. Specify the new version in the API Gateway target\nresource then redeploy it to a new stage.\nIncorrect\nTo deploy an API, you create an API deployment and associate it with a stage. A stage is a logical reference to a lifecycle state of your API (for\nexample, dev , prod , beta , v2 ).API stages are identified by the API ID and stage name. They're included in the URL that you use to\ninvoke the API. Each stage is a named reference to an API deployment and is made available for client applications to call.\nos Amazon API Gateway | APIs > test poudmenD) > Stages > v2\nAPs sto v2 Stage Editor\nCustom Domain Names LE Wl\nVPC Links\nSettings Logs/Tracing Stage Variables ~~ SDK Generation Export Deployment History ~~ Documentatio\nAptest Cache Settings."
      },
      "tags": {
        "services": [
          "Lambda",
          "VPC",
          "CloudFront",
          "API Gateway",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "microservices",
          "deployment",
          "VPC"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 165141.png",
      "parsed": {
        "question": "A company has launched a new serverless application using AWS Lambda. The app ran smoothly for a few weeks until it was featured on a\npopular website. As its popularity grew, so did the number of users receiving an error. Upon viewing the Lambda function’s monitoring graph,\nthe developer discovered a lot of throttled invocation requests.\nWhat can the developer do to troubleshoot this issue? (Select THREE.)\nRequest a service quota increase\nUse exponential backoff in the application.\nDeploy the Lambda function in VPC\nConfigure reserved concurrency\nUse a compiled language like GoLang to improve the function's performance\nIncrease Lambda function timeout\nIncorrect\nAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\nWith Lambda, you can run code for virtually any type of application or backend service — all with zero administration. Just upload your code, and\nLambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from\nother AWS services or call it directly from any web or mobile app.",
        "options": [
          "A: company has launched a new serverless application using AWS Lambda. The app ran smoothly for a few weeks until it was featured on a",
          "a: r website. As its popularity grew, so did the number of users receiving an error. Upon viewing the Lambda function’s monitoring graph,",
          "d: eveloper discovered a lot of throttled invocation requests.",
          "a: t can the developer do to troubleshoot this issue? (Select THREE.)",
          "a: service quota increase",
          "a: l backoff in the application.",
          "D: eploy the Lambda function in VPC",
          "C: onfigure reserved concurrency",
          "a: compiled language like GoLang to improve the function's performance",
          "c: rease Lambda function timeout",
          "c: orrect",
          "A: WS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.",
          "a: mbda, you can run code for virtually any type of application or backend service — all with zero administration. Just upload your code, and",
          "a: mbda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from",
          "A: WS services or call it directly from any web or mobile app."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "1. QUESTION\nA company has launched a new serverless application using AWS Lambda. The app ran smoothly for a few weeks until it was featured on a\npopular website. As its popularity grew, so did the number of users receiving an error. Upon viewing the Lambda function’s monitoring graph,\nthe developer discovered a lot of throttled invocation requests.\nWhat can the developer do to troubleshoot this issue? (Select THREE.)\nRequest a service quota increase\nUse exponential backoff in the application.\nDeploy the Lambda function in VPC\nConfigure reserved concurrency\nUse a compiled language like GoLang to improve the function's performance\nIncrease Lambda function timeout\nIncorrect\nAWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume.\nWith Lambda, you can run code for virtually any type of application or backend service — all with zero administration. Just upload your code, and\nLambda takes care of everything required to run and scale your code with high availability. You can set up your code to automatically trigger from\nother AWS services or call it directly from any web or mobile app."
      },
      "tags": {
        "services": [
          "Lambda",
          "EKS",
          "EBS",
          "VPC",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "high availability",
          "performance",
          "monitoring",
          "VPC"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 165247.png",
      "parsed": {
        "question": "A company is storing highly classified documents on its file server. These documents contain blueprints for electronic devices and are never to\nbe made public due to a legal agreement. To comply with the strict policy, you must explore the capabilities of AWS KMS to improve data\nsecurity.\nWhich of the following is the MOST suitable procedure for encrypting data?\nUse a symmetric key for encryption and decryption.\nGenerate a data key using a KMS key. Then, encrypt data with the ciphertext version of the data key.\n® Use a combination of symmetric and asymmetric encryption. Encrypt the data with a symmetric key and use the asymmetric\nprivate key to decrypt the data.\nGenerate a data key using a KMS key. Then, encrypt data with the plaintext data key.\nIncorrect\nYour data is protected when you encrypt it, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the\npractice of encrypting plaintext data with a data key and then encrypting the data key under another key.\nYou can even encrypt the data encryption key under another encryption key and encrypt that encryption key under another encryption key. But,\neventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key-encryption key is known as the\nmaster key.\n| on oN ~~ N",
        "options": [
          "A: company is storing highly classified documents on its file server. These documents contain blueprints for electronic devices and are never to",
          "b: e made public due to a legal agreement. To comply with the strict policy, you must explore the capabilities of AWS KMS to improve data",
          "c: urity.",
          "c: h of the following is the MOST suitable procedure for encrypting data?",
          "a: symmetric key for encryption and decryption.",
          "a: te a data key using a KMS key. Then, encrypt data with the ciphertext version of the data key.",
          "a: combination of symmetric and asymmetric encryption. Encrypt the data with a symmetric key and use the asymmetric",
          "a: te key to decrypt the data.",
          "a: te a data key using a KMS key. Then, encrypt data with the plaintext data key.",
          "c: orrect",
          "d: ata is protected when you encrypt it, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the",
          "a: ctice of encrypting plaintext data with a data key and then encrypting the data key under another key.",
          "c: an even encrypt the data encryption key under another encryption key and encrypt that encryption key under another encryption key. But,",
          "a: lly, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key-encryption key is known as the",
          "a: ster key."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "GY \\ 4\n®\n3. QUESTION\nA company is storing highly classified documents on its file server. These documents contain blueprints for electronic devices and are never to\nbe made public due to a legal agreement. To comply with the strict policy, you must explore the capabilities of AWS KMS to improve data\nsecurity.\nWhich of the following is the MOST suitable procedure for encrypting data?\nUse a symmetric key for encryption and decryption.\nGenerate a data key using a KMS key. Then, encrypt data with the ciphertext version of the data key.\n® Use a combination of symmetric and asymmetric encryption. Encrypt the data with a symmetric key and use the asymmetric\nprivate key to decrypt the data.\nGenerate a data key using a KMS key. Then, encrypt data with the plaintext data key.\nIncorrect\nYour data is protected when you encrypt it, but you have to protect your encryption key. One strategy is to encrypt it. Envelope encryption is the\npractice of encrypting plaintext data with a data key and then encrypting the data key under another key.\nYou can even encrypt the data encryption key under another encryption key and encrypt that encryption key under another encryption key. But,\neventually, one key must remain in plaintext so you can decrypt the keys and your data. This top-level plaintext key-encryption key is known as the\nmaster key.\n| on oN ~~ N"
      },
      "tags": {
        "services": [
          "KMS",
          "ECR"
        ],
        "domains": [
          "Security"
        ],
        "keywords": [
          "security",
          "encryption",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 165523.png",
      "parsed": {
        "question": "A full-stack developer has developed an application written in Node.js to host an upcoming mobile game tournament. The developer has\ndecided to deploy the application using AWS Elastic Beanstalk because of its ease-of-use. Upon experimenting, he learned that he could\nconfigure the webserver environment with several resources.\nWhich of the following services can the developer configure with Elastic Beanstalk? (Select THREE.)\nAWS Lambda\nAmazon EC2 Instance\nAmazon CloudFront\nAmazon CloudWatch\nAmazon Athena\nApplication Load Balancer\nIncorrect\nAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP,\nNode. js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\nYou can upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to\napplication health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the\nunderlying resources.\nErs— eB\nInternet",
        "options": [
          "A: full-stack developer has developed an application written in Node.js to host an upcoming mobile game tournament. The developer has",
          "d: ecided to deploy the application using AWS Elastic Beanstalk because of its ease-of-use. Upon experimenting, he learned that he could",
          "c: onfigure the webserver environment with several resources.",
          "c: h of the following services can the developer configure with Elastic Beanstalk? (Select THREE.)",
          "A: WS Lambda",
          "A: mazon EC2 Instance",
          "A: mazon CloudFront",
          "A: mazon CloudWatch",
          "A: mazon Athena",
          "A: pplication Load Balancer",
          "c: orrect",
          "A: WS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP,",
          "d: e. js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.",
          "c: an upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to",
          "a: pplication health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the",
          "d: erlying resources.",
          "B: Internet"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "9. QUESTION\nA full-stack developer has developed an application written in Node.js to host an upcoming mobile game tournament. The developer has\ndecided to deploy the application using AWS Elastic Beanstalk because of its ease-of-use. Upon experimenting, he learned that he could\nconfigure the webserver environment with several resources.\nWhich of the following services can the developer configure with Elastic Beanstalk? (Select THREE.)\nAWS Lambda\nAmazon EC2 Instance\nAmazon CloudFront\nAmazon CloudWatch\nAmazon Athena\nApplication Load Balancer\nIncorrect\nAWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP,\nNode. js, Python, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\nYou can upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, load balancing, auto-scaling to\napplication health monitoring. At the same time, you retain full control over the AWS resources powering your application and can access the\nunderlying resources.\nErs— eB\nInternet"
      },
      "tags": {
        "services": [
          "EC2",
          "Lambda",
          "Elastic Beanstalk",
          "EBS",
          "CloudFront",
          "CloudWatch",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "scaling",
          "monitoring",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 165810.png",
      "parsed": {
        "question": "A company processes data from loT (Internet of Things) devices using an AWS Lambda function. To ensure that the Lambda function performs A\nwithin the expected performance criteria, the business must closely monitor it to comply with its necessary service level agreement (SL",
        "options": [
          "A: company processes data from loT (Internet of Things) devices using an AWS Lambda function. To ensure that the Lambda function performs A",
          "c: ted performance criteria, the business must closely monitor it to comply with its necessary service level agreement (SLA). The N",
          "c: ompany wants to measure the application's throughput by tracking the total number of messages the Lambda function processes within a",
          "c: ified timeframe.",
          "c: h action should a developer take to meet the requirements?",
          "a: ble AWS Step Functions to orchestrate the message processing workflow of the Lambda function and monitor the",
          "c: ution times to infer throughput.",
          "d: ate the application to send custom Amazon CloudWatch metrics for each message processed by the Lambda function,",
          "a: nd use these metrics for throughput calculation.",
          "a: mbda function's ConcurrentExecutions metric in Amazon CloudWatch to assess throughput.",
          "d: ate the application to log throughput metrics to Amazon CloudWatch Logs and configure Amazon EventBridge to",
          "d: ically trigger a secondary Lambda function for processing these logs.",
          "c: orrect",
          "C: oncurrentExecutions metric in Amazon CloudWatch explicitly measures the number of instances of a Lambda function that are",
          "a: t the same time. This metric is crucial for understanding how many processes or tasks the Lambda function handles concurrently, which",
          "c: an serve as a proxy for throughput, especially when you want to ensure that your Lambda function is scaling appropriately to meet demands",
          "d: irectly measuring message processing times. While this metric does not directly measure the throughput in terms of messages",
          "c: essed per unit of time, it indirectly indicates the capacity and utilization of the Lambda function, offering insights into whether the function is",
          "d: service level agreement (SLA) by handling loads effectively.",
          "C: loudWatch x = CloudWatch > Metrics ®",
          "C: L — — 1 —"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "14. QUESTION\nA company processes data from loT (Internet of Things) devices using an AWS Lambda function. To ensure that the Lambda function performs A\nwithin the expected performance criteria, the business must closely monitor it to comply with its necessary service level agreement (SLA). The N\ncompany wants to measure the application's throughput by tracking the total number of messages the Lambda function processes within a\nspecified timeframe.\nWhich action should a developer take to meet the requirements?\nEnable AWS Step Functions to orchestrate the message processing workflow of the Lambda function and monitor the\nexecution times to infer throughput.\nUpdate the application to send custom Amazon CloudWatch metrics for each message processed by the Lambda function,\nand use these metrics for throughput calculation.\nUtilize the Lambda function's ConcurrentExecutions metric in Amazon CloudWatch to assess throughput.\n® Update the application to log throughput metrics to Amazon CloudWatch Logs and configure Amazon EventBridge to\nperiodically trigger a secondary Lambda function for processing these logs.\nIncorrect\nThe ConcurrentExecutions metric in Amazon CloudWatch explicitly measures the number of instances of a Lambda function that are\nrunning at the same time. This metric is crucial for understanding how many processes or tasks the Lambda function handles concurrently, which\ncan serve as a proxy for throughput, especially when you want to ensure that your Lambda function is scaling appropriately to meet demands\nwithout directly measuring message processing times. While this metric does not directly measure the throughput in terms of messages\nprocessed per unit of time, it indirectly indicates the capacity and utilization of the Lambda function, offering insights into whether the function is\nmeeting its required service level agreement (SLA) by handling loads effectively.\nCloudWatch x = CloudWatch > Metrics ®\nCL — — 1 —"
      },
      "tags": {
        "services": [
          "Lambda",
          "EventBridge",
          "Step Functions",
          "CloudWatch",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "scaling",
          "performance"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 165952.png",
      "parsed": {
        "question": "A web developer must deploy new features to a web application that can be turned on or off based on certain conditions. These new\nfunctionalities should not be accessible until fully developed and tested. The company wants to seamlessly control the visibility and access of\nthese new features without revealing them until it's officially released.\nWhich of the following solutions would fulfill these criteria?\nEmploy AWS Amplify DataStore to maintain prerelease feature data. Use AWS Amplify DataStore cloud synchronization\ncapabilities to modify feature visibility.\nUtilize AWS AppConfig to set the feature flag by creating a feature flag configuration profile. Enable and disable the feature\nflags according to development progress.\nUtilize Amazon DynamoDB to store information about upcoming features. Leverage Amazon DynamoDB Streams to switch\nfeatures’ status from hidden to visible upon updates.\nDevelop a feature flag feature using AWS Lambda functions. Store the feature flag values in AWS Systems Manager\n@® Parameter Store. Use the Lambda function to fetch and check the feature flag status to manage the availability of new\nfunctionalities.\nIncorrect\nAWS AppConfig is a specialized service that enables developers to create, manage, and deploy application configurations quickly and efficiently.\nIt also provides the functionality to manage feature flags, a powerful technique that allows developers to test and control new features in live\nenvironments without affecting all users. With AWS AppConfig, developers can gradually introduce new features, control who can access them\nbased on specific criteria such as user role or location, and easily update or revert features without deploying new code. This capability helps\ndevelopers seamlessly control the visibility and access of new features while keeping them hidden until they're ready for release.",
        "options": [
          "A: web developer must deploy new features to a web application that can be turned on or off based on certain conditions. These new",
          "c: tionalities should not be accessible until fully developed and tested. The company wants to seamlessly control the visibility and access of",
          "a: tures without revealing them until it's officially released.",
          "c: h of the following solutions would fulfill these criteria?",
          "A: WS Amplify DataStore to maintain prerelease feature data. Use AWS Amplify DataStore cloud synchronization",
          "c: apabilities to modify feature visibility.",
          "A: WS AppConfig to set the feature flag by creating a feature flag configuration profile. Enable and disable the feature",
          "a: gs according to development progress.",
          "A: mazon DynamoDB to store information about upcoming features. Leverage Amazon DynamoDB Streams to switch",
          "a: tures’ status from hidden to visible upon updates.",
          "D: evelop a feature flag feature using AWS Lambda functions. Store the feature flag values in AWS Systems Manager",
          "a: rameter Store. Use the Lambda function to fetch and check the feature flag status to manage the availability of new",
          "c: tionalities.",
          "c: orrect",
          "A: WS AppConfig is a specialized service that enables developers to create, manage, and deploy application configurations quickly and efficiently.",
          "a: lso provides the functionality to manage feature flags, a powerful technique that allows developers to test and control new features in live",
          "a: ffecting all users. With AWS AppConfig, developers can gradually introduce new features, control who can access them",
          "b: ased on specific criteria such as user role or location, and easily update or revert features without deploying new code. This capability helps",
          "d: evelopers seamlessly control the visibility and access of new features while keeping them hidden until they're ready for release."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "17. QUESTION\nA web developer must deploy new features to a web application that can be turned on or off based on certain conditions. These new\nfunctionalities should not be accessible until fully developed and tested. The company wants to seamlessly control the visibility and access of\nthese new features without revealing them until it's officially released.\nWhich of the following solutions would fulfill these criteria?\nEmploy AWS Amplify DataStore to maintain prerelease feature data. Use AWS Amplify DataStore cloud synchronization\ncapabilities to modify feature visibility.\nUtilize AWS AppConfig to set the feature flag by creating a feature flag configuration profile. Enable and disable the feature\nflags according to development progress.\nUtilize Amazon DynamoDB to store information about upcoming features. Leverage Amazon DynamoDB Streams to switch\nfeatures’ status from hidden to visible upon updates.\nDevelop a feature flag feature using AWS Lambda functions. Store the feature flag values in AWS Systems Manager\n@® Parameter Store. Use the Lambda function to fetch and check the feature flag status to manage the availability of new\nfunctionalities.\nIncorrect\nAWS AppConfig is a specialized service that enables developers to create, manage, and deploy application configurations quickly and efficiently.\nIt also provides the functionality to manage feature flags, a powerful technique that allows developers to test and control new features in live\nenvironments without affecting all users. With AWS AppConfig, developers can gradually introduce new features, control who can access them\nbased on specific criteria such as user role or location, and easily update or revert features without deploying new code. This capability helps\ndevelopers seamlessly control the visibility and access of new features while keeping them hidden until they're ready for release."
      },
      "tags": {
        "services": [
          "Lambda",
          "DynamoDB",
          "Systems Manager",
          "Parameter Store",
          "Config",
          "Systems Manager",
          "Amplify"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 170121.png",
      "parsed": {
        "question": "An engineer is developing a cloud-native application on AWS designed to handle a large volume of data. Within this application, a state\nmachine in AWS Step Functions calls upon multiple AWS Lambda functions for processing. Occasionally, one of these Lambda functions does\nnot complete its task within the allocated time, particularly during spikes in data volume, leading to timeout errors. The engineer must set up\nthe system so that any invocation of the Lambda function that fails due to these timeouts is automatically attempted again.\nWhich approach would ensure the system meets this requirement?\nEstablish a designated Fail state within the AWS Step Functions state machine architecture. Determine a retry limit for\nhandling execution failures.\n® Set the TimeoutSeconds value within the AWS Step Functions state machine setup. Assign a limit for retry attempts in\nresponse to operation timeouts.\nModify the AWS Step Functions state machine to forward the function call to an Amazon SNS topic and link the topic to the\nLambda function. Set a retry limit for the Lambda to handle timeout error scenarios.\nIntegrate a Retry field into the AWS Step Functions state machine's configuration. Specify the maximum attempts for retries\nand the target timeout error type as the retry trigger.\nIncorrect\nAWS Step Functions is a service that helps developers create and run workflows that integrate different AWS services, like AWS Lambda, into\napplications without worrying about servers. At its core, Step Functions uses Amazon States Language, a JSON-based language that defines state\nmachines. These state machines represent workflows as a series of steps, where each step corresponds to a state that can perform work (Task\nstates), make decisions (Choice states), wait for a certain time or event (Wait states), succeed or fail (Succeed/Fail states), and more. This\nprovides a robust framework for managing complex application logic, handling error scenarios, and ensuring application components interact\nseamlessly.",
        "options": [
          "A: n engineer is developing a cloud-native application on AWS designed to handle a large volume of data. Within this application, a state",
          "a: chine in AWS Step Functions calls upon multiple AWS Lambda functions for processing. Occasionally, one of these Lambda functions does",
          "c: omplete its task within the allocated time, particularly during spikes in data volume, leading to timeout errors. The engineer must set up",
          "a: t any invocation of the Lambda function that fails due to these timeouts is automatically attempted again.",
          "c: h approach would ensure the system meets this requirement?",
          "a: blish a designated Fail state within the AWS Step Functions state machine architecture. Determine a retry limit for",
          "a: ndling execution failures.",
          "c: onds value within the AWS Step Functions state machine setup. Assign a limit for retry attempts in",
          "a: tion timeouts.",
          "d: ify the AWS Step Functions state machine to forward the function call to an Amazon SNS topic and link the topic to the",
          "a: mbda function. Set a retry limit for the Lambda to handle timeout error scenarios.",
          "a: te a Retry field into the AWS Step Functions state machine's configuration. Specify the maximum attempts for retries",
          "a: nd the target timeout error type as the retry trigger.",
          "c: orrect",
          "A: WS Step Functions is a service that helps developers create and run workflows that integrate different AWS services, like AWS Lambda, into",
          "a: pplications without worrying about servers. At its core, Step Functions uses Amazon States Language, a JSON-based language that defines state",
          "a: chines. These state machines represent workflows as a series of steps, where each step corresponds to a state that can perform work (Task",
          "a: tes), make decisions (Choice states), wait for a certain time or event (Wait states), succeed or fail (Succeed/Fail states), and more. This",
          "d: es a robust framework for managing complex application logic, handling error scenarios, and ensuring application components interact",
          "a: mlessly."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "20. QUESTION\nAn engineer is developing a cloud-native application on AWS designed to handle a large volume of data. Within this application, a state\nmachine in AWS Step Functions calls upon multiple AWS Lambda functions for processing. Occasionally, one of these Lambda functions does\nnot complete its task within the allocated time, particularly during spikes in data volume, leading to timeout errors. The engineer must set up\nthe system so that any invocation of the Lambda function that fails due to these timeouts is automatically attempted again.\nWhich approach would ensure the system meets this requirement?\nEstablish a designated Fail state within the AWS Step Functions state machine architecture. Determine a retry limit for\nhandling execution failures.\n® Set the TimeoutSeconds value within the AWS Step Functions state machine setup. Assign a limit for retry attempts in\nresponse to operation timeouts.\nModify the AWS Step Functions state machine to forward the function call to an Amazon SNS topic and link the topic to the\nLambda function. Set a retry limit for the Lambda to handle timeout error scenarios.\nIntegrate a Retry field into the AWS Step Functions state machine's configuration. Specify the maximum attempts for retries\nand the target timeout error type as the retry trigger.\nIncorrect\nAWS Step Functions is a service that helps developers create and run workflows that integrate different AWS services, like AWS Lambda, into\napplications without worrying about servers. At its core, Step Functions uses Amazon States Language, a JSON-based language that defines state\nmachines. These state machines represent workflows as a series of steps, where each step corresponds to a state that can perform work (Task\nstates), make decisions (Choice states), wait for a certain time or event (Wait states), succeed or fail (Succeed/Fail states), and more. This\nprovides a robust framework for managing complex application logic, handling error scenarios, and ensuring application components interact\nseamlessly."
      },
      "tags": {
        "services": [
          "Lambda",
          "SNS",
          "Step Functions",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "topic"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-18 170219.png",
      "parsed": {
        "question": "A technology firm manages an internal portal that contains proprietary information. The firm intends to make this portal available to the general A\npublic. However, access must be restricted solely to employees authenticated through the firm’s OpenID Connect (OIDC) identity provider N\n(IdP). This authentication mechanism must be implemented without modifying the existing website code.\nWhich action will accomplish this requirement?\n® Set up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 80. Add a\ndefault authenticating operation that returns the OIDC IdP configuration.\nSet up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443.\nConfigure the rule's default action to authenticate users using the OIDC IdP configuration.\nSet up an internet-facing Network Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443.\nConfigure the rule's default action to authenticate users using the OIDC IdP configuratio\nSet up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443.\nConfigure the rule's default action to invoke an AWS Lambda function for OIDC authentication.\nIncorrect\nApplication Load Balancer (ALB) is created to handle both HTTP and HTTPS traffic at the application layer (Layer 7). It provides advanced routing\ncapabilities and supports features such as user authentication and SSL termination, which are essential for securing web applications exposed to\nthe internet. By configuring a public ALB, the company ensures that all incoming traffic to the internal website is managed through a central point\nthat can enforce security policies, including OIDC authentication. The ALB can be easily integrated with the company’s OIDC identity provider to\nauthenticate users, making it an ideal solution for this requirement.\nListener configuration\n— The listener will be identified by the protocol and port.",
        "options": [
          "A: technology firm manages an internal portal that contains proprietary information. The firm intends to make this portal available to the general A",
          "b: lic. However, access must be restricted solely to employees authenticated through the firm’s OpenID Connect (OIDC) identity provider N",
          "d: P). This authentication mechanism must be implemented without modifying the existing website code.",
          "c: h action will accomplish this requirement?",
          "a: n internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 80. Add a",
          "d: efault authenticating operation that returns the OIDC IdP configuration.",
          "a: n internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443.",
          "C: onfigure the rule's default action to authenticate users using the OIDC IdP configuration.",
          "a: n internet-facing Network Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443.",
          "C: onfigure the rule's default action to authenticate users using the OIDC IdP configuratio",
          "a: n internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443.",
          "C: onfigure the rule's default action to invoke an AWS Lambda function for OIDC authentication.",
          "c: orrect",
          "A: pplication Load Balancer (ALB) is created to handle both HTTP and HTTPS traffic at the application layer (Layer 7). It provides advanced routing",
          "c: apabilities and supports features such as user authentication and SSL termination, which are essential for securing web applications exposed to",
          "B: y configuring a public ALB, the company ensures that all incoming traffic to the internal website is managed through a central point",
          "a: t can enforce security policies, including OIDC authentication. The ALB can be easily integrated with the company’s OIDC identity provider to",
          "a: uthenticate users, making it an ideal solution for this requirement.",
          "c: onfiguration",
          "b: e identified by the protocol and port."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "21. QUESTION\nA technology firm manages an internal portal that contains proprietary information. The firm intends to make this portal available to the general A\npublic. However, access must be restricted solely to employees authenticated through the firm’s OpenID Connect (OIDC) identity provider N\n(IdP). This authentication mechanism must be implemented without modifying the existing website code.\nWhich action will accomplish this requirement?\n® Set up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 80. Add a\ndefault authenticating operation that returns the OIDC IdP configuration.\nSet up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443.\nConfigure the rule's default action to authenticate users using the OIDC IdP configuration.\nSet up an internet-facing Network Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443.\nConfigure the rule's default action to authenticate users using the OIDC IdP configuratio\nSet up an internet-facing Application Load Balancer. Create a listener rule for the load balancer for HTTPS on port 443.\nConfigure the rule's default action to invoke an AWS Lambda function for OIDC authentication.\nIncorrect\nApplication Load Balancer (ALB) is created to handle both HTTP and HTTPS traffic at the application layer (Layer 7). It provides advanced routing\ncapabilities and supports features such as user authentication and SSL termination, which are essential for securing web applications exposed to\nthe internet. By configuring a public ALB, the company ensures that all incoming traffic to the internal website is managed through a central point\nthat can enforce security policies, including OIDC authentication. The ALB can be easily integrated with the company’s OIDC identity provider to\nauthenticate users, making it an ideal solution for this requirement.\nListener configuration\n— The listener will be identified by the protocol and port."
      },
      "tags": {
        "services": [
          "Lambda",
          "EBS",
          "ALB",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "security",
          "authentication"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-19 120516.png",
      "parsed": {
        "question": "®\nA Docker application hosted on an ECS cluster has encountered intermittent unavailability issues and timeouts. The lead DevOps engineer ki\ninstructed you to instrument the application to detect where high latencies are occurring and to determine the specific services and paths\nimpacting application performance.\nWhich of the following steps should you take to accomplish this task properly? (Select TWO.)\nAdd the xray-daemon.config configuration file in your Docker image\nCreate a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your\nAmazon ECS cluster.\nConfigure the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000.\nManually install the X-Ray daemon to the instances via a user data script.\nConfigure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.\nIncorrect\nThe AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the\nSDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the\ndaemon alongside your application.\nTo properly instrument your applications in Amazon ECS, you have to create a Docker image that runs the X-Ray daemon, upload it to a Docker\nimage repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file\nto allow your application to communicate with the daemon container.\nRun the AWS X-Ray daemon\nA eo Be CI ee",
        "options": [
          "A: Docker application hosted on an ECS cluster has encountered intermittent unavailability issues and timeouts. The lead DevOps engineer ki",
          "c: ted you to instrument the application to detect where high latencies are occurring and to determine the specific services and paths",
          "a: cting application performance.",
          "c: h of the following steps should you take to accomplish this task properly? (Select TWO.)",
          "A: dd the xray-daemon.config configuration file in your Docker image",
          "C: reate a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your",
          "A: mazon ECS cluster.",
          "C: onfigure the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000.",
          "a: nually install the X-Ray daemon to the instances via a user data script.",
          "C: onfigure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.",
          "c: orrect",
          "A: WS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the",
          "D: K sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the",
          "d: aemon alongside your application.",
          "a: pplications in Amazon ECS, you have to create a Docker image that runs the X-Ray daemon, upload it to a Docker",
          "a: ge repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file",
          "a: llow your application to communicate with the daemon container.",
          "A: WS X-Ray daemon",
          "A: eo Be CI ee"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "2. QUESTION\n®\nA Docker application hosted on an ECS cluster has encountered intermittent unavailability issues and timeouts. The lead DevOps engineer ki\ninstructed you to instrument the application to detect where high latencies are occurring and to determine the specific services and paths\nimpacting application performance.\nWhich of the following steps should you take to accomplish this task properly? (Select TWO.)\nAdd the xray-daemon.config configuration file in your Docker image\nCreate a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your\nAmazon ECS cluster.\nConfigure the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000.\nManually install the X-Ray daemon to the instances via a user data script.\nConfigure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.\nIncorrect\nThe AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the\nSDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the\ndaemon alongside your application.\nTo properly instrument your applications in Amazon ECS, you have to create a Docker image that runs the X-Ray daemon, upload it to a Docker\nimage repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file\nto allow your application to communicate with the daemon container.\nRun the AWS X-Ray daemon\nA eo Be CI ee"
      },
      "tags": {
        "services": [
          "ECS",
          "X-Ray",
          "Config"
        ],
        "domains": [
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "container",
          "performance"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-19 120538.png",
      "parsed": {
        "question": "A serverless application is using API Gateway with a non-proxy Lambda Integration. A developer was tasked to expose a GET method on a new\n/getcourses resource to invoke the Lambda function, which will allow the consumers to fetch a list of online courses in JSON format. The\nconsumers must include a query string parameter named courseType in their request to get the data.\nWhat is the MOST efficient solution that the developer should do to accomplish this requirement?\nConfigure the method response of the resource.\n@® Configure the integration request of the resource.\nConfigure the method request of the resource.\nConfigure the integration response of the resource.\nIncorrect\nIn Lambda non-proxy (or custom) integration, you can specify how the incoming request data is mapped to the integration request and how the\nresulting integration response data is mapped to the method response.\nFor an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API",
        "options": [
          "a: lsdojo.com/instrumenting-your-application-with-aws-x-ray/",
          "A: serverless application is using API Gateway with a non-proxy Lambda Integration. A developer was tasked to expose a GET method on a new",
          "c: ourses resource to invoke the Lambda function, which will allow the consumers to fetch a list of online courses in JSON format. The",
          "c: onsumers must include a query string parameter named courseType in their request to get the data.",
          "a: t is the MOST efficient solution that the developer should do to accomplish this requirement?",
          "C: onfigure the method response of the resource.",
          "C: onfigure the integration request of the resource.",
          "C: onfigure the method request of the resource.",
          "C: onfigure the integration response of the resource.",
          "c: orrect",
          "a: mbda non-proxy (or custom) integration, you can specify how the incoming request data is mapped to the integration request and how the",
          "a: tion response data is mapped to the method response.",
          "a: n AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "https://tutorialsdojo.com/instrumenting-your-application-with-aws-x-ray/\n3. QUESTION\nA serverless application is using API Gateway with a non-proxy Lambda Integration. A developer was tasked to expose a GET method on a new\n/getcourses resource to invoke the Lambda function, which will allow the consumers to fetch a list of online courses in JSON format. The\nconsumers must include a query string parameter named courseType in their request to get the data.\nWhat is the MOST efficient solution that the developer should do to accomplish this requirement?\nConfigure the method response of the resource.\n@® Configure the integration request of the resource.\nConfigure the method request of the resource.\nConfigure the integration response of the resource.\nIncorrect\nIn Lambda non-proxy (or custom) integration, you can specify how the incoming request data is mapped to the integration request and how the\nresulting integration response data is mapped to the method response.\nFor an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API"
      },
      "tags": {
        "services": [
          "Lambda",
          "API Gateway",
          "X-Ray",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-19 180209.png",
      "parsed": {
        "question": "A company processes data from loT (Internet of Things) devices using an AWS Lambda function. To ensure that the Lambda function performs\nwithin the expected performance criteria, the business must closely monitor it to comply with its necessary service level agreement (SL",
        "options": [
          "A: company processes data from loT (Internet of Things) devices using an AWS Lambda function. To ensure that the Lambda function performs",
          "c: ted performance criteria, the business must closely monitor it to comply with its necessary service level agreement (SLA). The",
          "c: ompany wants to measure the application's throughput by tracking the total number of messages the Lambda function processes within a",
          "c: ified timeframe.",
          "c: h action should a developer take to meet the requirements?",
          "d: ate the application to log throughput metrics to Amazon CloudWatch Logs and configure Amazon EventBridge to",
          "d: ically trigger a secondary Lambda function for processing these logs.",
          "a: ble AWS Step Functions to orchestrate the message processing workflow of the Lambda function and monitor the",
          "c: ution times to infer throughput.",
          "a: mbda function's ConcurrentExecutions metric in Amazon CloudWatch to assess throughput.",
          "d: ate the application to send custom Amazon CloudWatch metrics for each message processed by the Lambda function,",
          "a: nd use these metrics for throughput calculation.",
          "c: orrect",
          "C: oncurrentExecutions metric in Amazon CloudWatch explicitly measures the number of instances of a Lambda function that are",
          "a: t the same time. This metric is crucial for understanding how many processes or tasks the Lambda function handles concurrently, which",
          "c: an serve as a proxy for throughput, especially when you want to ensure that your Lambda function is scaling appropriately to meet demands"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "4. QUESTION\nA company processes data from loT (Internet of Things) devices using an AWS Lambda function. To ensure that the Lambda function performs\nwithin the expected performance criteria, the business must closely monitor it to comply with its necessary service level agreement (SLA). The\ncompany wants to measure the application's throughput by tracking the total number of messages the Lambda function processes within a\nspecified timeframe.\nWhich action should a developer take to meet the requirements?\nUpdate the application to log throughput metrics to Amazon CloudWatch Logs and configure Amazon EventBridge to\nperiodically trigger a secondary Lambda function for processing these logs.\n® Enable AWS Step Functions to orchestrate the message processing workflow of the Lambda function and monitor the\nexecution times to infer throughput.\nUtilize the Lambda function's ConcurrentExecutions metric in Amazon CloudWatch to assess throughput.\nUpdate the application to send custom Amazon CloudWatch metrics for each message processed by the Lambda function,\nand use these metrics for throughput calculation.\nIncorrect\nThe ConcurrentExecutions metric in Amazon CloudWatch explicitly measures the number of instances of a Lambda function that are\nrunning at the same time. This metric is crucial for understanding how many processes or tasks the Lambda function handles concurrently, which\ncan serve as a proxy for throughput, especially when you want to ensure that your Lambda function is scaling appropriately to meet demands"
      },
      "tags": {
        "services": [
          "Lambda",
          "EventBridge",
          "Step Functions",
          "CloudWatch",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "scaling",
          "performance"
        ]
      },
      "isCorrect": true
    }
  ],
  "summary": {
    "totalQuestions": 66,
    "correctCount": 66,
    "incorrectCount": 0,
    "accuracy": 100,
    "serviceBreakdown": {
      "Lambda": {
        "total": 36,
        "correct": 36,
        "incorrect": 0,
        "accuracy": 100
      },
      "RDS": {
        "total": 8,
        "correct": 8,
        "incorrect": 0,
        "accuracy": 100
      },
      "VPC": {
        "total": 6,
        "correct": 6,
        "incorrect": 0,
        "accuracy": 100
      },
      "IAM": {
        "total": 10,
        "correct": 10,
        "incorrect": 0,
        "accuracy": 100
      },
      "Config": {
        "total": 30,
        "correct": 30,
        "incorrect": 0,
        "accuracy": 100
      },
      "DynamoDB": {
        "total": 16,
        "correct": 16,
        "incorrect": 0,
        "accuracy": 100
      },
      "Aurora": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "Cognito": {
        "total": 4,
        "correct": 4,
        "incorrect": 0,
        "accuracy": 100
      },
      "AppSync": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "Amplify": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "EC2": {
        "total": 13,
        "correct": 13,
        "incorrect": 0,
        "accuracy": 100
      },
      "Kinesis": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudWatch": {
        "total": 10,
        "correct": 10,
        "incorrect": 0,
        "accuracy": 100
      },
      "X-Ray": {
        "total": 4,
        "correct": 4,
        "incorrect": 0,
        "accuracy": 100
      },
      "SAM": {
        "total": 17,
        "correct": 17,
        "incorrect": 0,
        "accuracy": 100
      },
      "Elastic Beanstalk": {
        "total": 4,
        "correct": 4,
        "incorrect": 0,
        "accuracy": 100
      },
      "S3": {
        "total": 14,
        "correct": 14,
        "incorrect": 0,
        "accuracy": 100
      },
      "CodeDeploy": {
        "total": 3,
        "correct": 3,
        "incorrect": 0,
        "accuracy": 100
      },
      "CodePipeline": {
        "total": 3,
        "correct": 3,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudFormation": {
        "total": 5,
        "correct": 5,
        "incorrect": 0,
        "accuracy": 100
      },
      "SQS": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "SNS": {
        "total": 3,
        "correct": 3,
        "incorrect": 0,
        "accuracy": 100
      },
      "Step Functions": {
        "total": 9,
        "correct": 9,
        "incorrect": 0,
        "accuracy": 100
      },
      "CodeBuild": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "Inspector": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "ECR": {
        "total": 6,
        "correct": 6,
        "incorrect": 0,
        "accuracy": 100
      },
      "EBS": {
        "total": 8,
        "correct": 8,
        "incorrect": 0,
        "accuracy": 100
      },
      "API Gateway": {
        "total": 9,
        "correct": 9,
        "incorrect": 0,
        "accuracy": 100
      },
      "ECS": {
        "total": 4,
        "correct": 4,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudTrail": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "Redshift": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "EventBridge": {
        "total": 5,
        "correct": 5,
        "incorrect": 0,
        "accuracy": 100
      },
      "KMS": {
        "total": 4,
        "correct": 4,
        "incorrect": 0,
        "accuracy": 100
      },
      "EFS": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudFront": {
        "total": 3,
        "correct": 3,
        "incorrect": 0,
        "accuracy": 100
      },
      "EKS": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "Systems Manager": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "Parameter Store": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "ALB": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      }
    },
    "domainBreakdown": {
      "Development with AWS Services": {
        "total": 52,
        "correct": 52,
        "incorrect": 0,
        "accuracy": 100
      },
      "Security": {
        "total": 20,
        "correct": 20,
        "incorrect": 0,
        "accuracy": 100
      },
      "Troubleshooting and Optimization": {
        "total": 50,
        "correct": 50,
        "incorrect": 0,
        "accuracy": 100
      },
      "Deployment": {
        "total": 23,
        "correct": 23,
        "incorrect": 0,
        "accuracy": 100
      }
    },
    "weakAreas": [],
    "strongAreas": [
      {
        "service": "Lambda",
        "accuracy": 100,
        "questionsReviewed": 36
      },
      {
        "service": "RDS",
        "accuracy": 100,
        "questionsReviewed": 8
      },
      {
        "service": "VPC",
        "accuracy": 100,
        "questionsReviewed": 6
      },
      {
        "service": "IAM",
        "accuracy": 100,
        "questionsReviewed": 10
      },
      {
        "service": "Config",
        "accuracy": 100,
        "questionsReviewed": 30
      },
      {
        "service": "DynamoDB",
        "accuracy": 100,
        "questionsReviewed": 16
      },
      {
        "service": "Aurora",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "Cognito",
        "accuracy": 100,
        "questionsReviewed": 4
      },
      {
        "service": "Amplify",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "EC2",
        "accuracy": 100,
        "questionsReviewed": 13
      },
      {
        "service": "Kinesis",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "CloudWatch",
        "accuracy": 100,
        "questionsReviewed": 10
      },
      {
        "service": "X-Ray",
        "accuracy": 100,
        "questionsReviewed": 4
      },
      {
        "service": "SAM",
        "accuracy": 100,
        "questionsReviewed": 17
      },
      {
        "service": "Elastic Beanstalk",
        "accuracy": 100,
        "questionsReviewed": 4
      },
      {
        "service": "S3",
        "accuracy": 100,
        "questionsReviewed": 14
      },
      {
        "service": "CodeDeploy",
        "accuracy": 100,
        "questionsReviewed": 3
      },
      {
        "service": "CodePipeline",
        "accuracy": 100,
        "questionsReviewed": 3
      },
      {
        "service": "CloudFormation",
        "accuracy": 100,
        "questionsReviewed": 5
      },
      {
        "service": "SQS",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "SNS",
        "accuracy": 100,
        "questionsReviewed": 3
      },
      {
        "service": "Step Functions",
        "accuracy": 100,
        "questionsReviewed": 9
      },
      {
        "service": "CodeBuild",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "ECR",
        "accuracy": 100,
        "questionsReviewed": 6
      },
      {
        "service": "EBS",
        "accuracy": 100,
        "questionsReviewed": 8
      },
      {
        "service": "API Gateway",
        "accuracy": 100,
        "questionsReviewed": 9
      },
      {
        "service": "ECS",
        "accuracy": 100,
        "questionsReviewed": 4
      },
      {
        "service": "CloudTrail",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "EventBridge",
        "accuracy": 100,
        "questionsReviewed": 5
      },
      {
        "service": "KMS",
        "accuracy": 100,
        "questionsReviewed": 4
      },
      {
        "service": "CloudFront",
        "accuracy": 100,
        "questionsReviewed": 3
      },
      {
        "service": "Systems Manager",
        "accuracy": 100,
        "questionsReviewed": 2
      }
    ],
    "topKeywords": [
      {
        "keyword": "serverless",
        "count": 16
      },
      {
        "keyword": "security",
        "count": 15
      },
      {
        "keyword": "deployment",
        "count": 13
      },
      {
        "keyword": "performance",
        "count": 11
      },
      {
        "keyword": "policy",
        "count": 8
      },
      {
        "keyword": "VPC",
        "count": 6
      },
      {
        "keyword": "container",
        "count": 5
      },
      {
        "keyword": "authentication",
        "count": 5
      },
      {
        "keyword": "IAM role",
        "count": 5
      },
      {
        "keyword": "scaling",
        "count": 5
      },
      {
        "keyword": "synchronous",
        "count": 5
      },
      {
        "keyword": "authorization",
        "count": 4
      },
      {
        "keyword": "security group",
        "count": 3
      },
      {
        "keyword": "queue",
        "count": 3
      },
      {
        "keyword": "microservices",
        "count": 3
      }
    ],
    "recommendations": [
      "🎯 Excellent performance! You're exam-ready. Focus on scenario-based practice"
    ]
  },
  "heatmap": [
    {
      "service": "Lambda",
      "accuracy": 100,
      "total": 36,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "RDS",
      "accuracy": 100,
      "total": 8,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "VPC",
      "accuracy": 100,
      "total": 6,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "IAM",
      "accuracy": 100,
      "total": 10,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Config",
      "accuracy": 100,
      "total": 30,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "DynamoDB",
      "accuracy": 100,
      "total": 16,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Aurora",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Cognito",
      "accuracy": 100,
      "total": 4,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "AppSync",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Amplify",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "EC2",
      "accuracy": 100,
      "total": 13,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Kinesis",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudWatch",
      "accuracy": 100,
      "total": 10,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "X-Ray",
      "accuracy": 100,
      "total": 4,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "SAM",
      "accuracy": 100,
      "total": 17,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Elastic Beanstalk",
      "accuracy": 100,
      "total": 4,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "S3",
      "accuracy": 100,
      "total": 14,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CodeDeploy",
      "accuracy": 100,
      "total": 3,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CodePipeline",
      "accuracy": 100,
      "total": 3,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudFormation",
      "accuracy": 100,
      "total": 5,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "SQS",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "SNS",
      "accuracy": 100,
      "total": 3,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Step Functions",
      "accuracy": 100,
      "total": 9,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CodeBuild",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Inspector",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ECR",
      "accuracy": 100,
      "total": 6,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "EBS",
      "accuracy": 100,
      "total": 8,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "API Gateway",
      "accuracy": 100,
      "total": 9,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ECS",
      "accuracy": 100,
      "total": 4,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudTrail",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Redshift",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "EventBridge",
      "accuracy": 100,
      "total": 5,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "KMS",
      "accuracy": 100,
      "total": 4,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "EFS",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudFront",
      "accuracy": 100,
      "total": 3,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "EKS",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Systems Manager",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Parameter Store",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ALB",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    }
  ],
  "quickStats": {
    "accuracy": "100.0%",
    "correct": 66,
    "incorrect": 0,
    "totalServices": 39,
    "weakAreaCount": 0
  }
}