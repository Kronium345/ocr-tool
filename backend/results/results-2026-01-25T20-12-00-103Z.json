{
  "success": true,
  "processedCount": 44,
  "timestamp": "2026-01-25T20:12:00.103Z",
  "questions": [
    {
      "file": "Screenshot 2026-01-15 215653.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services i\nAn application performs various workflows and processes long-running tasks that take a long time to complete. Users are complaining that the\napplication is unresponsive since the workflow substantially increases the time it takes to complete a user request. The development team is\nlooking for a managed solution that can handle background tasks efficiently, scale automatically, and integrate seamlessly with the existing\napplication deployed on Elastic Beanstalk.\nWhich of the following is the BEST way to improve the performance of the application?\n@® Use a multicontainer docker environment in Elastic Beanstalk to process the long-running tasks asynchronously.\nSpawn a worker process locally in the EC2 instances and process the tasks asynchronously.\nUse an Amazon ECS Cluster with a Fargate launch type to process the tasks asynchronously.\nUse an Elastic Beanstalk worker environment to process the tasks asynchronously.\nIncorrect\nIf your application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker\nenvironment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your\napplication stays responsive under load.\nA long-running task is anything that substantially increases the time it takes to complete a request, such as processing images or videos, sending\nemails, or generating a ZIP archive. These operations can take only a second or two to complete, but a delay of a few seconds is a lot for a web",
        "options": [
          "C: ategory: CDA - Development with AWS Services i",
          "A: n application performs various workflows and processes long-running tasks that take a long time to complete. Users are complaining that the",
          "a: pplication is unresponsive since the workflow substantially increases the time it takes to complete a user request. The development team is",
          "a: managed solution that can handle background tasks efficiently, scale automatically, and integrate seamlessly with the existing",
          "a: pplication deployed on Elastic Beanstalk.",
          "c: h of the following is the BEST way to improve the performance of the application?",
          "a: multicontainer docker environment in Elastic Beanstalk to process the long-running tasks asynchronously.",
          "a: wn a worker process locally in the EC2 instances and process the tasks asynchronously.",
          "a: n Amazon ECS Cluster with a Fargate launch type to process the tasks asynchronously.",
          "a: n Elastic Beanstalk worker environment to process the tasks asynchronously.",
          "c: orrect",
          "a: pplication performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker",
          "D: ecoupling your web application front end from a process that performs blocking operations is a common way to ensure that your",
          "a: pplication stays responsive under load.",
          "A: long-running task is anything that substantially increases the time it takes to complete a request, such as processing images or videos, sending",
          "a: ils, or generating a ZIP archive. These operations can take only a second or two to complete, but a delay of a few seconds is a lot for a web"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "56. QUESTION\nCategory: CDA - Development with AWS Services i\nAn application performs various workflows and processes long-running tasks that take a long time to complete. Users are complaining that the\napplication is unresponsive since the workflow substantially increases the time it takes to complete a user request. The development team is\nlooking for a managed solution that can handle background tasks efficiently, scale automatically, and integrate seamlessly with the existing\napplication deployed on Elastic Beanstalk.\nWhich of the following is the BEST way to improve the performance of the application?\n@® Use a multicontainer docker environment in Elastic Beanstalk to process the long-running tasks asynchronously.\nSpawn a worker process locally in the EC2 instances and process the tasks asynchronously.\nUse an Amazon ECS Cluster with a Fargate launch type to process the tasks asynchronously.\nUse an Elastic Beanstalk worker environment to process the tasks asynchronously.\nIncorrect\nIf your application performs operations or workflows that take a long time to complete, you can offload those tasks to a dedicated worker\nenvironment. Decoupling your web application front end from a process that performs blocking operations is a common way to ensure that your\napplication stays responsive under load.\nA long-running task is anything that substantially increases the time it takes to complete a request, such as processing images or videos, sending\nemails, or generating a ZIP archive. These operations can take only a second or two to complete, but a delay of a few seconds is a lot for a web"
      },
      "tags": {
        "services": [
          "EC2",
          "Elastic Beanstalk",
          "ECS",
          "Fargate"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": [
          "container",
          "performance",
          "asynchronous",
          "synchronous"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-15 215725.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer is building a photo-sharing application that automatically enhances images uploaded by users to Amazon S3. When a user uploads E\nan image, its S3 path is sent to an image-processing application hosted on AWS Lambda. The Lambda function applies the selected filter to\nthe image and stores it back to S3.\nIf the upload is successful, the application will return a prompt telling the user that the request has been accepted. The entire processing\ntypically takes an average of 5 minutes to complete, which causes the application to become unresponsive.\nWhich of the following is the MOST suitable and cost-effective option which will prevent the application from being unresponsive?\nConfigure the application to asynchronously process the requests and change the invocation type of the Lambda function to\nEvent .\nUse a combination of Lambda and Step Functions to orchestrate service components and asynchronously process the\nrequests.\nUse AWS Serverless Application Model (AWS SAM) to allow asynchronous requests to your Lambda function.\n® Configure the application to asynchronously process the requests and use the default invocation type of the Lambda\nfunction.\nIncorrect\nAWS Lambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke\na Lambda function (referred to as on-demand invocation). The following examples illustrate on-demand invocations:\n— Your custom application invokes a Lambda function.",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer is building a photo-sharing application that automatically enhances images uploaded by users to Amazon S3. When a user uploads E",
          "a: n image, its S3 path is sent to an image-processing application hosted on AWS Lambda. The Lambda function applies the selected filter to",
          "a: ge and stores it back to S3.",
          "a: d is successful, the application will return a prompt telling the user that the request has been accepted. The entire processing",
          "c: ally takes an average of 5 minutes to complete, which causes the application to become unresponsive.",
          "c: h of the following is the MOST suitable and cost-effective option which will prevent the application from being unresponsive?",
          "C: onfigure the application to asynchronously process the requests and change the invocation type of the Lambda function to",
          "a: combination of Lambda and Step Functions to orchestrate service components and asynchronously process the",
          "A: WS Serverless Application Model (AWS SAM) to allow asynchronous requests to your Lambda function.",
          "C: onfigure the application to asynchronously process the requests and use the default invocation type of the Lambda",
          "c: tion.",
          "c: orrect",
          "A: WS Lambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke",
          "a: Lambda function (referred to as on-demand invocation). The following examples illustrate on-demand invocations:",
          "c: ustom application invokes a Lambda function."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "57. QUESTION\nCategory: CDA - Development with AWS Services\nA developer is building a photo-sharing application that automatically enhances images uploaded by users to Amazon S3. When a user uploads E\nan image, its S3 path is sent to an image-processing application hosted on AWS Lambda. The Lambda function applies the selected filter to\nthe image and stores it back to S3.\nIf the upload is successful, the application will return a prompt telling the user that the request has been accepted. The entire processing\ntypically takes an average of 5 minutes to complete, which causes the application to become unresponsive.\nWhich of the following is the MOST suitable and cost-effective option which will prevent the application from being unresponsive?\nConfigure the application to asynchronously process the requests and change the invocation type of the Lambda function to\nEvent .\nUse a combination of Lambda and Step Functions to orchestrate service components and asynchronously process the\nrequests.\nUse AWS Serverless Application Model (AWS SAM) to allow asynchronous requests to your Lambda function.\n® Configure the application to asynchronously process the requests and use the default invocation type of the Lambda\nfunction.\nIncorrect\nAWS Lambda supports synchronous and asynchronous invocation of a Lambda function. You can control the invocation type only when you invoke\na Lambda function (referred to as on-demand invocation). The following examples illustrate on-demand invocations:\n— Your custom application invokes a Lambda function."
      },
      "tags": {
        "services": [
          "Lambda",
          "S3",
          "Step Functions",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "asynchronous",
          "synchronous"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-15 215756.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nAn API gateway with a Lambda proxy integration takes a long time to complete its processing. There were also occurrences where some\nrequests timed out. You want to monitor the responsiveness of your API calls as well as the underlying Lambda function.\nWhich of the following CloudWatch metrics should you use to troubleshoot this issue? (Select TWO.)\nCacheMissCount\nCacheHitCount\nLatency\nIntegrationLatency\nIncorrect\nYou can monitor API execution using CloudWatch, which collects and processes raw data from API Gateway into readable, near-real-time metrics.\nThese statistics are recorded for a period of two weeks so that you can access historical information and gain a better perspective on how your\nweb application or service is performing. By default, API Gateway metric data is automatically sent to CloudWatch in one-minute periods.\nThe metrics reported by API Gateway provide information that you can analyze in different ways. The list below shows some common uses for the\nmetrics. These are suggestions to get you started, not a comprehensive list.",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: n API gateway with a Lambda proxy integration takes a long time to complete its processing. There were also occurrences where some",
          "d: out. You want to monitor the responsiveness of your API calls as well as the underlying Lambda function.",
          "c: h of the following CloudWatch metrics should you use to troubleshoot this issue? (Select TWO.)",
          "C: acheMissCount",
          "C: acheHitCount",
          "a: tency",
          "a: tionLatency",
          "c: orrect",
          "c: an monitor API execution using CloudWatch, which collects and processes raw data from API Gateway into readable, near-real-time metrics.",
          "a: tistics are recorded for a period of two weeks so that you can access historical information and gain a better perspective on how your",
          "b: application or service is performing. By default, API Gateway metric data is automatically sent to CloudWatch in one-minute periods.",
          "c: s reported by API Gateway provide information that you can analyze in different ways. The list below shows some common uses for the",
          "c: s. These are suggestions to get you started, not a comprehensive list."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "58. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nAn API gateway with a Lambda proxy integration takes a long time to complete its processing. There were also occurrences where some\nrequests timed out. You want to monitor the responsiveness of your API calls as well as the underlying Lambda function.\nWhich of the following CloudWatch metrics should you use to troubleshoot this issue? (Select TWO.)\nCacheMissCount\nCacheHitCount\nLatency\nIntegrationLatency\nIncorrect\nYou can monitor API execution using CloudWatch, which collects and processes raw data from API Gateway into readable, near-real-time metrics.\nThese statistics are recorded for a period of two weeks so that you can access historical information and gain a better perspective on how your\nweb application or service is performing. By default, API Gateway metric data is automatically sent to CloudWatch in one-minute periods.\nThe metrics reported by API Gateway provide information that you can analyze in different ways. The list below shows some common uses for the\nmetrics. These are suggestions to get you started, not a comprehensive list."
      },
      "tags": {
        "services": [
          "Lambda",
          "EKS",
          "API Gateway",
          "CloudWatch"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-15 215821.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nYou want to update a Lambda function on your production environment and ensure that when you publish the updated version, you still have a\nquick way to roll back to the older version in case you encountered a problem. To prevent any sudden user interruptions, you want to gradually\nincrease the traffic going to the new version.\nWhich of the following implementation is the BEST option to use?\n® Use ELB to route traffic to both Lambda functions.\nUse Traffic Shifting with Lambda Aliases.\nUse stage variables in your Lambda function.\nUse Route 53 weighted routing to two Lambda functions.\nIncorrect\nBy default, an alias points to a single Lambda function version. When the alias is updated to point to a different function version, incoming request\ntraffic in turn instantly points to the updated version. This exposes that alias to any potential instabilities introduced by the new version. To\nminimize this impact, you can implement the routing-config parameter of the Lambda alias that allows you to point to two different versions\nof the Lambda function and dictate what percentage of incoming traffic is sent to each version.",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "a: nt to update a Lambda function on your production environment and ensure that when you publish the updated version, you still have a",
          "c: k way to roll back to the older version in case you encountered a problem. To prevent any sudden user interruptions, you want to gradually",
          "c: rease the traffic going to the new version.",
          "c: h of the following implementation is the BEST option to use?",
          "B: to route traffic to both Lambda functions.",
          "a: ffic Shifting with Lambda Aliases.",
          "a: ge variables in your Lambda function.",
          "d: routing to two Lambda functions.",
          "c: orrect",
          "B: y default, an alias points to a single Lambda function version. When the alias is updated to point to a different function version, incoming request",
          "a: ffic in turn instantly points to the updated version. This exposes that alias to any potential instabilities introduced by the new version. To",
          "a: ct, you can implement the routing-config parameter of the Lambda alias that allows you to point to two different versions",
          "a: mbda function and dictate what percentage of incoming traffic is sent to each version."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "59. QUESTION\nCategory: CDA - Development with AWS Services\nYou want to update a Lambda function on your production environment and ensure that when you publish the updated version, you still have a\nquick way to roll back to the older version in case you encountered a problem. To prevent any sudden user interruptions, you want to gradually\nincrease the traffic going to the new version.\nWhich of the following implementation is the BEST option to use?\n® Use ELB to route traffic to both Lambda functions.\nUse Traffic Shifting with Lambda Aliases.\nUse stage variables in your Lambda function.\nUse Route 53 weighted routing to two Lambda functions.\nIncorrect\nBy default, an alias points to a single Lambda function version. When the alias is updated to point to a different function version, incoming request\ntraffic in turn instantly points to the updated version. This exposes that alias to any potential instabilities introduced by the new version. To\nminimize this impact, you can implement the routing-config parameter of the Lambda alias that allows you to point to two different versions\nof the Lambda function and dictate what percentage of incoming traffic is sent to each version."
      },
      "tags": {
        "services": [
          "Lambda",
          "Route 53",
          "ELB",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-15 215852.png",
      "parsed": {
        "question": "E\nCategory: CDA - Development with AWS Services\nYour serverless AWS Lambda functions are integrated with Amazon API gateway using Lambda proxy integration. The API caching feature is\nenabled in the API Gateway with a TTL value of 300 seconds. A client would like to fetch the latest data from your endpoints every time a\nrequest is sent and invalidate the existing cache.\nWhat should the client do in order to get the latest data?\nHave the client send a request with the cached: false header.\n® Modify cache TTL value to a shorter period.\nOverride API caching by allowing the client to send requests to the endpoint directly.\nHave the client send a request with the cache-control: max-age=0 header.\nIncorrect\nA client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a\nrequest that contains the Cache-Control: max-age=0 header.\ntest Stage Editor Delete Stage\n©® Invoke URL: hitps://&liiil ib execute-api.us-east-1.amazonaws com/test",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: WS Lambda functions are integrated with Amazon API gateway using Lambda proxy integration. The API caching feature is",
          "a: bled in the API Gateway with a TTL value of 300 seconds. A client would like to fetch the latest data from your endpoints every time a",
          "a: nd invalidate the existing cache.",
          "a: t should the client do in order to get the latest data?",
          "a: ve the client send a request with the cached: false header.",
          "d: ify cache TTL value to a shorter period.",
          "d: e API caching by allowing the client to send requests to the endpoint directly.",
          "a: ve the client send a request with the cache-control: max-age=0 header.",
          "c: orrect",
          "A: client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a",
          "a: t contains the Cache-Control: max-age=0 header.",
          "a: ge Editor Delete Stage",
          "b: execute-api.us-east-1.amazonaws com/test"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "60. QUESTION E\nCategory: CDA - Development with AWS Services\nYour serverless AWS Lambda functions are integrated with Amazon API gateway using Lambda proxy integration. The API caching feature is\nenabled in the API Gateway with a TTL value of 300 seconds. A client would like to fetch the latest data from your endpoints every time a\nrequest is sent and invalidate the existing cache.\nWhat should the client do in order to get the latest data?\nHave the client send a request with the cached: false header.\n® Modify cache TTL value to a shorter period.\nOverride API caching by allowing the client to send requests to the endpoint directly.\nHave the client send a request with the cache-control: max-age=0 header.\nIncorrect\nA client of your API can invalidate an existing cache entry and reload it from the integration endpoint for individual requests. The client must send a\nrequest that contains the Cache-Control: max-age=0 header.\ntest Stage Editor Delete Stage\n©® Invoke URL: hitps://&liiil ib execute-api.us-east-1.amazonaws com/test"
      },
      "tags": {
        "services": [
          "Lambda",
          "API Gateway"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "caching"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-15 220021.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA recently deployed Lambda function has an intermittent issue in processing customer data. You enabled the active tracing option in order to\ndetect, analyze, and optimize performance issues of your function using the X-Ray service.\nWhich of the following environment variables are used by AWS Lambda to facilitate communication with X-Ray? (Select TWO.)\nAWS_XRAY CONTEXT MISSING\nAUTO_INSTRUMENT\nAWS_XRAY TRACING NAME\n_X_AMZN_TRACE_ID\nIncorrect\nAWS X-Ray is an AWS service that allows you to detect, analyze, and optimize performance issues with your AWS Lambda applications. X-Ray\ncollects metadata from the Lambda service and any upstream or downstream services that make up your application. X-Ray uses this metadata to\ngenerate a detailed service graph that illustrates performance bottlenecks, latency spikes, and other issues that impact the performance of your\nLambda application.\navg. 629ms\n1 tmin\navg. 570ms",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: recently deployed Lambda function has an intermittent issue in processing customer data. You enabled the active tracing option in order to",
          "d: etect, analyze, and optimize performance issues of your function using the X-Ray service.",
          "c: h of the following environment variables are used by AWS Lambda to facilitate communication with X-Ray? (Select TWO.)",
          "A: WS_XRAY CONTEXT MISSING",
          "A: UTO_INSTRUMENT",
          "A: WS_XRAY TRACING NAME",
          "A: MZN_TRACE_ID",
          "c: orrect",
          "A: WS X-Ray is an AWS service that allows you to detect, analyze, and optimize performance issues with your AWS Lambda applications. X-Ray",
          "c: ollects metadata from the Lambda service and any upstream or downstream services that make up your application. X-Ray uses this metadata to",
          "a: te a detailed service graph that illustrates performance bottlenecks, latency spikes, and other issues that impact the performance of your",
          "a: mbda application.",
          "a: vg. 629ms",
          "a: vg. 570ms"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "63. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA recently deployed Lambda function has an intermittent issue in processing customer data. You enabled the active tracing option in order to\ndetect, analyze, and optimize performance issues of your function using the X-Ray service.\nWhich of the following environment variables are used by AWS Lambda to facilitate communication with X-Ray? (Select TWO.)\nAWS_XRAY CONTEXT MISSING\nAUTO_INSTRUMENT\nAWS_XRAY TRACING NAME\n_X_AMZN_TRACE_ID\nIncorrect\nAWS X-Ray is an AWS service that allows you to detect, analyze, and optimize performance issues with your AWS Lambda applications. X-Ray\ncollects metadata from the Lambda service and any upstream or downstream services that make up your application. X-Ray uses this metadata to\ngenerate a detailed service graph that illustrates performance bottlenecks, latency spikes, and other issues that impact the performance of your\nLambda application.\navg. 629ms\n1 tmin\navg. 570ms"
      },
      "tags": {
        "services": [
          "Lambda",
          "X-Ray"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "performance"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172136.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nA serverless application, which is composed of multiple Lambda functions, has been deployed using AWS SAM. A developer was instructed to\neasily manage the deployments of the functions using CodeDeploy. When there is a new deployment, 10 percent of the incoming traffic should\nbe shifted to the new version every 10 minutes until all traffic is shifted from the old version.\nWhat should the developer do to properly deploy the functions that satisfies this requirement?\nDeploy the functions using an All-at-once deployment configuration.\nDeploy the functions using an Immutable deployment configuration.\n® Deploy the functions using a Canary deployment configuration.\nDeploy the functions using a Linear deployment configuration.\nIncorrect\nCodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless\nLambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets,\nGitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to\nyour existing code before you can use CodeDeploy.\nCodeDeploy supports the following deployment configurations:",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: serverless application, which is composed of multiple Lambda functions, has been deployed using AWS SAM. A developer was instructed to",
          "a: sily manage the deployments of the functions using CodeDeploy. When there is a new deployment, 10 percent of the incoming traffic should",
          "b: e shifted to the new version every 10 minutes until all traffic is shifted from the old version.",
          "a: t should the developer do to properly deploy the functions that satisfies this requirement?",
          "D: eploy the functions using an All-at-once deployment configuration.",
          "D: eploy the functions using an Immutable deployment configuration.",
          "D: eploy the functions using a Canary deployment configuration.",
          "D: eploy the functions using a Linear deployment configuration.",
          "c: orrect",
          "C: odeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless",
          "a: mbda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets,",
          "b: repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to",
          "c: ode before you can use CodeDeploy.",
          "C: odeDeploy supports the following deployment configurations:"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "1. QUESTION\nCategory: CDA - Deployment\nA serverless application, which is composed of multiple Lambda functions, has been deployed using AWS SAM. A developer was instructed to\neasily manage the deployments of the functions using CodeDeploy. When there is a new deployment, 10 percent of the incoming traffic should\nbe shifted to the new version every 10 minutes until all traffic is shifted from the old version.\nWhat should the developer do to properly deploy the functions that satisfies this requirement?\nDeploy the functions using an All-at-once deployment configuration.\nDeploy the functions using an Immutable deployment configuration.\n® Deploy the functions using a Canary deployment configuration.\nDeploy the functions using a Linear deployment configuration.\nIncorrect\nCodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless\nLambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets,\nGitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to\nyour existing code before you can use CodeDeploy.\nCodeDeploy supports the following deployment configurations:"
      },
      "tags": {
        "services": [
          "EC2",
          "Lambda",
          "ECS",
          "S3",
          "CodeDeploy",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "deployment",
          "canary"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172227.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA company has 5 different applications running on several On-Demand EC2 instances. The DevOps team is required to set up a graphical\nrepresentation of the key performance metrics for each application. These system metrics must be available on a single shared screen for\nmore effective and visible monitoring.\nWhich of the following should the DevOps team do to satisfy this requirement using Amazon CloudWatch?\nSet up a custom CloudWatch namespace with a unique metric name for each application.\nSet up a custom CloudWatch dimension with a unique metric name for each application.\n® Set up a custom CloudWatch Alarm with a unique metric name for each application.\nSet up a custom CloudWatch Event with a unique metric name for each application.\nIncorrect\nAmazon CloudWatch is basically a metrics repository. An AWS service—such as Amazon EC2—puts metrics into the repository, and you retrieve\nstatistics based on those metrics. If you put your own custom metrics into the repository, you can retrieve statistics on these metrics as well.\nA namespace is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other so that metrics from different\napplications are not mistakenly aggregated into the same statistics.\nH Amazon [] 1 Actions\ni Cloudwatch H",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: company has 5 different applications running on several On-Demand EC2 instances. The DevOps team is required to set up a graphical",
          "a: tion of the key performance metrics for each application. These system metrics must be available on a single shared screen for",
          "c: tive and visible monitoring.",
          "c: h of the following should the DevOps team do to satisfy this requirement using Amazon CloudWatch?",
          "a: custom CloudWatch namespace with a unique metric name for each application.",
          "a: custom CloudWatch dimension with a unique metric name for each application.",
          "a: custom CloudWatch Alarm with a unique metric name for each application.",
          "a: custom CloudWatch Event with a unique metric name for each application.",
          "c: orrect",
          "A: mazon CloudWatch is basically a metrics repository. An AWS service—such as Amazon EC2—puts metrics into the repository, and you retrieve",
          "a: tistics based on those metrics. If you put your own custom metrics into the repository, you can retrieve statistics on these metrics as well.",
          "A: namespace is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other so that metrics from different",
          "a: pplications are not mistakenly aggregated into the same statistics.",
          "A: mazon [] 1 Actions",
          "C: loudwatch H"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "3. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA company has 5 different applications running on several On-Demand EC2 instances. The DevOps team is required to set up a graphical\nrepresentation of the key performance metrics for each application. These system metrics must be available on a single shared screen for\nmore effective and visible monitoring.\nWhich of the following should the DevOps team do to satisfy this requirement using Amazon CloudWatch?\nSet up a custom CloudWatch namespace with a unique metric name for each application.\nSet up a custom CloudWatch dimension with a unique metric name for each application.\n® Set up a custom CloudWatch Alarm with a unique metric name for each application.\nSet up a custom CloudWatch Event with a unique metric name for each application.\nIncorrect\nAmazon CloudWatch is basically a metrics repository. An AWS service—such as Amazon EC2—puts metrics into the repository, and you retrieve\nstatistics based on those metrics. If you put your own custom metrics into the repository, you can retrieve statistics on these metrics as well.\nA namespace is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other so that metrics from different\napplications are not mistakenly aggregated into the same statistics.\nH Amazon [] 1 Actions\ni Cloudwatch H"
      },
      "tags": {
        "services": [
          "EC2",
          "CloudWatch",
          "SAM"
        ],
        "domains": [
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "container",
          "performance",
          "monitoring"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172304.png",
      "parsed": {
        "question": "L\nCategory: CDA - Development with AWS Services\nA company is re-architecting its legacy application to use AWS Lambda and DynamoDB. The table is provisioned to have 10 read capacity\nunits, and each item has a size of 4 KB.\nHow many eventual and strong consistent read requests can the table handle per second?\n@® 10 strongly consistent reads and 20 eventually consistent reads per second\n\n10 strongly consistent reads and 10 eventually consistent reads per second\n\n5 strongly consistent reads and 20 eventually consistent reads per second\n\n20 strongly consistent reads and 10 eventually consistent reads per second\n\nCorrect",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: company is re-architecting its legacy application to use AWS Lambda and DynamoDB. The table is provisioned to have 10 read capacity",
          "a: nd each item has a size of 4 KB.",
          "a: ny eventual and strong consistent read requests can the table handle per second?",
          "c: onsistent reads and 20 eventually consistent reads per second",
          "c: onsistent reads and 10 eventually consistent reads per second",
          "c: onsistent reads and 20 eventually consistent reads per second",
          "c: onsistent reads and 10 eventually consistent reads per second",
          "C: orrect"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "4. QUESTION L\nCategory: CDA - Development with AWS Services\nA company is re-architecting its legacy application to use AWS Lambda and DynamoDB. The table is provisioned to have 10 read capacity\nunits, and each item has a size of 4 KB.\nHow many eventual and strong consistent read requests can the table handle per second?\n@® 10 strongly consistent reads and 20 eventually consistent reads per second\n\n10 strongly consistent reads and 10 eventually consistent reads per second\n\n5 strongly consistent reads and 20 eventually consistent reads per second\n\n20 strongly consistent reads and 10 eventually consistent reads per second\n\nCorrect"
      },
      "tags": {
        "services": [
          "Lambda",
          "DynamoDB"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172340.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer is designing the cloud architecture of an internal application which will be used by about a hundred employees. She needs to i\nensure that the architecture is elastic enough to adequately match the supply of resources to the demand while maintaining its cost-\neffectiveness.\nWhich of the following services can provide the MOST elasticity to the architecture? (Select TWO.)\nAmazon EC2 Spot Fleet\nAmazon CloudFront\nAWS WAF\nAmazon DynamoDB\nAmazon RDS\nIncorrect\nIn the traditional data center-based model of IT, once the infrastructure is deployed, it typically runs whether it is needed or not, and all the\ncapacity is paid for, regardless of how much it gets used. In the cloud, resources are elastic, meaning they can instantly grow or shrink to match\nthe requirements of a specific application.\nElasticity allows you to match the supply of resources—which cost money—to demand. Because cloud resources are paid for based on usage,\nmatching needs to utilization is critical for cost optimization. Demand includes both external usage, such as the number of customers who visit a\nwebsite over a given period, and internal usage, such as an application team using development and test environments.\nThere are two basic types of elasticity:\nn",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer is designing the cloud architecture of an internal application which will be used by about a hundred employees. She needs to i",
          "a: t the architecture is elastic enough to adequately match the supply of resources to the demand while maintaining its cost-",
          "c: tiveness.",
          "c: h of the following services can provide the MOST elasticity to the architecture? (Select TWO.)",
          "A: mazon EC2 Spot Fleet",
          "A: mazon CloudFront",
          "A: WS WAF",
          "A: mazon DynamoDB",
          "A: mazon RDS",
          "c: orrect",
          "a: ditional data center-based model of IT, once the infrastructure is deployed, it typically runs whether it is needed or not, and all the",
          "c: apacity is paid for, regardless of how much it gets used. In the cloud, resources are elastic, meaning they can instantly grow or shrink to match",
          "a: specific application.",
          "a: sticity allows you to match the supply of resources—which cost money—to demand. Because cloud resources are paid for based on usage,",
          "a: tching needs to utilization is critical for cost optimization. Demand includes both external usage, such as the number of customers who visit a",
          "b: site over a given period, and internal usage, such as an application team using development and test environments.",
          "a: re two basic types of elasticity:"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "5. QUESTION\nCategory: CDA - Development with AWS Services\nA developer is designing the cloud architecture of an internal application which will be used by about a hundred employees. She needs to i\nensure that the architecture is elastic enough to adequately match the supply of resources to the demand while maintaining its cost-\neffectiveness.\nWhich of the following services can provide the MOST elasticity to the architecture? (Select TWO.)\nAmazon EC2 Spot Fleet\nAmazon CloudFront\nAWS WAF\nAmazon DynamoDB\nAmazon RDS\nIncorrect\nIn the traditional data center-based model of IT, once the infrastructure is deployed, it typically runs whether it is needed or not, and all the\ncapacity is paid for, regardless of how much it gets used. In the cloud, resources are elastic, meaning they can instantly grow or shrink to match\nthe requirements of a specific application.\nElasticity allows you to match the supply of resources—which cost money—to demand. Because cloud resources are paid for based on usage,\nmatching needs to utilization is critical for cost optimization. Demand includes both external usage, such as the number of customers who visit a\nwebsite over a given period, and internal usage, such as an application team using development and test environments.\nThere are two basic types of elasticity:\nn"
      },
      "tags": {
        "services": [
          "EC2",
          "EBS",
          "RDS",
          "DynamoDB",
          "CloudFront",
          "WAF"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "cost optimization"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172529.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nAn application is hosted in Elastic Beanstalk, which is currently running in Java 7 runtime environment. A new version of the application is\nready to be deployed, and the developer was tasked to upgrade the platform to Java 8 to accommodate the changes. All user traffic must be\nimmediately directed to the new version. If problems arise, the developer should be able to quickly revert to the previous version.\nWhich of the following is the MOST appropriate action that the developer should do to upgrade the platform?\nPerform a Blue/Green Deployment.\n® Perform a Traffic splitting deployment.\nManually upgrade the Java runtime environment of the EC2 instances in the Elastic Beanstalk environment.\nUpdate the environment's platform version to Java 8.\nIncorrect\nElastic Beanstalk regularly releases new platform versions to update all Linux-based and Windows Server-based platforms. New platform versions\nprovide updates to existing software components and support for new features and configuration options.\nYou can use the Elastic Beanstalk console or the EB CLI to update your environment’s platform version. Depending on the platform version you'd\nlike to update to, Elastic Beanstalk recommends one of two methods for performing platform updates.\nMethod 1 - Update your Environment's Platform Version — This is the recommended method when you're updating to the latest platform\nversion, without a change in runtime, web server, or application server versions, and without a change in the major platform version. This is the\nmost common and routine platform update.",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: n application is hosted in Elastic Beanstalk, which is currently running in Java 7 runtime environment. A new version of the application is",
          "a: dy to be deployed, and the developer was tasked to upgrade the platform to Java 8 to accommodate the changes. All user traffic must be",
          "d: iately directed to the new version. If problems arise, the developer should be able to quickly revert to the previous version.",
          "c: h of the following is the MOST appropriate action that the developer should do to upgrade the platform?",
          "a: Blue/Green Deployment.",
          "a: Traffic splitting deployment.",
          "a: nually upgrade the Java runtime environment of the EC2 instances in the Elastic Beanstalk environment.",
          "d: ate the environment's platform version to Java 8.",
          "c: orrect",
          "a: stic Beanstalk regularly releases new platform versions to update all Linux-based and Windows Server-based platforms. New platform versions",
          "d: e updates to existing software components and support for new features and configuration options.",
          "c: an use the Elastic Beanstalk console or the EB CLI to update your environment’s platform version. Depending on the platform version you'd",
          "d: ate to, Elastic Beanstalk recommends one of two methods for performing platform updates.",
          "d: 1 - Update your Environment's Platform Version — This is the recommended method when you're updating to the latest platform",
          "a: change in runtime, web server, or application server versions, and without a change in the major platform version. This is the",
          "c: ommon and routine platform update."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "10. QUESTION\nCategory: CDA - Deployment\nAn application is hosted in Elastic Beanstalk, which is currently running in Java 7 runtime environment. A new version of the application is\nready to be deployed, and the developer was tasked to upgrade the platform to Java 8 to accommodate the changes. All user traffic must be\nimmediately directed to the new version. If problems arise, the developer should be able to quickly revert to the previous version.\nWhich of the following is the MOST appropriate action that the developer should do to upgrade the platform?\nPerform a Blue/Green Deployment.\n® Perform a Traffic splitting deployment.\nManually upgrade the Java runtime environment of the EC2 instances in the Elastic Beanstalk environment.\nUpdate the environment's platform version to Java 8.\nIncorrect\nElastic Beanstalk regularly releases new platform versions to update all Linux-based and Windows Server-based platforms. New platform versions\nprovide updates to existing software components and support for new features and configuration options.\nYou can use the Elastic Beanstalk console or the EB CLI to update your environment’s platform version. Depending on the platform version you'd\nlike to update to, Elastic Beanstalk recommends one of two methods for performing platform updates.\nMethod 1 - Update your Environment's Platform Version — This is the recommended method when you're updating to the latest platform\nversion, without a change in runtime, web server, or application server versions, and without a change in the major platform version. This is the\nmost common and routine platform update."
      },
      "tags": {
        "services": [
          "EC2",
          "Elastic Beanstalk",
          "Config"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": [
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172649.png",
      "parsed": {
        "question": "il\nCategory: CDA - Security\nA developer is building an application that will be hosted in ECS and must be configured to run tasks and services using the Fargate launch\ntype. The application will have four different tasks, each of which will access different AWS resources than the others.\nWhich of the following is the MOST efficient solution that can provide your application in ECS access to the required AWS resources?\nCreate 4 different IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.\n® Create an IAM Group with all the required permissions and attach them to each of the 4 ECS tasks.\nCreate 4 different Container Instance IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.\nCreate 4 different Service-Linked Roles with the required permissions and attach them to each of the 4 ECS tasks.\nIncorrect\nBy default, IAM users don’t have permission to create or modify Amazon ECS resources or perform tasks using the Amazon ECS API. This means\nthat they also can’t do so using the Amazon ECS console or the AWS CLI. To allow IAM users to create or modify resources and perform tasks, you\nmust create IAM policies. Policies grant IAM users permission to use specific resources and API actions. Then, attach those policies to the IAM\nusers or groups that require those permissions.",
        "options": [
          "C: ategory: CDA - Security",
          "A: developer is building an application that will be hosted in ECS and must be configured to run tasks and services using the Fargate launch",
          "a: pplication will have four different tasks, each of which will access different AWS resources than the others.",
          "c: h of the following is the MOST efficient solution that can provide your application in ECS access to the required AWS resources?",
          "C: reate 4 different IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.",
          "C: reate an IAM Group with all the required permissions and attach them to each of the 4 ECS tasks.",
          "C: reate 4 different Container Instance IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.",
          "C: reate 4 different Service-Linked Roles with the required permissions and attach them to each of the 4 ECS tasks.",
          "c: orrect",
          "B: y default, IAM users don’t have permission to create or modify Amazon ECS resources or perform tasks using the Amazon ECS API. This means",
          "a: t they also can’t do so using the Amazon ECS console or the AWS CLI. To allow IAM users to create or modify resources and perform tasks, you",
          "c: reate IAM policies. Policies grant IAM users permission to use specific resources and API actions. Then, attach those policies to the IAM",
          "a: t require those permissions."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "13. QUESTION il\nCategory: CDA - Security\nA developer is building an application that will be hosted in ECS and must be configured to run tasks and services using the Fargate launch\ntype. The application will have four different tasks, each of which will access different AWS resources than the others.\nWhich of the following is the MOST efficient solution that can provide your application in ECS access to the required AWS resources?\nCreate 4 different IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.\n® Create an IAM Group with all the required permissions and attach them to each of the 4 ECS tasks.\nCreate 4 different Container Instance IAM Roles with the required permissions and attach them to each of the 4 ECS tasks.\nCreate 4 different Service-Linked Roles with the required permissions and attach them to each of the 4 ECS tasks.\nIncorrect\nBy default, IAM users don’t have permission to create or modify Amazon ECS resources or perform tasks using the Amazon ECS API. This means\nthat they also can’t do so using the Amazon ECS console or the AWS CLI. To allow IAM users to create or modify resources and perform tasks, you\nmust create IAM policies. Policies grant IAM users permission to use specific resources and API actions. Then, attach those policies to the IAM\nusers or groups that require those permissions."
      },
      "tags": {
        "services": [
          "ECS",
          "Fargate",
          "IAM",
          "Config"
        ],
        "domains": [
          "Security"
        ],
        "keywords": [
          "container",
          "security",
          "IAM role"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172704.png",
      "parsed": {
        "question": "uired permissions and attach them to each of the\n4 ECS tasks.\nThe option that says: Creating an IAM Group with all the required permissions and attaching them to each of the 4 ECS tasks is incorrect\nbecause you cannot directly attach an IAM Group to an ECS Task. Attaching an IAM Role is a more suitable solution in this scenario and not an IAM\nGroup.\nThe option that says: Creating 4 different Container Instance IAM Roles with the required permissions and attaching them to each of the 4\nECS tasks is incorrect because a Container Instance IAM Role only applies if you are using the EC2 launch type. Take note that the scenario says\nthat the application will be using a Fargate launch type.\nThe option that says: Creating 4 different Service-Linked Roles with the required permissions and attaching them to each of the 4 ECS\ntasks is incorrect because a service-linked role is a unique type of IAM role that is linked directly to Amazon ECS itself, not on the ECS task.",
        "options": [
          "a: stic network elastic network elastic network |",
          "a: ce interface interface <",
          "a: ltsiiuiuisatn",
          "C: l",
          "a: sk definition task task |",
          "d: Service I | |!",
          "C: C® 66 ®",
          "a: stic network elastic network elastic network: ©",
          "a: ce interface interface | |",
          "A: mazon ECS duster Cl",
          "A: Z2 2)",
          "a: ttach a policy to a user or group of users, it allows or denies the users permission to perform the specified tasks on the specified",
          "c: es. Likewise, Amazon ECS container instances make calls to the Amazon ECS and Amazon EC2 APIs on your behalf, so they need to",
          "a: uthenticate with your credentials. This authentication is accomplished by creating an IAM role for your container instances and associating that",
          "c: ontainer instances when you launch them.",
          "a: n Elastic Load Balancing load balancer with your Amazon ECS services, calls to the Amazon EC2 and Elastic Load Balancing APIs are",
          "a: de on your behalf to register and deregister container instances with your load balancers.",
          "c: e, the most suitable solution in this scenario is: Create 4 different IAM Roles with the required permissions and attach them to each of the",
          "C: S tasks.",
          "a: t says: Creating an IAM Group with all the required permissions and attaching them to each of the 4 ECS tasks is incorrect",
          "b: ecause you cannot directly attach an IAM Group to an ECS Task. Attaching an IAM Role is a more suitable solution in this scenario and not an IAM",
          "a: t says: Creating 4 different Container Instance IAM Roles with the required permissions and attaching them to each of the 4",
          "C: S tasks is incorrect because a Container Instance IAM Role only applies if you are using the EC2 launch type. Take note that the scenario says",
          "a: t the application will be using a Fargate launch type.",
          "a: t says: Creating 4 different Service-Linked Roles with the required permissions and attaching them to each of the 4 ECS",
          "a: sks is incorrect because a service-linked role is a unique type of IAM role that is linked directly to Amazon ECS itself, not on the ECS task."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "' ~~ ~~ ~ oy\n| - elastic network elastic network elastic network |\n| . interface interface interface <\nFE pn\nJitu siulsotisiivsisutaltsiiuiuisatn\nI i Tl\n: | Cl\nTask definition task task |\n| |\n| : od\nService I | |!\n| |\n! CC® 66 ®\n| . elastic network elastic network elastic network: ©\n| | interface interface interface | |\n| . Amazon ECS duster Cl\n. AZ2 2)\nWhen you attach a policy to a user or group of users, it allows or denies the users permission to perform the specified tasks on the specified\nresources. Likewise, Amazon ECS container instances make calls to the Amazon ECS and Amazon EC2 APIs on your behalf, so they need to\nauthenticate with your credentials. This authentication is accomplished by creating an IAM role for your container instances and associating that\nrole with your container instances when you launch them.\nIf you use an Elastic Load Balancing load balancer with your Amazon ECS services, calls to the Amazon EC2 and Elastic Load Balancing APIs are\nmade on your behalf to register and deregister container instances with your load balancers.\nHence, the most suitable solution in this scenario is: Create 4 different IAM Roles with the required permissions and attach them to each of the\n4 ECS tasks.\nThe option that says: Creating an IAM Group with all the required permissions and attaching them to each of the 4 ECS tasks is incorrect\nbecause you cannot directly attach an IAM Group to an ECS Task. Attaching an IAM Role is a more suitable solution in this scenario and not an IAM\nGroup.\nThe option that says: Creating 4 different Container Instance IAM Roles with the required permissions and attaching them to each of the 4\nECS tasks is incorrect because a Container Instance IAM Role only applies if you are using the EC2 launch type. Take note that the scenario says\nthat the application will be using a Fargate launch type.\nThe option that says: Creating 4 different Service-Linked Roles with the required permissions and attaching them to each of the 4 ECS\ntasks is incorrect because a service-linked role is a unique type of IAM role that is linked directly to Amazon ECS itself, not on the ECS task."
      },
      "tags": {
        "services": [
          "EC2",
          "ECS",
          "Fargate",
          "IAM"
        ],
        "domains": [
          "Security"
        ],
        "keywords": [
          "container",
          "authentication",
          "IAM role",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172747.png",
      "parsed": {
        "question": "K\nCategory: CDA - Deployment\nA developer is preparing the application specification ( AppsSpec ) file in CodeDeploy, which will be used to deploy her Lambda functions to\nAWS. In the deployment, she needs to configure CodeDeploy to run a task before the traffic is shifted to the deployed Lambda function\nversion.\nWhich deployment lifecycle event should she configure in this scenario?\nBeforeAllowTraffic\nBeforelnstall\nStart\nIncorrect\nThe content inthe 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment.\nThe 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more\nscripts. The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment\nlifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or\nLambda validation functions as part of the deployment.\nAn AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed\nonce per deployment. Following are descriptions of the hooks that are available for use in your AppSpec file.\n1 BeforeAllowTraffic — Use to run tasks before traffic is shifted to the deployed Lambda function version.",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: developer is preparing the application specification ( AppsSpec ) file in CodeDeploy, which will be used to deploy her Lambda functions to",
          "A: WS. In the deployment, she needs to configure CodeDeploy to run a task before the traffic is shifted to the deployed Lambda function",
          "c: h deployment lifecycle event should she configure in this scenario?",
          "B: eforeAllowTraffic",
          "B: eforelnstall",
          "a: rt",
          "c: orrect",
          "c: ontent inthe 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment.",
          "c: tion for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more",
          "c: ripts. The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment",
          "c: ycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or",
          "a: mbda validation functions as part of the deployment.",
          "A: n AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed",
          "c: e per deployment. Following are descriptions of the hooks that are available for use in your AppSpec file.",
          "B: eforeAllowTraffic — Use to run tasks before traffic is shifted to the deployed Lambda function version."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "15. QUESTION K\nCategory: CDA - Deployment\nA developer is preparing the application specification ( AppsSpec ) file in CodeDeploy, which will be used to deploy her Lambda functions to\nAWS. In the deployment, she needs to configure CodeDeploy to run a task before the traffic is shifted to the deployed Lambda function\nversion.\nWhich deployment lifecycle event should she configure in this scenario?\nBeforeAllowTraffic\nBeforelnstall\nStart\nIncorrect\nThe content inthe 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment.\nThe 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more\nscripts. The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment\nlifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or\nLambda validation functions as part of the deployment.\nAn AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed\nonce per deployment. Following are descriptions of the hooks that are available for use in your AppSpec file.\n1 BeforeAllowTraffic — Use to run tasks before traffic is shifted to the deployed Lambda function version."
      },
      "tags": {
        "services": [
          "EC2",
          "Lambda",
          "ECS",
          "CodeDeploy",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172817.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nAn organization has a serverless application using AWS Lambda, Amazon API Gateway. Recently, the DevOps team discovered that the IAM\nroles associated with the Lambda functions had been manually modified. The organization must identify these unauthorized changes and\nensure all resources are in sync with the CloudFormation stack.\nWhich solution will help the company identify these changes?\nRun a drift detection check on the CloudFormation stack.\nUse AWS Config to monitor updates made to the Lambda functions and IAM roles.\nAnalyze CloudWatch Logs to identify changes to the IAM role permissions.\n® Review CloudTrail logs to trace IAM role updates for the Lambda functions.\nIncorrect\nCloudFormation drift detection is a feature that allows you to identify differences between the actual configuration of your AWS resources and\ntheir expected configuration as defined in your CloudFormation stack template.\nSome of its capabilities are:\n— Compares a stack’s current state of resources with their expected state as defined in the CloudFormation template.\n— Identifies any discrepancies or “drift” between the actual and expected configurations of resources.\n— Assists in identifying potential issues affecting stack operations or overall infrastructure integrity.\n. — Helps maintain consistency and compliance by detecting unintended or unauthorized changes outside of CloudFormation.",
        "options": [
          "A: 4 4 A 4",
          "C: ategory: CDA - Deployment",
          "A: n organization has a serverless application using AWS Lambda, Amazon API Gateway. Recently, the DevOps team discovered that the IAM",
          "a: ssociated with the Lambda functions had been manually modified. The organization must identify these unauthorized changes and",
          "a: ll resources are in sync with the CloudFormation stack.",
          "c: h solution will help the company identify these changes?",
          "a: drift detection check on the CloudFormation stack.",
          "A: WS Config to monitor updates made to the Lambda functions and IAM roles.",
          "A: nalyze CloudWatch Logs to identify changes to the IAM role permissions.",
          "C: loudTrail logs to trace IAM role updates for the Lambda functions.",
          "c: orrect",
          "C: loudFormation drift detection is a feature that allows you to identify differences between the actual configuration of your AWS resources and",
          "c: ted configuration as defined in your CloudFormation stack template.",
          "c: apabilities are:",
          "C: ompares a stack’s current state of resources with their expected state as defined in the CloudFormation template.",
          "d: entifies any discrepancies or “drift” between the actual and expected configurations of resources.",
          "A: ssists in identifying potential issues affecting stack operations or overall infrastructure integrity.",
          "a: intain consistency and compliance by detecting unintended or unauthorized changes outside of CloudFormation."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "A 4 4 A 4\n16. QUESTION\nCategory: CDA - Deployment\nAn organization has a serverless application using AWS Lambda, Amazon API Gateway. Recently, the DevOps team discovered that the IAM\nroles associated with the Lambda functions had been manually modified. The organization must identify these unauthorized changes and\nensure all resources are in sync with the CloudFormation stack.\nWhich solution will help the company identify these changes?\nRun a drift detection check on the CloudFormation stack.\nUse AWS Config to monitor updates made to the Lambda functions and IAM roles.\nAnalyze CloudWatch Logs to identify changes to the IAM role permissions.\n® Review CloudTrail logs to trace IAM role updates for the Lambda functions.\nIncorrect\nCloudFormation drift detection is a feature that allows you to identify differences between the actual configuration of your AWS resources and\ntheir expected configuration as defined in your CloudFormation stack template.\nSome of its capabilities are:\n— Compares a stack’s current state of resources with their expected state as defined in the CloudFormation template.\n— Identifies any discrepancies or “drift” between the actual and expected configurations of resources.\n— Assists in identifying potential issues affecting stack operations or overall infrastructure integrity.\n. — Helps maintain consistency and compliance by detecting unintended or unauthorized changes outside of CloudFormation."
      },
      "tags": {
        "services": [
          "Lambda",
          "API Gateway",
          "IAM",
          "CloudFormation",
          "CloudWatch",
          "CloudTrail",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "deployment",
          "IAM role"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172840.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nAn application has recently been migrated from an on-premises data center to a development Elastic Beanstalk environment. A developer will\ndo iterative tests and therefore needs to deploy code changes and view them as quickly as possible.\nWhich of the following options take the LEAST amount of time to complete the deployment?\n® Immutable\nRolling with additional batch\nRolling\nAll at once\nIncorrect\nIn ElasticBeanstalk, you can choose from a variety of deployment methods:\n-All at once - Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time\nwhile the deployment occurs. This is the method that provides the least amount of time for deployment.\n-Rolling — Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment'’s\ncapacity by the number of instances in a batch.\n-Rolling with additional batch — Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during\nthe deployment process.",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: n application has recently been migrated from an on-premises data center to a development Elastic Beanstalk environment. A developer will",
          "d: o iterative tests and therefore needs to deploy code changes and view them as quickly as possible.",
          "c: h of the following options take the LEAST amount of time to complete the deployment?",
          "a: ble",
          "a: dditional batch",
          "A: ll at once",
          "c: orrect",
          "a: sticBeanstalk, you can choose from a variety of deployment methods:",
          "A: ll at once - Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time",
          "d: eployment occurs. This is the method that provides the least amount of time for deployment.",
          "D: eploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment'’s",
          "c: apacity by the number of instances in a batch.",
          "a: dditional batch — Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during",
          "d: eployment process."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "17. QUESTION\nCategory: CDA - Deployment\nAn application has recently been migrated from an on-premises data center to a development Elastic Beanstalk environment. A developer will\ndo iterative tests and therefore needs to deploy code changes and view them as quickly as possible.\nWhich of the following options take the LEAST amount of time to complete the deployment?\n® Immutable\nRolling with additional batch\nRolling\nAll at once\nIncorrect\nIn ElasticBeanstalk, you can choose from a variety of deployment methods:\n-All at once - Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time\nwhile the deployment occurs. This is the method that provides the least amount of time for deployment.\n-Rolling — Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment'’s\ncapacity by the number of instances in a batch.\n-Rolling with additional batch — Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during\nthe deployment process."
      },
      "tags": {
        "services": [
          "Elastic Beanstalk"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": [
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 172943.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer wants to track the number of visitors on their website, which has a DynamoDB database. This is primarily used to give a rough E\nidea on how many people visit the site whenever they launch a new advertisement, which means it can tolerate a slight over-counting or\nundercounting of website visitors.\nWhich of the following will satisfy the requirement with MINIMAL configuration?\n® Enable DynamoDB Streams to track the number of new visitors.\nUse conditional writes to update the counter item in the DynamoDB table and set the ReturnConsumedCapacity\nparameter to TOTAL .\nUse atomic counters to increment the counter item in the DynamoDB table for every new visitor.\nUse conditional writes to update the counter item in the DynamoDB table only if the item has a unique primary key and\nthe new value is greater than the current value.\nIncorrect\nYou can use the UpdateItem operation to implement an atomic counter — a numeric attribute that is incremented, unconditionally, without\ninterfering with other write requests. (All write requests are applied in the order in which they were received). With an atomic counter, the updates\nare not idempotent. In other words, the numeric value will increment each time you call UpdateItem .\nYou might use an atomic counter to keep track of the number of visitors to a website. In this case, your application would increment a numeric\nvalue, regardless of its current value. If an UpdateItem operation should fail, the application could simply retry the operation. This would risk\nupdating the counter twice, but you could probably tolerate a slight overcounting or undercounting of website visitors.\nAn atomic counter would not be appropriate where overcounting or undercounting cannot be tolerated (For example, in a banking application). In\n— PO Tr Ty TT FC STU Tr PPR Ur Ey Ir Ss",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer wants to track the number of visitors on their website, which has a DynamoDB database. This is primarily used to give a rough E",
          "d: ea on how many people visit the site whenever they launch a new advertisement, which means it can tolerate a slight over-counting or",
          "d: ercounting of website visitors.",
          "c: h of the following will satisfy the requirement with MINIMAL configuration?",
          "a: ble DynamoDB Streams to track the number of new visitors.",
          "c: onditional writes to update the counter item in the DynamoDB table and set the ReturnConsumedCapacity",
          "a: rameter to TOTAL .",
          "a: tomic counters to increment the counter item in the DynamoDB table for every new visitor.",
          "c: onditional writes to update the counter item in the DynamoDB table only if the item has a unique primary key and",
          "a: lue is greater than the current value.",
          "c: orrect",
          "c: an use the UpdateItem operation to implement an atomic counter — a numeric attribute that is incremented, unconditionally, without",
          "A: ll write requests are applied in the order in which they were received). With an atomic counter, the updates",
          "a: re not idempotent. In other words, the numeric value will increment each time you call UpdateItem .",
          "a: n atomic counter to keep track of the number of visitors to a website. In this case, your application would increment a numeric",
          "a: lue, regardless of its current value. If an UpdateItem operation should fail, the application could simply retry the operation. This would risk",
          "d: ating the counter twice, but you could probably tolerate a slight overcounting or undercounting of website visitors.",
          "A: n atomic counter would not be appropriate where overcounting or undercounting cannot be tolerated (For example, in a banking application). In",
          "C: STU Tr PPR Ur Ey Ir Ss"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "19. QUESTION\nCategory: CDA - Development with AWS Services\nA developer wants to track the number of visitors on their website, which has a DynamoDB database. This is primarily used to give a rough E\nidea on how many people visit the site whenever they launch a new advertisement, which means it can tolerate a slight over-counting or\nundercounting of website visitors.\nWhich of the following will satisfy the requirement with MINIMAL configuration?\n® Enable DynamoDB Streams to track the number of new visitors.\nUse conditional writes to update the counter item in the DynamoDB table and set the ReturnConsumedCapacity\nparameter to TOTAL .\nUse atomic counters to increment the counter item in the DynamoDB table for every new visitor.\nUse conditional writes to update the counter item in the DynamoDB table only if the item has a unique primary key and\nthe new value is greater than the current value.\nIncorrect\nYou can use the UpdateItem operation to implement an atomic counter — a numeric attribute that is incremented, unconditionally, without\ninterfering with other write requests. (All write requests are applied in the order in which they were received). With an atomic counter, the updates\nare not idempotent. In other words, the numeric value will increment each time you call UpdateItem .\nYou might use an atomic counter to keep track of the number of visitors to a website. In this case, your application would increment a numeric\nvalue, regardless of its current value. If an UpdateItem operation should fail, the application could simply retry the operation. This would risk\nupdating the counter twice, but you could probably tolerate a slight overcounting or undercounting of website visitors.\nAn atomic counter would not be appropriate where overcounting or undercounting cannot be tolerated (For example, in a banking application). In\n— PO Tr Ty TT FC STU Tr PPR Ur Ey Ir Ss"
      },
      "tags": {
        "services": [
          "EBS",
          "RDS",
          "DynamoDB",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "idempotent"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 173007.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization E\nA developer has a set of EC2 instances that runs the Amazon Kinesis Client Library to process a data stream in AWS. Based on the custom\nmetrics, it shows that the instances are maxing out their CPU Utilization, and there are insufficient Kinesis shards to handle the rate of data\nflowing through the stream.\nWhich of the following is the BEST course of action that the developer should take to solve this issue and prevent this situation from re-\noccurring in the future?\nIncrease both the instance size and the number of open shards.\n® Increase the number of shards.\nIncrease the number of instances up to the number of open shards.\nIncrease the instance size to a larger type.\nIncorrect\nResharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through\nthe stream. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also\ntracks the shards in the stream using an Amazon DynamoDB table. When new shards are created as a result of resharding, the KCL discovers the\nnew shards and populates new rows in the table. The workers automatically discover the new shards and create processors to handle the data\nfrom them. The KCL also distributes the shards in the stream across all the available workers and record processors.\nBy increasing the instance size and number of shards in your Kinesis stream, the developer allows the instances to handle more record processors,\nwhich are running in parallel within the instance. It also allows the stream to properly accommodate the rate of data being sent in. The data\ncapacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the\n| eae. ata Lm.",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization E",
          "A: developer has a set of EC2 instances that runs the Amazon Kinesis Client Library to process a data stream in AWS. Based on the custom",
          "c: s, it shows that the instances are maxing out their CPU Utilization, and there are insufficient Kinesis shards to handle the rate of data",
          "a: m.",
          "c: h of the following is the BEST course of action that the developer should take to solve this issue and prevent this situation from re-",
          "c: curring in the future?",
          "c: rease both the instance size and the number of open shards.",
          "c: rease the number of shards.",
          "c: rease the number of instances up to the number of open shards.",
          "c: rease the instance size to a larger type.",
          "c: orrect",
          "a: rding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through",
          "a: m. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also",
          "a: cks the shards in the stream using an Amazon DynamoDB table. When new shards are created as a result of resharding, the KCL discovers the",
          "a: rds and populates new rows in the table. The workers automatically discover the new shards and create processors to handle the data",
          "C: L also distributes the shards in the stream across all the available workers and record processors.",
          "B: y increasing the instance size and number of shards in your Kinesis stream, the developer allows the instances to handle more record processors,",
          "c: h are running in parallel within the instance. It also allows the stream to properly accommodate the rate of data being sent in. The data",
          "c: apacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the",
          "a: e. ata Lm."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "20. QUESTION\nCategory: CDA - Troubleshooting and Optimization E\nA developer has a set of EC2 instances that runs the Amazon Kinesis Client Library to process a data stream in AWS. Based on the custom\nmetrics, it shows that the instances are maxing out their CPU Utilization, and there are insufficient Kinesis shards to handle the rate of data\nflowing through the stream.\nWhich of the following is the BEST course of action that the developer should take to solve this issue and prevent this situation from re-\noccurring in the future?\nIncrease both the instance size and the number of open shards.\n® Increase the number of shards.\nIncrease the number of instances up to the number of open shards.\nIncrease the instance size to a larger type.\nIncorrect\nResharding enables you to increase or decrease the number of shards in a stream in order to adapt to changes in the rate of data flowing through\nthe stream. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that shard. It also\ntracks the shards in the stream using an Amazon DynamoDB table. When new shards are created as a result of resharding, the KCL discovers the\nnew shards and populates new rows in the table. The workers automatically discover the new shards and create processors to handle the data\nfrom them. The KCL also distributes the shards in the stream across all the available workers and record processors.\nBy increasing the instance size and number of shards in your Kinesis stream, the developer allows the instances to handle more record processors,\nwhich are running in parallel within the instance. It also allows the stream to properly accommodate the rate of data being sent in. The data\ncapacity of your stream is a function of the number of shards that you specify for the stream. The total capacity of the stream is the sum of the\n| eae. ata Lm."
      },
      "tags": {
        "services": [
          "EC2",
          "RDS",
          "DynamoDB",
          "Kinesis",
          "ECR"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 173219.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nA company is heavily using a range of AWS services to host their enterprise applications. Currently, their deployment process still has a lot of E\nmanual steps which is why they plan to automate their software delivery process using continuous integration and delivery (CI/CD) pipelines in\nAWS. They will use CodePipeline to orchestrate each step of their release process and CodeDeploy for deploying applications to various\ncompute platforms in AWS.\nIn this architecture, which of the following are valid considerations when using CodeDeploy? (Select TWO.)\nThe CodeDeploy agent communicates using HTTP over port 80.\nCodeDeploy can deploy applications to EC2, AWS Lambda, and Amazon ECS only.\nYou have to install and use the CodeDeploy agent installed on your EC2 instances and ECS cluster.\nAWS Lambda compute platform deployments cannot use an in-place deployment type.\nCodeDeploy can deploy applications to both your EC2 instances as well as your on-premises servers.\nIncorrect\nCodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless\nLambda functions, or Amazon ECS services.\nCodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket\nrepositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can\nuse CodeDeploy.\n. ys - AWS CodePipeline -_— TTT",
        "options": [
          "C: ategory: CDA - Deployment",
          "A: company is heavily using a range of AWS services to host their enterprise applications. Currently, their deployment process still has a lot of E",
          "a: nual steps which is why they plan to automate their software delivery process using continuous integration and delivery (CI/CD) pipelines in",
          "A: WS. They will use CodePipeline to orchestrate each step of their release process and CodeDeploy for deploying applications to various",
          "c: ompute platforms in AWS.",
          "a: rchitecture, which of the following are valid considerations when using CodeDeploy? (Select TWO.)",
          "C: odeDeploy agent communicates using HTTP over port 80.",
          "C: odeDeploy can deploy applications to EC2, AWS Lambda, and Amazon ECS only.",
          "a: ve to install and use the CodeDeploy agent installed on your EC2 instances and ECS cluster.",
          "A: WS Lambda compute platform deployments cannot use an in-place deployment type.",
          "C: odeDeploy can deploy applications to both your EC2 instances as well as your on-premises servers.",
          "c: orrect",
          "C: odeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless",
          "a: mbda functions, or Amazon ECS services.",
          "C: odeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket",
          "C: odeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can",
          "C: odeDeploy.",
          "A: WS CodePipeline -_— TTT"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "28. QUESTION\nCategory: CDA - Deployment\nA company is heavily using a range of AWS services to host their enterprise applications. Currently, their deployment process still has a lot of E\nmanual steps which is why they plan to automate their software delivery process using continuous integration and delivery (CI/CD) pipelines in\nAWS. They will use CodePipeline to orchestrate each step of their release process and CodeDeploy for deploying applications to various\ncompute platforms in AWS.\nIn this architecture, which of the following are valid considerations when using CodeDeploy? (Select TWO.)\nThe CodeDeploy agent communicates using HTTP over port 80.\nCodeDeploy can deploy applications to EC2, AWS Lambda, and Amazon ECS only.\nYou have to install and use the CodeDeploy agent installed on your EC2 instances and ECS cluster.\nAWS Lambda compute platform deployments cannot use an in-place deployment type.\nCodeDeploy can deploy applications to both your EC2 instances as well as your on-premises servers.\nIncorrect\nCodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless\nLambda functions, or Amazon ECS services.\nCodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket\nrepositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can\nuse CodeDeploy.\n. ys - AWS CodePipeline -_— TTT"
      },
      "tags": {
        "services": [
          "EC2",
          "Lambda",
          "ECS",
          "S3",
          "CodeDeploy",
          "CodePipeline"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "deployment",
          "CI/CD"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 173450.png",
      "parsed": {
        "question": "Category: CDA - Security\nA company has an AWS Amplify application, relying on Amazon Cognito for user authentication. Multi-factor authentication (MF",
        "options": [
          "C: ategory: CDA - Security",
          "A: company has an AWS Amplify application, relying on Amazon Cognito for user authentication. Multi-factor authentication (MFA) is disabled KE",
          "a: s been a recent data breach in a popular website. The company is worried that attackers might exploit",
          "c: ompromised email addresses and passwords to sign into their applications. For this reason, they want to enforce MFA only on users with",
          "c: ious login attempts.",
          "c: an the company satisfy these requirements",
          "a: ble the Time-based one-time password (TOTP) software token MFA for the User Pool",
          "c: reate the User Pool and enable SMS text message MFA.",
          "C: reate a subscription filter Lambda function that monitors for the compromisedcredentialRisk metric from Advanced",
          "c: urity Metrics in CloudWatch Logs and triggers MFA when detected",
          "a: ble Adaptive Authentication for the User Pool",
          "c: orrect",
          "a: daptive authentication, you can configure your user pool to block suspicious sign-ins or add second factor authentication in response to an",
          "c: reased risk level. For each sign-in attempt, Amazon Cognito generates a risk score for how likely the sign-in request is to be from a",
          "c: ompromised source. This risk score is based on factors that include device and user information.",
          "A: daptive authentication info",
          "C: onfigure how Cognito responds to each level of risk when it detects potential malicious activity.",
          "C: ustomize adaptive authentication",
          "a: y I ane"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "36. QUESTION\nCategory: CDA - Security\nA company has an AWS Amplify application, relying on Amazon Cognito for user authentication. Multi-factor authentication (MFA) is disabled KE\nfor their User Pool. There has been a recent data breach in a popular website. The company is worried that attackers might exploit\ncompromised email addresses and passwords to sign into their applications. For this reason, they want to enforce MFA only on users with\nsuspicious login attempts.\nHow can the company satisfy these requirements\nEnable the Time-based one-time password (TOTP) software token MFA for the User Pool\nRecreate the User Pool and enable SMS text message MFA.\n® Create a subscription filter Lambda function that monitors for the compromisedcredentialRisk metric from Advanced\nSecurity Metrics in CloudWatch Logs and triggers MFA when detected\nEnable Adaptive Authentication for the User Pool\nIncorrect\nWith adaptive authentication, you can configure your user pool to block suspicious sign-ins or add second factor authentication in response to an\nincreased risk level. For each sign-in attempt, Amazon Cognito generates a risk score for how likely the sign-in request is to be from a\ncompromised source. This risk score is based on factors that include device and user information.\nAdaptive authentication info\nConfigure how Cognito responds to each level of risk when it detects potential malicious activity.\nCustomize adaptive authentication\nPay I ane"
      },
      "tags": {
        "services": [
          "Lambda",
          "EBS",
          "RDS",
          "Cognito",
          "CloudWatch",
          "Config",
          "ECR",
          "Amplify"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "security",
          "authentication"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 173705.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA developer is instrumenting an application that will be hosted in a large On-Demand EC2 instance in AWS. All of the downstream calls invoked\nby the application must be traced properly, including the AWS SDK calls. A user-defined data should also be present to expedite the\ntroubleshooting process.\nWhich of the following are valid considerations in AWS X-Ray that the developer should follow? (Select TWO.)\nSet the namespace subsegment field to remote for AWS SDK calls and aws for other downstream calls.\nSet the annotations object with any additional custom data that you want to store in the segment.\nSet the metadata object with key-value pairs that you want X-Ray to index for search.\nSet the metadata object with any additional custom data that you want to store in the segment.\nSet the namespace subsegment field to aws for AWS SDK calls and remote for other downstream calls.\nIncorrect\nA segment document conveys information about a segment to X-Ray. A segment document can be up to 64 kB and contain a whole segment with\nsubsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send\nsegment documents directly to X-Ray by using the PutTraceSegments API.\nX-Ray compiles and processes segment documents to generate queryable trace summaries and full traces that you can access by using\nthe GetTracesSummaries and BatchGetTraces APIs, respectively. In addition to the segments and subsegments that you send to X-Ray,\nthe service uses information in subsegments to generate inferred segments and adds them to the full trace. Inferred segments represent\ndownstream services and resources in the service map.\n1 RT SUI VY, FNNRAGUVL, OVE IN ATR WT LR RANT ems an.",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: developer is instrumenting an application that will be hosted in a large On-Demand EC2 instance in AWS. All of the downstream calls invoked",
          "b: y the application must be traced properly, including the AWS SDK calls. A user-defined data should also be present to expedite the",
          "b: leshooting process.",
          "c: h of the following are valid considerations in AWS X-Ray that the developer should follow? (Select TWO.)",
          "a: mespace subsegment field to remote for AWS SDK calls and aws for other downstream calls.",
          "a: nnotations object with any additional custom data that you want to store in the segment.",
          "a: data object with key-value pairs that you want X-Ray to index for search.",
          "a: data object with any additional custom data that you want to store in the segment.",
          "a: mespace subsegment field to aws for AWS SDK calls and remote for other downstream calls.",
          "c: orrect",
          "A: segment document conveys information about a segment to X-Ray. A segment document can be up to 64 kB and contain a whole segment with",
          "b: segments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send",
          "d: ocuments directly to X-Ray by using the PutTraceSegments API.",
          "a: y compiles and processes segment documents to generate queryable trace summaries and full traces that you can access by using",
          "a: cesSummaries and BatchGetTraces APIs, respectively. In addition to the segments and subsegments that you send to X-Ray,",
          "c: e uses information in subsegments to generate inferred segments and adds them to the full trace. Inferred segments represent",
          "d: ownstream services and resources in the service map.",
          "A: GUVL, OVE IN ATR WT LR RANT ems an."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "41. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA developer is instrumenting an application that will be hosted in a large On-Demand EC2 instance in AWS. All of the downstream calls invoked\nby the application must be traced properly, including the AWS SDK calls. A user-defined data should also be present to expedite the\ntroubleshooting process.\nWhich of the following are valid considerations in AWS X-Ray that the developer should follow? (Select TWO.)\nSet the namespace subsegment field to remote for AWS SDK calls and aws for other downstream calls.\nSet the annotations object with any additional custom data that you want to store in the segment.\nSet the metadata object with key-value pairs that you want X-Ray to index for search.\nSet the metadata object with any additional custom data that you want to store in the segment.\nSet the namespace subsegment field to aws for AWS SDK calls and remote for other downstream calls.\nIncorrect\nA segment document conveys information about a segment to X-Ray. A segment document can be up to 64 kB and contain a whole segment with\nsubsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send\nsegment documents directly to X-Ray by using the PutTraceSegments API.\nX-Ray compiles and processes segment documents to generate queryable trace summaries and full traces that you can access by using\nthe GetTracesSummaries and BatchGetTraces APIs, respectively. In addition to the segments and subsegments that you send to X-Ray,\nthe service uses information in subsegments to generate inferred segments and adds them to the full trace. Inferred segments represent\ndownstream services and resources in the service map.\n1 RT SUI VY, FNNRAGUVL, OVE IN ATR WT LR RANT ems an."
      },
      "tags": {
        "services": [
          "EC2",
          "X-Ray"
        ],
        "domains": [
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 173752.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA Docker application hosted on an ECS cluster has encountered intermittent unavailability issues and timeouts. The lead DevOps engineer\ninstructed you to instrument the application to detect where high latencies are occurring and to determine the specific services and paths\nimpacting application performance.\nWhich of the following steps should you take to accomplish this task properly? (Select TWO.)\nConfigure the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000.\nCreate a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your\nAmazon ECS cluster.\nConfigure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.\nAdd the xray-daemon.config configuration file in your Docker image\nManually install the X-Ray daemon to the instances via a user data script.\nIncorrect\nThe AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the\nSDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the\ndaemon alongside your application.\nTo properly instrument your applications in Amazon ECS, you have to create a Docker image that runs the X-Ray daemon, upload it to a Docker\nimage repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file\nto allow your application to communicate with the daemon container.\nRun the AWS X-Ray daemon",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: Docker application hosted on an ECS cluster has encountered intermittent unavailability issues and timeouts. The lead DevOps engineer",
          "c: ted you to instrument the application to detect where high latencies are occurring and to determine the specific services and paths",
          "a: cting application performance.",
          "c: h of the following steps should you take to accomplish this task properly? (Select TWO.)",
          "C: onfigure the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000.",
          "C: reate a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your",
          "A: mazon ECS cluster.",
          "C: onfigure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.",
          "A: dd the xray-daemon.config configuration file in your Docker image",
          "a: nually install the X-Ray daemon to the instances via a user data script.",
          "c: orrect",
          "A: WS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the",
          "D: K sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the",
          "d: aemon alongside your application.",
          "a: pplications in Amazon ECS, you have to create a Docker image that runs the X-Ray daemon, upload it to a Docker",
          "a: ge repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file",
          "a: llow your application to communicate with the daemon container.",
          "A: WS X-Ray daemon"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "42. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA Docker application hosted on an ECS cluster has encountered intermittent unavailability issues and timeouts. The lead DevOps engineer\ninstructed you to instrument the application to detect where high latencies are occurring and to determine the specific services and paths\nimpacting application performance.\nWhich of the following steps should you take to accomplish this task properly? (Select TWO.)\nConfigure the port mappings and network mode settings in the container agent to allow traffic on TCP port 2000.\nCreate a Docker image that runs the X-Ray daemon, upload it to a Docker image repository, and then deploy it to your\nAmazon ECS cluster.\nConfigure the port mappings and network mode settings in your task definition file to allow traffic on UDP port 2000.\nAdd the xray-daemon.config configuration file in your Docker image\nManually install the X-Ray daemon to the instances via a user data script.\nIncorrect\nThe AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the\nSDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the\ndaemon alongside your application.\nTo properly instrument your applications in Amazon ECS, you have to create a Docker image that runs the X-Ray daemon, upload it to a Docker\nimage repository, and then deploy it to your Amazon ECS cluster. You can use port mappings and network mode settings in your task definition file\nto allow your application to communicate with the daemon container.\nRun the AWS X-Ray daemon"
      },
      "tags": {
        "services": [
          "ECS",
          "X-Ray",
          "Config"
        ],
        "domains": [
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "container",
          "performance"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 173820.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA company has a global multi-player game with a multi-master DynamoDB database topology which stores data in multiple AWS regions. You |\nwere assigned to develop a real-time data analytics application which will track and store the recent changes on all the tables from various\nregions. Only the new data of the recently updated item is needed to be tracked by your application.\nWhich of the following is the MOST suitable way to configure the data analytics application to detect and retrieve the updated database entries\nautomatically?\nEnable DynamoDB Streams and set the value of streamviewType to NEW_AND_OLD_IMAGE. Use Kinesis Adapter in the\napplication to consume streams from DynamoDB.\nEnable DynamoDB Streams and set the value of streamviewType to NEW_AND_OLD_IMAGE. Create a trigger in AWS\nLambda to capture stream data and forward it to your application.\nEnable DynamoDB Streams and set the value of streamviewType to NEW_IMAGE. Use Kinesis Adapter in the application to\nconsume streams from DynamoDB.\n® Enable DynamoDB Streams and set the value of streamviewType to NEW_IMAGE. Create a trigger in AWS Lambda to\ncapture stream data and forward it to your application.\nIncorrect\nDynamoDB Streams provides a time-ordered sequence of item-level changes in any DynamoDB table. The changes are de-duplicated and stored\nfor 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time.\nThe Kinesis Adapter is the recommended way to consume streams from DynamoDB for real-time processing. The DynamoDB Streams API is\nintentionally similar to that of Kinesis Streams, a service for real-time processing of streaming data at a massive scale. You can write applications\nfor Kinesis Streams using the Kinesis Client Library (KCL). The KCL simplifies coding by providing useful abstractions above the low-level Kinesis",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: company has a global multi-player game with a multi-master DynamoDB database topology which stores data in multiple AWS regions. You |",
          "a: ssigned to develop a real-time data analytics application which will track and store the recent changes on all the tables from various",
          "d: ata of the recently updated item is needed to be tracked by your application.",
          "c: h of the following is the MOST suitable way to configure the data analytics application to detect and retrieve the updated database entries",
          "a: utomatically?",
          "a: ble DynamoDB Streams and set the value of streamviewType to NEW_AND_OLD_IMAGE. Use Kinesis Adapter in the",
          "a: pplication to consume streams from DynamoDB.",
          "a: ble DynamoDB Streams and set the value of streamviewType to NEW_AND_OLD_IMAGE. Create a trigger in AWS",
          "a: mbda to capture stream data and forward it to your application.",
          "a: ble DynamoDB Streams and set the value of streamviewType to NEW_IMAGE. Use Kinesis Adapter in the application to",
          "c: onsume streams from DynamoDB.",
          "a: ble DynamoDB Streams and set the value of streamviewType to NEW_IMAGE. Create a trigger in AWS Lambda to",
          "c: apture stream data and forward it to your application.",
          "c: orrect",
          "D: ynamoDB Streams provides a time-ordered sequence of item-level changes in any DynamoDB table. The changes are de-duplicated and stored",
          "A: pplications can access this log and view the data items as they appeared before and after they were modified, in near real time.",
          "A: dapter is the recommended way to consume streams from DynamoDB for real-time processing. The DynamoDB Streams API is",
          "a: lly similar to that of Kinesis Streams, a service for real-time processing of streaming data at a massive scale. You can write applications",
          "a: ms using the Kinesis Client Library (KCL). The KCL simplifies coding by providing useful abstractions above the low-level Kinesis"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "43. QUESTION\nCategory: CDA - Development with AWS Services\nA company has a global multi-player game with a multi-master DynamoDB database topology which stores data in multiple AWS regions. You |\nwere assigned to develop a real-time data analytics application which will track and store the recent changes on all the tables from various\nregions. Only the new data of the recently updated item is needed to be tracked by your application.\nWhich of the following is the MOST suitable way to configure the data analytics application to detect and retrieve the updated database entries\nautomatically?\nEnable DynamoDB Streams and set the value of streamviewType to NEW_AND_OLD_IMAGE. Use Kinesis Adapter in the\napplication to consume streams from DynamoDB.\nEnable DynamoDB Streams and set the value of streamviewType to NEW_AND_OLD_IMAGE. Create a trigger in AWS\nLambda to capture stream data and forward it to your application.\nEnable DynamoDB Streams and set the value of streamviewType to NEW_IMAGE. Use Kinesis Adapter in the application to\nconsume streams from DynamoDB.\n® Enable DynamoDB Streams and set the value of streamviewType to NEW_IMAGE. Create a trigger in AWS Lambda to\ncapture stream data and forward it to your application.\nIncorrect\nDynamoDB Streams provides a time-ordered sequence of item-level changes in any DynamoDB table. The changes are de-duplicated and stored\nfor 24 hours. Applications can access this log and view the data items as they appeared before and after they were modified, in near real time.\nThe Kinesis Adapter is the recommended way to consume streams from DynamoDB for real-time processing. The DynamoDB Streams API is\nintentionally similar to that of Kinesis Streams, a service for real-time processing of streaming data at a massive scale. You can write applications\nfor Kinesis Streams using the Kinesis Client Library (KCL). The KCL simplifies coding by providing useful abstractions above the low-level Kinesis"
      },
      "tags": {
        "services": [
          "Lambda",
          "DynamoDB",
          "Kinesis",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 173903.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nYou recently deployed an application to a newly created AWS account, which uses two identical Lambda functions to process ad-hoc requests. [\nThe first function processes incoming requests efficiently but the second one has a longer processing time even though both of the functions\nhave exactly the same code. Based on your monitoring, the Throttles metric of the second function is greater than the first one in Amazon\nCloudWatch.\nWhich of the following are possible solutions that you can implement to fix this issue? (Select TWO.)\nSet the concurrency execution limit of both functions to 500.\nSet the concurrency execution limit of the second function to 0.\nConfigure the second function to use an unreserved account concurrency.\nSet the concurrency execution limit of both functions to 450.\nDecrease the concurrency execution limit of the first function.\nIncorrect\nThe unit of scale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may\nwant to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a\ndownstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level.\nThe concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the\nconcurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing\nevents from a poll-based event source.\n=",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "c: ently deployed an application to a newly created AWS account, which uses two identical Lambda functions to process ad-hoc requests. [",
          "c: tion processes incoming requests efficiently but the second one has a longer processing time even though both of the functions",
          "a: ve exactly the same code. Based on your monitoring, the Throttles metric of the second function is greater than the first one in Amazon",
          "C: loudWatch.",
          "c: h of the following are possible solutions that you can implement to fix this issue? (Select TWO.)",
          "c: oncurrency execution limit of both functions to 500.",
          "c: oncurrency execution limit of the second function to 0.",
          "C: onfigure the second function to use an unreserved account concurrency.",
          "c: oncurrency execution limit of both functions to 450.",
          "D: ecrease the concurrency execution limit of the first function.",
          "c: orrect",
          "c: ale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may",
          "a: nt to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a",
          "d: ownstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level.",
          "c: oncurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the",
          "c: oncurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing",
          "a: poll-based event source."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "44. QUESTION\nCategory: CDA - Development with AWS Services\nYou recently deployed an application to a newly created AWS account, which uses two identical Lambda functions to process ad-hoc requests. [\nThe first function processes incoming requests efficiently but the second one has a longer processing time even though both of the functions\nhave exactly the same code. Based on your monitoring, the Throttles metric of the second function is greater than the first one in Amazon\nCloudWatch.\nWhich of the following are possible solutions that you can implement to fix this issue? (Select TWO.)\nSet the concurrency execution limit of both functions to 500.\nSet the concurrency execution limit of the second function to 0.\nConfigure the second function to use an unreserved account concurrency.\nSet the concurrency execution limit of both functions to 450.\nDecrease the concurrency execution limit of the first function.\nIncorrect\nThe unit of scale for AWS Lambda is a concurrent execution. However, scaling indefinitely is not desirable in all scenarios. For example, you may\nwant to control your concurrency for cost reasons or to regulate how long it takes you to process a batch of events, or to simply match it with a\ndownstream resource. To assist with this, Lambda provides a concurrent execution limit control at both the account level and the function level.\nThe concurrent executions refers to the number of executions of your function code that are happening at any given time. You can estimate the\nconcurrent execution count, but the concurrent execution count will differ depending on whether or not your Lambda function is processing\nevents from a poll-based event source.\n="
      },
      "tags": {
        "services": [
          "Lambda",
          "CloudWatch",
          "Config",
          "ECR",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "scaling",
          "monitoring"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 173935.png",
      "parsed": {
        "question": "[\nCategory: CDA - Security\nYour development team is currently developing a financial application in AWS. One of the requirements is to create and control the encryption\nkeys used to encrypt your data using the envelope encryption strategy to comply with the strict IT security policy of the company.\nWhich of the following correctly describes the process of envelope encryption?\n| @® Encrypt plaintext data with a data key and then encrypt the data key with a top-level encrypted key. |\nEncrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level plaintext data key.\nEncrypt plaintext data with a data key and then encrypt the data key with a top-level plaintext key.\nEncrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level encrypted data key.\nIncorrect\nWhen you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope\nencryption is the practice of encrypting plaintext data with a data key and then encrypting the data key under another key.\nGenerates data key Encrypts\n: =",
        "options": [
          "D: O",
          "C: ategory: CDA - Security",
          "d: evelopment team is currently developing a financial application in AWS. One of the requirements is to create and control the encryption",
          "d: to encrypt your data using the envelope encryption strategy to comply with the strict IT security policy of the company.",
          "c: h of the following correctly describes the process of envelope encryption?",
          "c: rypt plaintext data with a data key and then encrypt the data key with a top-level encrypted key. |",
          "c: rypt plaintext data with a KMS key and then encrypt the KMS key with a top-level plaintext data key.",
          "c: rypt plaintext data with a data key and then encrypt the data key with a top-level plaintext key.",
          "c: rypt plaintext data with a KMS key and then encrypt the KMS key with a top-level encrypted data key.",
          "c: orrect",
          "c: rypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope",
          "c: ryption is the practice of encrypting plaintext data with a data key and then encrypting the data key under another key.",
          "a: tes data key Encrypts"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "EDO\n45. QUESTION [\nCategory: CDA - Security\nYour development team is currently developing a financial application in AWS. One of the requirements is to create and control the encryption\nkeys used to encrypt your data using the envelope encryption strategy to comply with the strict IT security policy of the company.\nWhich of the following correctly describes the process of envelope encryption?\n| @® Encrypt plaintext data with a data key and then encrypt the data key with a top-level encrypted key. |\nEncrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level plaintext data key.\nEncrypt plaintext data with a data key and then encrypt the data key with a top-level plaintext key.\nEncrypt plaintext data with a KMS key and then encrypt the KMS key with a top-level encrypted data key.\nIncorrect\nWhen you encrypt your data, your data is protected, but you have to protect your encryption key. One strategy is to encrypt it. Envelope\nencryption is the practice of encrypting plaintext data with a data key and then encrypting the data key under another key.\nGenerates data key Encrypts\n: ="
      },
      "tags": {
        "services": [
          "KMS"
        ],
        "domains": [
          "Security"
        ],
        "keywords": [
          "security",
          "encryption",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 213246.png",
      "parsed": {
        "question": "Category: CDA - Security\nA company has different AWS accounts, namely Account A, Account B, and Account C, which are used for their Development, Test, and\nProduction environments respectively. A developer needs access to perform an audit whenever a new version of the application has been\ndeployed to the Test (Account B) and production (Account C) environments.\nWhat is the MOST efficient way to provide the developer access to execute the specified task?\n® Set up AWS Organizations and attach a Service Control Policy to the developer to access the other accounts.\nGrant the developer cross-account access to the resources of Accounts B and C.\nCreate separate identities and passwords for the developer on both the Test and Production accounts.\nEnable AWS multi-factor authentication (MF",
        "options": [
          "C: ategory: CDA - Security",
          "A: company has different AWS accounts, namely Account A, Account B, and Account C, which are used for their Development, Test, and",
          "d: uction environments respectively. A developer needs access to perform an audit whenever a new version of the application has been",
          "d: eployed to the Test (Account B) and production (Account C) environments.",
          "a: t is the MOST efficient way to provide the developer access to execute the specified task?",
          "A: WS Organizations and attach a Service Control Policy to the developer to access the other accounts.",
          "a: nt the developer cross-account access to the resources of Accounts B and C.",
          "C: reate separate identities and passwords for the developer on both the Test and Production accounts.",
          "a: ble AWS multi-factor authentication (MFA) to the IAM User of the developer.",
          "c: orrect",
          "c: an grant your IAM users permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own.",
          "a: gine that you have Amazon EC2 instances that are critical to your organization. Instead of directly granting your users permission to terminate",
          "a: nces, you can create a role with those privileges. Then allow administrators to switch to the role when they need to terminate an instance.",
          "D: oing this adds the following layers of protection to the instances:",
          "c: itly grant your users permission to assume the role.",
          "a: ctively switch to the role using the AWS Management Console or assume the role using the AWS CLI or AWS API."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "46. QUESTION\nCategory: CDA - Security\nA company has different AWS accounts, namely Account A, Account B, and Account C, which are used for their Development, Test, and\nProduction environments respectively. A developer needs access to perform an audit whenever a new version of the application has been\ndeployed to the Test (Account B) and production (Account C) environments.\nWhat is the MOST efficient way to provide the developer access to execute the specified task?\n® Set up AWS Organizations and attach a Service Control Policy to the developer to access the other accounts.\nGrant the developer cross-account access to the resources of Accounts B and C.\nCreate separate identities and passwords for the developer on both the Test and Production accounts.\nEnable AWS multi-factor authentication (MFA) to the IAM User of the developer.\nIncorrect\nYou can grant your IAM users permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own.\nImagine that you have Amazon EC2 instances that are critical to your organization. Instead of directly granting your users permission to terminate\nthe instances, you can create a role with those privileges. Then allow administrators to switch to the role when they need to terminate an instance.\nDoing this adds the following layers of protection to the instances:\n— You must explicitly grant your users permission to assume the role.\n— Your users must actively switch to the role using the AWS Management Console or assume the role using the AWS CLI or AWS API."
      },
      "tags": {
        "services": [
          "EC2",
          "RDS",
          "IAM"
        ],
        "domains": [
          "Security"
        ],
        "keywords": [
          "security",
          "authentication",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 213359.png",
      "parsed": {
        "question": "Category: CDA - Security\no\nYour manager assigned you a task of implementing server-side encryption with customer-provided encryption keys (SSE-C) to your S3 bucket, ki\nwhich will allow you to set your own encryption keys. Amazon S3 will manage both the encryption and decryption process using your key when\nyou access your objects, which will remove the burden of maintaining any code to perform data encryption and decryption.\nTo properly upload data to this bucket, which of the following headers must be included in your request?\nx-amz-server-side-encryption-customer-key header only\nx-amz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers\nx-amz-server-side-encryption-customer-algorithm , x-amz-server-side-encryption-customer-key and x-amz-\nserver-side-encryption-customer-key-MD5 headers\n® x-amz-server-side-encryption , x-amz-server-side-encryption-customer-key and x-amz-server-side-\nencryption-customer-key-MD5 headers\nIncorrect\nServer-side encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you\nto set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it\nwrites to disks, and decryption, when you access your objects. Therefore, you don’t need to maintain any code to perform data encryption and\ndecryption. The only thing you do is manage the encryption keys you provide.\nEncrypted Object CED\nSSL\n| EG) | gy ErenetedObiect Sg",
        "options": [
          "C: ategory: CDA - Security",
          "a: nager assigned you a task of implementing server-side encryption with customer-provided encryption keys (SSE-C) to your S3 bucket, ki",
          "c: h will allow you to set your own encryption keys. Amazon S3 will manage both the encryption and decryption process using your key when",
          "a: ccess your objects, which will remove the burden of maintaining any code to perform data encryption and decryption.",
          "a: d data to this bucket, which of the following headers must be included in your request?",
          "a: mz-server-side-encryption-customer-key header only",
          "a: mz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers",
          "a: mz-server-side-encryption-customer-algorithm , x-amz-server-side-encryption-customer-key and x-amz-",
          "d: e-encryption-customer-key-MD5 headers",
          "a: mz-server-side-encryption , x-amz-server-side-encryption-customer-key and x-amz-server-side-",
          "c: ryption-customer-key-MD5 headers",
          "c: orrect",
          "d: e encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you",
          "c: ryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it",
          "d: isks, and decryption, when you access your objects. Therefore, you don’t need to maintain any code to perform data encryption and",
          "d: ecryption. The only thing you do is manage the encryption keys you provide.",
          "c: rypted Object CED",
          "d: Obiect Sg"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "48. QUESTION\nCategory: CDA - Security\no\nYour manager assigned you a task of implementing server-side encryption with customer-provided encryption keys (SSE-C) to your S3 bucket, ki\nwhich will allow you to set your own encryption keys. Amazon S3 will manage both the encryption and decryption process using your key when\nyou access your objects, which will remove the burden of maintaining any code to perform data encryption and decryption.\nTo properly upload data to this bucket, which of the following headers must be included in your request?\nx-amz-server-side-encryption-customer-key header only\nx-amz-server-side-encryption and x-amz-server-side-encryption-aws-kms-key-id headers\nx-amz-server-side-encryption-customer-algorithm , x-amz-server-side-encryption-customer-key and x-amz-\nserver-side-encryption-customer-key-MD5 headers\n® x-amz-server-side-encryption , x-amz-server-side-encryption-customer-key and x-amz-server-side-\nencryption-customer-key-MD5 headers\nIncorrect\nServer-side encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you\nto set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it\nwrites to disks, and decryption, when you access your objects. Therefore, you don’t need to maintain any code to perform data encryption and\ndecryption. The only thing you do is manage the encryption keys you provide.\nEncrypted Object CED\nSSL\n| EG) | gy ErenetedObiect Sg"
      },
      "tags": {
        "services": [
          "S3",
          "KMS",
          "ECR"
        ],
        "domains": [
          "Development with AWS Services",
          "Security"
        ],
        "keywords": [
          "security",
          "encryption"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 213442.png",
      "parsed": {
        "question": "is\nCategory: CDA - Development with AWS Services\nA company has a central data repository in Amazon S3 that needs to be accessed by developers belonging to different AWS accounts. The\nrequired IAM role has been created with the appropriate S3 permissions.\nGiven that the developers mostly interact with S3 via APIs, which API should the developers call to use the IAM role?\nAssumeRoleWithWebIdentity\nAssumeRoleWithSAML\nAssumeRole\nIncorrect\nA role specifies a set of permissions that you can use to access AWS resources. In that sense, it is similar to an IAM User. A principal (person or\napplication) assumes a role to receive temporary permissions to carry out required tasks and interact with AWS resources. The role can be in your\nown account or any other AWS account.\nTo assume a role, an application calls the AWS STS AssumeRole API operation and passes the ARN of the role to use. The operation creates a\nnew session with temporary credentials. This session has the same permissions as the identity-based policies for that role.\nAWS Account\n2. Developer launches an",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: company has a central data repository in Amazon S3 that needs to be accessed by developers belonging to different AWS accounts. The",
          "d: IAM role has been created with the appropriate S3 permissions.",
          "a: t the developers mostly interact with S3 via APIs, which API should the developers call to use the IAM role?",
          "A: ssumeRoleWithWebIdentity",
          "A: ssumeRoleWithSAML",
          "A: ssumeRole",
          "c: orrect",
          "A: role specifies a set of permissions that you can use to access AWS resources. In that sense, it is similar to an IAM User. A principal (person or",
          "a: pplication) assumes a role to receive temporary permissions to carry out required tasks and interact with AWS resources. The role can be in your",
          "a: ccount or any other AWS account.",
          "a: ssume a role, an application calls the AWS STS AssumeRole API operation and passes the ARN of the role to use. The operation creates a",
          "a: ry credentials. This session has the same permissions as the identity-based policies for that role.",
          "A: WS Account",
          "D: eveloper launches an"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "o\n50. QUESTION is\nCategory: CDA - Development with AWS Services\nA company has a central data repository in Amazon S3 that needs to be accessed by developers belonging to different AWS accounts. The\nrequired IAM role has been created with the appropriate S3 permissions.\nGiven that the developers mostly interact with S3 via APIs, which API should the developers call to use the IAM role?\nAssumeRoleWithWebIdentity\nAssumeRoleWithSAML\nAssumeRole\nIncorrect\nA role specifies a set of permissions that you can use to access AWS resources. In that sense, it is similar to an IAM User. A principal (person or\napplication) assumes a role to receive temporary permissions to carry out required tasks and interact with AWS resources. The role can be in your\nown account or any other AWS account.\nTo assume a role, an application calls the AWS STS AssumeRole API operation and passes the ARN of the role to use. The operation creates a\nnew session with temporary credentials. This session has the same permissions as the identity-based policies for that role.\nAWS Account\n2. Developer launches an"
      },
      "tags": {
        "services": [
          "S3",
          "IAM",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Deployment"
        ],
        "keywords": [
          "IAM role"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 213512.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services E\nA data analytics company has installed sensors to track the number of people that goes to the mall. The data sets are collected in real-time by\nan Amazon Kinesis Data Stream which has a consumer that is configured to process data every other day and store the results to S3. Your\nteam noticed that your S3 bucket is only receiving half of the data that is being sent to the Kinesis stream but after checking, you have verified\nthat the sensors are properly sending the data to Amazon Kinesis in real-time without any issues.\nWhich of the following is the MOST likely root cause of this issue?\nThe sensors are having intermittent connection issues.\n® The Amazon Kinesis Data Stream has too many open shards.\nThe Amazon Kinesis Data Stream automatically deletes duplicate data.\nBy default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream.\nIncorrect\nAmazon Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence\nof records meant to be written to and read from in real-time. Data records are therefore stored in shards in your stream temporarily. The time\nperiod from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores record for up\nto 24 hours by default.\nAWS\n5 LL (1 4 TT )\nJ = C1 SEED =",
        "options": [
          "C: ategory: CDA - Development with AWS Services E",
          "A: data analytics company has installed sensors to track the number of people that goes to the mall. The data sets are collected in real-time by",
          "a: n Amazon Kinesis Data Stream which has a consumer that is configured to process data every other day and store the results to S3. Your",
          "a: m noticed that your S3 bucket is only receiving half of the data that is being sent to the Kinesis stream but after checking, you have verified",
          "a: t the sensors are properly sending the data to Amazon Kinesis in real-time without any issues.",
          "c: h of the following is the MOST likely root cause of this issue?",
          "a: re having intermittent connection issues.",
          "A: mazon Kinesis Data Stream has too many open shards.",
          "A: mazon Kinesis Data Stream automatically deletes duplicate data.",
          "B: y default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream.",
          "c: orrect",
          "A: mazon Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence",
          "c: ords meant to be written to and read from in real-time. Data records are therefore stored in shards in your stream temporarily. The time",
          "d: from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores record for up",
          "b: y default.",
          "A: WS",
          "C: 1 SEED ="
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "51. QUESTION\nCategory: CDA - Development with AWS Services E\nA data analytics company has installed sensors to track the number of people that goes to the mall. The data sets are collected in real-time by\nan Amazon Kinesis Data Stream which has a consumer that is configured to process data every other day and store the results to S3. Your\nteam noticed that your S3 bucket is only receiving half of the data that is being sent to the Kinesis stream but after checking, you have verified\nthat the sensors are properly sending the data to Amazon Kinesis in real-time without any issues.\nWhich of the following is the MOST likely root cause of this issue?\nThe sensors are having intermittent connection issues.\n® The Amazon Kinesis Data Stream has too many open shards.\nThe Amazon Kinesis Data Stream automatically deletes duplicate data.\nBy default, the data records are only accessible for 24 hours from the time they are added to a Kinesis stream.\nIncorrect\nAmazon Kinesis Data Streams supports changes to the data record retention period of your stream. A Kinesis data stream is an ordered sequence\nof records meant to be written to and read from in real-time. Data records are therefore stored in shards in your stream temporarily. The time\nperiod from when a record is added to when it is no longer accessible is called the retention period. A Kinesis data stream stores record for up\nto 24 hours by default.\nAWS\n5 LL (1 4 TT )\nJ = C1 SEED ="
      },
      "tags": {
        "services": [
          "S3",
          "RDS",
          "Kinesis",
          "Config"
        ],
        "domains": [
          "Development with AWS Services"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 213820.png",
      "parsed": {
        "question": "E\nCategory: CDA - Development with AWS Services\nA serverless application is using API Gateway with a non-proxy Lambda Integration. A developer was tasked to expose a GET method on a new\n/getcourses resource to invoke the Lambda function, which will allow the consumers to fetch a list of online courses in JSON format. The\nconsumers must include a query string parameter named courseType in their request to get the data.\nWhat is the MOST efficient solution that the developer should do to accomplish this requirement?\nConfigure the method response of the resource.\nConfigure the integration request of the resource.\n® Configure the integration response of the resource.\nConfigure the method request of the resource.\nIncorrect\nIn Lambda non-proxy (or custom) integration, you can specify how the incoming request data is mapped to the integration request and how the\nresulting integration response data is mapped to the method response.\nFor an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API\nGateway serves as an integration endpoint to respond to a method request. The Lambda custom integration is a special case of the AWS\nintegration, where the integration endpoint corresponds to the function-invoking action of the Lambda service.\n/my-resource - GET - Method Execution 8\nJ a Method Request ® Intearation Request ®",
        "options": [
          "a: A 4 4 a 4",
          "C: ategory: CDA - Development with AWS Services",
          "A: serverless application is using API Gateway with a non-proxy Lambda Integration. A developer was tasked to expose a GET method on a new",
          "c: ourses resource to invoke the Lambda function, which will allow the consumers to fetch a list of online courses in JSON format. The",
          "c: onsumers must include a query string parameter named courseType in their request to get the data.",
          "a: t is the MOST efficient solution that the developer should do to accomplish this requirement?",
          "C: onfigure the method response of the resource.",
          "C: onfigure the integration request of the resource.",
          "C: onfigure the integration response of the resource.",
          "C: onfigure the method request of the resource.",
          "c: orrect",
          "a: mbda non-proxy (or custom) integration, you can specify how the incoming request data is mapped to the integration request and how the",
          "a: tion response data is mapped to the method response.",
          "a: n AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API",
          "a: teway serves as an integration endpoint to respond to a method request. The Lambda custom integration is a special case of the AWS",
          "a: tion, where the integration endpoint corresponds to the function-invoking action of the Lambda service.",
          "c: e - GET - Method Execution 8",
          "a: Method Request ® Intearation Request ®"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "aA 4 4 a 4\n54. QUESTION E\nCategory: CDA - Development with AWS Services\nA serverless application is using API Gateway with a non-proxy Lambda Integration. A developer was tasked to expose a GET method on a new\n/getcourses resource to invoke the Lambda function, which will allow the consumers to fetch a list of online courses in JSON format. The\nconsumers must include a query string parameter named courseType in their request to get the data.\nWhat is the MOST efficient solution that the developer should do to accomplish this requirement?\nConfigure the method response of the resource.\nConfigure the integration request of the resource.\n® Configure the integration response of the resource.\nConfigure the method request of the resource.\nIncorrect\nIn Lambda non-proxy (or custom) integration, you can specify how the incoming request data is mapped to the integration request and how the\nresulting integration response data is mapped to the method response.\nFor an AWS service action, you have the AWS integration of the non-proxy type only. API Gateway also supports the mock integration, where API\nGateway serves as an integration endpoint to respond to a method request. The Lambda custom integration is a special case of the AWS\nintegration, where the integration endpoint corresponds to the function-invoking action of the Lambda service.\n/my-resource - GET - Method Execution 8\nJ a Method Request ® Intearation Request ®"
      },
      "tags": {
        "services": [
          "Lambda",
          "API Gateway",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 213947.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA tech company has a real-time traffic monitoring system which uses Amazon Kinesis Data Stream to collect data and a group of EC2 B\ninstances that consume and process the data stream. Your development team is responsible for adjusting the number of shards in the data\nstream to adapt to changes in the rate of data flow.\nWhich of the following are correct regarding Kinesis resharding which your team should consider in managing the application? (Select TWO.)\nYou can increase the stream's capacity by splitting shards.\nYou can decrease the stream's capacity by merging shards.\nThe data records that are flowing to the parent shards will be lost when you reshard.\nYou have to split the cold shards to decrease the capacity of the stream.\nYou have to merge the hot shards to increase the capacity of the stream.\nIncorrect\nAmazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can\nconfigure hundreds of thousands of data producers to continuously put data into a Kinesis data stream. Data will be available within milliseconds\nto your Amazon Kinesis applications, and those applications will receive data records in the order they were generated.\nThe purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards\nto increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream.\nOne approach to resharding could be to split every shard in the stream—which would double the stream’s capacity. However, this might provide\nmore additional capacity than you actually need and therefore create unnecessary costs.\n1 Co",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: tech company has a real-time traffic monitoring system which uses Amazon Kinesis Data Stream to collect data and a group of EC2 B",
          "a: nces that consume and process the data stream. Your development team is responsible for adjusting the number of shards in the data",
          "a: m to adapt to changes in the rate of data flow.",
          "c: h of the following are correct regarding Kinesis resharding which your team should consider in managing the application? (Select TWO.)",
          "c: an increase the stream's capacity by splitting shards.",
          "c: an decrease the stream's capacity by merging shards.",
          "d: ata records that are flowing to the parent shards will be lost when you reshard.",
          "a: ve to split the cold shards to decrease the capacity of the stream.",
          "a: ve to merge the hot shards to increase the capacity of the stream.",
          "c: orrect",
          "A: mazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can",
          "c: onfigure hundreds of thousands of data producers to continuously put data into a Kinesis data stream. Data will be available within milliseconds",
          "A: mazon Kinesis applications, and those applications will receive data records in the order they were generated.",
          "a: rding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards",
          "c: rease the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream.",
          "a: pproach to resharding could be to split every shard in the stream—which would double the stream’s capacity. However, this might provide",
          "a: dditional capacity than you actually need and therefore create unnecessary costs.",
          "C: o"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "57. QUESTION\nCategory: CDA - Development with AWS Services\nA tech company has a real-time traffic monitoring system which uses Amazon Kinesis Data Stream to collect data and a group of EC2 B\ninstances that consume and process the data stream. Your development team is responsible for adjusting the number of shards in the data\nstream to adapt to changes in the rate of data flow.\nWhich of the following are correct regarding Kinesis resharding which your team should consider in managing the application? (Select TWO.)\nYou can increase the stream's capacity by splitting shards.\nYou can decrease the stream's capacity by merging shards.\nThe data records that are flowing to the parent shards will be lost when you reshard.\nYou have to split the cold shards to decrease the capacity of the stream.\nYou have to merge the hot shards to increase the capacity of the stream.\nIncorrect\nAmazon Kinesis Data Streams is a massively scalable, highly durable data ingestion and processing service optimized for streaming data. You can\nconfigure hundreds of thousands of data producers to continuously put data into a Kinesis data stream. Data will be available within milliseconds\nto your Amazon Kinesis applications, and those applications will receive data records in the order they were generated.\nThe purpose of resharding in Amazon Kinesis Data Streams is to enable your stream to adapt to changes in the rate of data flow. You split shards\nto increase the capacity (and cost) of your stream. You merge shards to reduce the cost (and capacity) of your stream.\nOne approach to resharding could be to split every shard in the stream—which would double the stream’s capacity. However, this might provide\nmore additional capacity than you actually need and therefore create unnecessary costs.\n1 Co"
      },
      "tags": {
        "services": [
          "EC2",
          "RDS",
          "Kinesis",
          "Config",
          "ECR"
        ],
        "domains": [],
        "keywords": [
          "monitoring"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 214013.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nAn application in your development account is running in an AWS Elastic Beanstalk environment which has an attached Amazon RDS database.\nYou noticed that if you terminate the environment, it also brings down the database which hinders you from performing seamless updates with\nblue-green deployments. This also poses a critical security risk if the company decides to deploy the application in production.\nIn this scenario, how can you decouple your database instance from your environment without having any data loss?\nUse the blue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment.\n@® Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with\nthe necessary information to connect to the Amazon RDS instance and delete the old environment.\nUse a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an\nRDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the\nnecessary information to connect to the Amazon RDS instance and delete the old environment.\nUse the blue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment.\nCreate an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with\nthe necessary information to connect to the Amazon RDS instance. Before terminating the old Elastic Beanstalk environment,\nremove its security group rule first before proceeding.\nUse a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an\nRDS DB snapshot of the database and then create a new Elastic Beanstalk environment with the necessary information to\nconnect to the Amazon RDS instance.\nIncorrect\nAWS Elastic Beanstalk provides support for running Amazon Relational Database Service (Amazon RDS) instances in your Elastic Beanstalk\nAnviranmant Thic wnrke Araat far dovalanment and factinA anviranmoante Hamauar i+ ien ideal far a mrad ictinm anviranment haratica it ioe tha",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: n application in your development account is running in an AWS Elastic Beanstalk environment which has an attached Amazon RDS database.",
          "c: ed that if you terminate the environment, it also brings down the database which hinders you from performing seamless updates with",
          "b: lue-green deployments. This also poses a critical security risk if the company decides to deploy the application in production.",
          "c: enario, how can you decouple your database instance from your environment without having any data loss?",
          "b: lue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment.",
          "C: reate an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with",
          "c: essary information to connect to the Amazon RDS instance and delete the old environment.",
          "a: Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an",
          "D: S DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the",
          "c: essary information to connect to the Amazon RDS instance and delete the old environment.",
          "b: lue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment.",
          "C: reate an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with",
          "c: essary information to connect to the Amazon RDS instance. Before terminating the old Elastic Beanstalk environment,",
          "c: urity group rule first before proceeding.",
          "a: Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an",
          "D: S DB snapshot of the database and then create a new Elastic Beanstalk environment with the necessary information to",
          "c: onnect to the Amazon RDS instance.",
          "c: orrect",
          "A: WS Elastic Beanstalk provides support for running Amazon Relational Database Service (Amazon RDS) instances in your Elastic Beanstalk",
          "A: nviranmant Thic wnrke Araat far dovalanment and factinA anviranmoante Hamauar i+ ien ideal far a mrad ictinm anviranment haratica it ioe tha"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "58. QUESTION\nCategory: CDA - Development with AWS Services\nAn application in your development account is running in an AWS Elastic Beanstalk environment which has an attached Amazon RDS database.\nYou noticed that if you terminate the environment, it also brings down the database which hinders you from performing seamless updates with\nblue-green deployments. This also poses a critical security risk if the company decides to deploy the application in production.\nIn this scenario, how can you decouple your database instance from your environment without having any data loss?\nUse the blue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment.\n@® Create an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with\nthe necessary information to connect to the Amazon RDS instance and delete the old environment.\nUse a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an\nRDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with the\nnecessary information to connect to the Amazon RDS instance and delete the old environment.\nUse the blue / green deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment.\nCreate an RDS DB snapshot of the database and enable deletion protection. Create a new Elastic Beanstalk environment with\nthe necessary information to connect to the Amazon RDS instance. Before terminating the old Elastic Beanstalk environment,\nremove its security group rule first before proceeding.\nUse a Canary deployment strategy to decouple the Amazon RDS instance from your Elastic Beanstalk environment. Create an\nRDS DB snapshot of the database and then create a new Elastic Beanstalk environment with the necessary information to\nconnect to the Amazon RDS instance.\nIncorrect\nAWS Elastic Beanstalk provides support for running Amazon Relational Database Service (Amazon RDS) instances in your Elastic Beanstalk\nAnviranmant Thic wnrke Araat far dovalanment and factinA anviranmoante Hamauar i+ ien ideal far a mrad ictinm anviranment haratica it ioe tha"
      },
      "tags": {
        "services": [
          "Elastic Beanstalk",
          "RDS"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": [
          "security",
          "deployment",
          "blue-green",
          "canary",
          "security group"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 214117.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nAn online magazine is deployed in AWS and uses an Application Load Balancer, an Auto Scaling group of EC2 instances, and an RDS MySQL\nDatabase. Some of the readers are complaining about the website's sluggish performance when loading the articles. Upon checking, there is a\nhigh number of read operations in the database, which affects the website's performance.\nWhich of the following actions should you take to resolve the issue with minimal code change?\nCreate an RDS Read Replica instance and configure the application to use this for read queries.\nUpgrade the EC2 instances to a higher instance type.\nLaunch a large ElastiCache Cluster as a database cache for RDS and apply the required code change.\n@® Set up a multi-AZ deployments configuration in RDS.\nIncorrect\nAmazon RDS Read Replicas provide enhanced performance and durability for the database (DB) instances. This feature makes it easy to\nelastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more\nreplicas of a given source DB Instance and serve high-volume application read.\nClient Production Read\nApplications Database Replica\n1. Create Read\nReplica\nM] S]\nH 2. Upgrade Read\n1 - Replica and catch",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: n online magazine is deployed in AWS and uses an Application Load Balancer, an Auto Scaling group of EC2 instances, and an RDS MySQL",
          "D: atabase. Some of the readers are complaining about the website's sluggish performance when loading the articles. Upon checking, there is a",
          "b: er of read operations in the database, which affects the website's performance.",
          "c: h of the following actions should you take to resolve the issue with minimal code change?",
          "C: reate an RDS Read Replica instance and configure the application to use this for read queries.",
          "a: de the EC2 instances to a higher instance type.",
          "a: unch a large ElastiCache Cluster as a database cache for RDS and apply the required code change.",
          "a: multi-AZ deployments configuration in RDS.",
          "c: orrect",
          "A: mazon RDS Read Replicas provide enhanced performance and durability for the database (DB) instances. This feature makes it easy to",
          "a: stically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more",
          "c: as of a given source DB Instance and serve high-volume application read.",
          "C: lient Production Read",
          "A: pplications Database Replica",
          "C: reate Read",
          "c: a",
          "a: de Read",
          "c: a and catch"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "59. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nAn online magazine is deployed in AWS and uses an Application Load Balancer, an Auto Scaling group of EC2 instances, and an RDS MySQL\nDatabase. Some of the readers are complaining about the website's sluggish performance when loading the articles. Upon checking, there is a\nhigh number of read operations in the database, which affects the website's performance.\nWhich of the following actions should you take to resolve the issue with minimal code change?\nCreate an RDS Read Replica instance and configure the application to use this for read queries.\nUpgrade the EC2 instances to a higher instance type.\nLaunch a large ElastiCache Cluster as a database cache for RDS and apply the required code change.\n@® Set up a multi-AZ deployments configuration in RDS.\nIncorrect\nAmazon RDS Read Replicas provide enhanced performance and durability for the database (DB) instances. This feature makes it easy to\nelastically scale out beyond the capacity constraints of a single DB instance for read-heavy database workloads. You can create one or more\nreplicas of a given source DB Instance and serve high-volume application read.\nClient Production Read\nApplications Database Replica\n1. Create Read\nReplica\nM] S]\nH 2. Upgrade Read\n1 - Replica and catch"
      },
      "tags": {
        "services": [
          "EC2",
          "EBS",
          "RDS",
          "ElastiCache",
          "Config"
        ],
        "domains": [
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "scaling",
          "performance",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 214209.png",
      "parsed": {
        "question": "®\nCategory: CDA - Deployment Fr\nA company is deploying the package of its Lambda function, which is compressed as a ZIP file, to AWS. However, they are getting an error in\nthe deployment process because the package is too large. The manager instructed the developer to keep the deployment package small to\nmake the development process much easier and more modularized. This should also help prevent errors that may occur when dependencies\nare installed and packaged with the function code.\nWhich of the following options is the MOST suitable solution that the developer should implement?\nCompress the deployment package as TAR file instead.\nUpload the deployment package to S3.\n@® Zip the deployment package again to further compress the zip file.\nUpload the other dependencies of your function as a separate Lambda Layer instead.\nIncorrect\nYou can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries,\na custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment\npackage.\nLayers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and\npackage dependencies with your function code. For Node.js, Python, and Ruby functions, you can develop your function code in the Lambda",
        "options": [
          "C: ategory: CDA - Deployment Fr",
          "A: company is deploying the package of its Lambda function, which is compressed as a ZIP file, to AWS. However, they are getting an error in",
          "d: eployment process because the package is too large. The manager instructed the developer to keep the deployment package small to",
          "a: ke the development process much easier and more modularized. This should also help prevent errors that may occur when dependencies",
          "a: re installed and packaged with the function code.",
          "c: h of the following options is the MOST suitable solution that the developer should implement?",
          "C: ompress the deployment package as TAR file instead.",
          "a: d the deployment package to S3.",
          "d: eployment package again to further compress the zip file.",
          "a: d the other dependencies of your function as a separate Lambda Layer instead.",
          "c: orrect",
          "c: an configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries,",
          "a: custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment",
          "a: ckage.",
          "a: yers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and",
          "a: ckage dependencies with your function code. For Node.js, Python, and Ruby functions, you can develop your function code in the Lambda"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "60. QUESTION\n®\nCategory: CDA - Deployment Fr\nA company is deploying the package of its Lambda function, which is compressed as a ZIP file, to AWS. However, they are getting an error in\nthe deployment process because the package is too large. The manager instructed the developer to keep the deployment package small to\nmake the development process much easier and more modularized. This should also help prevent errors that may occur when dependencies\nare installed and packaged with the function code.\nWhich of the following options is the MOST suitable solution that the developer should implement?\nCompress the deployment package as TAR file instead.\nUpload the deployment package to S3.\n@® Zip the deployment package again to further compress the zip file.\nUpload the other dependencies of your function as a separate Lambda Layer instead.\nIncorrect\nYou can configure your Lambda function to pull in additional code and content in the form of layers. A layer is a ZIP archive that contains libraries,\na custom runtime, or other dependencies. With layers, you can use libraries in your function without needing to include them in your deployment\npackage.\nLayers let you keep your deployment package small, which makes development easier. You can avoid errors that can occur when you install and\npackage dependencies with your function code. For Node.js, Python, and Ruby functions, you can develop your function code in the Lambda"
      },
      "tags": {
        "services": [
          "Lambda",
          "S3",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-16 214252.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer is creating a real-time auction app for second-hand cars using Kinesis Data Streams to ingest bids. The auction rules are as E\nfollows:\no A bid must be processed only once\no An EC2 instance consumer must process bids in the same order they were received.\nWhich solution will meet the requirement?\nEmbed a unique ID in each bid record. Use Kinesis pPutRecord API to write bids. Assign a timestamp-based value for the\nSequenceNumberForOrdering parameter.\nEmbed a unique ID in each bid record. Use Kinesis PutRecords API to write bids. Assign a timestamp-based value for the\npPartitionKey parameter.\nReplace the stream with an SQS FIFO queue and use the SendMessage API to write bids. Provide a unique id in the\nMessageDeduplicationTd parameter for each bid request.\n® Replace the stream with an SQS FIFO queue and use the sendMessageBatch API to write bids. Provide a unique id in the\nMessageDeduplicationTd parameter for each bid request.\nIncorrect\nThere are two API calls available for writing records to a Kinesis Data Stream: PutRecord or PutRecords . PutRecord writes a single\nrecord to the stream, while PutRecords writes multiple records to the stream in a batch. Kinesis Data Streams attempts to process all records\nineach PutRecords request. A single record failure does not stop the processing of subsequent records. As a result, PutRecords doesn’t",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer is creating a real-time auction app for second-hand cars using Kinesis Data Streams to ingest bids. The auction rules are as E",
          "A: bid must be processed only once",
          "A: n EC2 instance consumer must process bids in the same order they were received.",
          "c: h solution will meet the requirement?",
          "b: ed a unique ID in each bid record. Use Kinesis pPutRecord API to write bids. Assign a timestamp-based value for the",
          "c: eNumberForOrdering parameter.",
          "b: ed a unique ID in each bid record. Use Kinesis PutRecords API to write bids. Assign a timestamp-based value for the",
          "a: rtitionKey parameter.",
          "a: ce the stream with an SQS FIFO queue and use the SendMessage API to write bids. Provide a unique id in the",
          "a: geDeduplicationTd parameter for each bid request.",
          "a: ce the stream with an SQS FIFO queue and use the sendMessageBatch API to write bids. Provide a unique id in the",
          "a: geDeduplicationTd parameter for each bid request.",
          "c: orrect",
          "a: re two API calls available for writing records to a Kinesis Data Stream: PutRecord or PutRecords . PutRecord writes a single",
          "c: ord to the stream, while PutRecords writes multiple records to the stream in a batch. Kinesis Data Streams attempts to process all records",
          "a: ch PutRecords request. A single record failure does not stop the processing of subsequent records. As a result, PutRecords doesn’t"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "61. QUESTION\nCategory: CDA - Development with AWS Services\nA developer is creating a real-time auction app for second-hand cars using Kinesis Data Streams to ingest bids. The auction rules are as E\nfollows:\no A bid must be processed only once\no An EC2 instance consumer must process bids in the same order they were received.\nWhich solution will meet the requirement?\nEmbed a unique ID in each bid record. Use Kinesis pPutRecord API to write bids. Assign a timestamp-based value for the\nSequenceNumberForOrdering parameter.\nEmbed a unique ID in each bid record. Use Kinesis PutRecords API to write bids. Assign a timestamp-based value for the\npPartitionKey parameter.\nReplace the stream with an SQS FIFO queue and use the SendMessage API to write bids. Provide a unique id in the\nMessageDeduplicationTd parameter for each bid request.\n® Replace the stream with an SQS FIFO queue and use the sendMessageBatch API to write bids. Provide a unique id in the\nMessageDeduplicationTd parameter for each bid request.\nIncorrect\nThere are two API calls available for writing records to a Kinesis Data Stream: PutRecord or PutRecords . PutRecord writes a single\nrecord to the stream, while PutRecords writes multiple records to the stream in a batch. Kinesis Data Streams attempts to process all records\nineach PutRecords request. A single record failure does not stop the processing of subsequent records. As a result, PutRecords doesn’t"
      },
      "tags": {
        "services": [
          "EC2",
          "RDS",
          "SQS",
          "Kinesis",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment"
        ],
        "keywords": [
          "queue"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 085225.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nYou are using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The\nLambda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second.\nWhich of the following statements are TRUE regarding this scenario?\nThere will be at most 100 Lambda function invocations running concurrently.\n® The Lambda function has 500 concurrent executions.\nThe Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the\nLambda function.\nThe Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards.\nIncorrect\nYou can use an AWS Lambda function to process records in an Amazon Kinesis data stream. With Kinesis, you can collect data from many\nsources and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers. Lambda\nreads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in\nbatches and invokes your function to process records from the batch.",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "a: re using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The",
          "a: mbda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second.",
          "c: h of the following statements are TRUE regarding this scenario?",
          "b: e at most 100 Lambda function invocations running concurrently.",
          "a: mbda function has 500 concurrent executions.",
          "a: rds must be merged to increase the data capacity of the stream as well as the concurrency execution of the",
          "a: mbda function.",
          "a: mbda function will throttle the incoming requests due to the excessive number of Kinesis shards.",
          "c: orrect",
          "c: an use an AWS Lambda function to process records in an Amazon Kinesis data stream. With Kinesis, you can collect data from many",
          "c: es and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers. Lambda",
          "a: ds records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in",
          "b: atches and invokes your function to process records from the batch."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "| —— TR | — i\n1. QUESTION\nCategory: CDA - Development with AWS Services\nYou are using an AWS Lambda function to process records in an Amazon Kinesis Data Streams stream which has 100 active shards. The\nLambda function takes an average of 10 seconds to process the data and the stream is receiving 50 new items per second.\nWhich of the following statements are TRUE regarding this scenario?\nThere will be at most 100 Lambda function invocations running concurrently.\n® The Lambda function has 500 concurrent executions.\nThe Kinesis shards must be merged to increase the data capacity of the stream as well as the concurrency execution of the\nLambda function.\nThe Lambda function will throttle the incoming requests due to the excessive number of Kinesis shards.\nIncorrect\nYou can use an AWS Lambda function to process records in an Amazon Kinesis data stream. With Kinesis, you can collect data from many\nsources and process them with multiple consumers. Lambda supports standard data stream iterators and HTTP/2 stream consumers. Lambda\nreads records from the data stream and invokes your function synchronously with an event that contains stream records. Lambda reads records in\nbatches and invokes your function to process records from the batch."
      },
      "tags": {
        "services": [
          "Lambda",
          "RDS",
          "Kinesis"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "synchronous"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 085336.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data [\ninto your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams\nand stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle\nfailover and adequately process the amount of data coming in and out of the stream.\nWhich of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above\nrequirement in the most cost-effective and highly available way?\n1 shard : 6 instances\n6 shards : 1 instance\n® 4 shards: 8 instances\n4 shards : 2 instances\nIncorrect\nA stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the\ncapacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that\nshard. It also tracks the shards in the stream using an Amazon DynamoDB table.\nInstance\nEnhanced fan-out pipe\nShard A",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data [",
          "a: ming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams",
          "a: nd stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle",
          "a: ilover and adequately process the amount of data coming in and out of the stream.",
          "c: h of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above",
          "c: ost-effective and highly available way?",
          "a: rd : 6 instances",
          "a: rds : 1 instance",
          "a: rds: 8 instances",
          "a: rds : 2 instances",
          "c: orrect",
          "A: stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the",
          "c: apacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that",
          "a: rd. It also tracks the shards in the stream using an Amazon DynamoDB table.",
          "a: nce",
          "a: nced fan-out pipe",
          "a: rd A"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "3. QUESTION\nCategory: CDA - Development with AWS Services\nA developer will be building a game data feed application which will continuously collect data about player-game interactions and feed the data [\ninto your gaming platform. The application uses the Kinesis Client Library to process the data stream from the Amazon Kinesis Data Streams\nand stores the data to Amazon DynamoDB. It is required that the system should have enough shards and EC2 instances in order to handle\nfailover and adequately process the amount of data coming in and out of the stream.\nWhich of the following ratio of the number of Kinesis shards to EC2 worker instances should the developer implement to achieve the above\nrequirement in the most cost-effective and highly available way?\n1 shard : 6 instances\n6 shards : 1 instance\n® 4 shards: 8 instances\n4 shards : 2 instances\nIncorrect\nA stream is composed of one or more shards, each of which provides a fixed unit of capacity. The total capacity of the stream is the sum of the\ncapacities of its shards. The Kinesis Client Library (KCL) ensures that for every shard there is a record processor running and processing that\nshard. It also tracks the shards in the stream using an Amazon DynamoDB table.\nInstance\nEnhanced fan-out pipe\nShard A"
      },
      "tags": {
        "services": [
          "EC2",
          "RDS",
          "DynamoDB",
          "Kinesis"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 085426.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services i\nAn application is hosted in Elastic Beanstalk with an ElastiCache cluster that acts as a database cache layer for accessing its data in\nDynamoDB. It is currently configured to write the data to the cache only if there is a cache miss, which causes the data in the cache to become\nstale. A developer is instructed to ensure that the data in the cache is always current and to minimize wasted space in the cluster by\nautomatically deleting the data that are never read.\nWhat is the BEST way to implement this to satisfy the given requirement?\nImplement a Write Through caching strategy in the application and enable TTL in Elasticache.\nUse a Lazy Loading caching strategy.\n® Implement Lazy Loading in the application in conjunction with the Write Through caching strategy.\nUse a Write Through caching strategy.\nIncorrect\nThe write-through strategy adds data or updates data in the cache whenever data is written to the database. With this strategy, the data in the\ncache is never stale since the data in the cache is updated every time it is written to the database. This is why the data in the cache is always\ncurrent.\nOne of its disadvantages is that, since most data are never read in the cluster, you are actually wasting resources and disk space. This issue can\nbe rectified by simply adding TTL to minimize wasted space.\n[5]\n1 = 2\na \\Alritasa Theatr CAAA — rite — i",
        "options": [
          "C: ategory: CDA - Development with AWS Services i",
          "A: n application is hosted in Elastic Beanstalk with an ElastiCache cluster that acts as a database cache layer for accessing its data in",
          "D: ynamoDB. It is currently configured to write the data to the cache only if there is a cache miss, which causes the data in the cache to become",
          "a: le. A developer is instructed to ensure that the data in the cache is always current and to minimize wasted space in the cluster by",
          "a: utomatically deleting the data that are never read.",
          "a: t is the BEST way to implement this to satisfy the given requirement?",
          "a: Write Through caching strategy in the application and enable TTL in Elasticache.",
          "a: Lazy Loading caching strategy.",
          "a: zy Loading in the application in conjunction with the Write Through caching strategy.",
          "a: Write Through caching strategy.",
          "c: orrect",
          "a: tegy adds data or updates data in the cache whenever data is written to the database. With this strategy, the data in the",
          "c: ache is never stale since the data in the cache is updated every time it is written to the database. This is why the data in the cache is always",
          "c: urrent.",
          "d: isadvantages is that, since most data are never read in the cluster, you are actually wasting resources and disk space. This issue can",
          "b: e rectified by simply adding TTL to minimize wasted space.",
          "a: \\Alritasa Theatr CAAA — rite — i"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "4. QUESTION\nCategory: CDA - Development with AWS Services i\nAn application is hosted in Elastic Beanstalk with an ElastiCache cluster that acts as a database cache layer for accessing its data in\nDynamoDB. It is currently configured to write the data to the cache only if there is a cache miss, which causes the data in the cache to become\nstale. A developer is instructed to ensure that the data in the cache is always current and to minimize wasted space in the cluster by\nautomatically deleting the data that are never read.\nWhat is the BEST way to implement this to satisfy the given requirement?\nImplement a Write Through caching strategy in the application and enable TTL in Elasticache.\nUse a Lazy Loading caching strategy.\n® Implement Lazy Loading in the application in conjunction with the Write Through caching strategy.\nUse a Write Through caching strategy.\nIncorrect\nThe write-through strategy adds data or updates data in the cache whenever data is written to the database. With this strategy, the data in the\ncache is never stale since the data in the cache is updated every time it is written to the database. This is why the data in the cache is always\ncurrent.\nOne of its disadvantages is that, since most data are never read in the cluster, you are actually wasting resources and disk space. This issue can\nbe rectified by simply adding TTL to minimize wasted space.\n[5]\n1 = 2\na \\Alritasa Theatr CAAA — rite — i"
      },
      "tags": {
        "services": [
          "Elastic Beanstalk",
          "DynamoDB",
          "ElastiCache",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "caching"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 085533.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their\non-premises data center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to\ncreate a new API and populate it with the resources and methods from their Swagger definition.\nWhich of the following is the EASIEST way to accomplish this task?\nUse AWS SAM to migrate and deploy the company's web services to API Gateway.\n® Create models and templates for request and response mappings based on the company's API definitions.\nImport their Swagger or OpenAPI definitions to API Gateway using the AWS Console.\nUse CodeDeploy to migrate and deploy the company's web services to API Gateway.\nIncorrect\nYou can use the API Gateway Import API feature to import a REST API from an external definition file into API Gateway. Currently, the Import API\nfeature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. You can update an API by overwriting it with a new definition or merge a\ndefinition with an existing API. You specify the options using a mode query parameter in the request URL.\nYou can paste a Swagger API definition in the AWS Console to create a new API and populate it with the resources and methods from your\nSwagger or OpenAPI definition, just as shown below:\n]",
        "options": [
          "A: nswered [* For Review",
          "D: O |",
          "C: ategory: CDA - Development with AWS Services",
          "A: company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their",
          "d: ata center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to",
          "c: reate a new API and populate it with the resources and methods from their Swagger definition.",
          "c: h of the following is the EASIEST way to accomplish this task?",
          "A: WS SAM to migrate and deploy the company's web services to API Gateway.",
          "C: reate models and templates for request and response mappings based on the company's API definitions.",
          "a: gger or OpenAPI definitions to API Gateway using the AWS Console.",
          "C: odeDeploy to migrate and deploy the company's web services to API Gateway.",
          "c: orrect",
          "c: an use the API Gateway Import API feature to import a REST API from an external definition file into API Gateway. Currently, the Import API",
          "a: ture supports OpenAPI v2.0 and OpenAPI v3.0 definition files. You can update an API by overwriting it with a new definition or merge a",
          "d: efinition with an existing API. You specify the options using a mode query parameter in the request URL.",
          "c: an paste a Swagger API definition in the AWS Console to create a new API and populate it with the resources and methods from your",
          "a: gger or OpenAPI definition, just as shown below:"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "« Answered [* For Review\nDO |\n6. QUESTION\nCategory: CDA - Development with AWS Services\nA company is using OpenAPI, which is also known as Swagger, for the API specifications of their REST web services that are hosted on their\non-premises data center. They want to migrate their system to AWS using Lambda and API Gateway. In line with this, you are instructed to\ncreate a new API and populate it with the resources and methods from their Swagger definition.\nWhich of the following is the EASIEST way to accomplish this task?\nUse AWS SAM to migrate and deploy the company's web services to API Gateway.\n® Create models and templates for request and response mappings based on the company's API definitions.\nImport their Swagger or OpenAPI definitions to API Gateway using the AWS Console.\nUse CodeDeploy to migrate and deploy the company's web services to API Gateway.\nIncorrect\nYou can use the API Gateway Import API feature to import a REST API from an external definition file into API Gateway. Currently, the Import API\nfeature supports OpenAPI v2.0 and OpenAPI v3.0 definition files. You can update an API by overwriting it with a new definition or merge a\ndefinition with an existing API. You specify the options using a mode query parameter in the request URL.\nYou can paste a Swagger API definition in the AWS Console to create a new API and populate it with the resources and methods from your\nSwagger or OpenAPI definition, just as shown below:\n]"
      },
      "tags": {
        "services": [
          "Lambda",
          "API Gateway",
          "CodeDeploy",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 085606.png",
      "parsed": {
        "question": "Category: CDA - Deployment B\nThe current application deployment process of a company is tedious and is prone to errors. They asked a developer to set up CodeDeploy as\ntheir deployment service, which can automate their application deployments on their hybrid cloud architecture.\nWhich of the following deployment types does CodeDeploy support? (Select TWO.)\nIn-place deployments to AWS Lambda.\nRolling deployments to ECS.\nIn-place deployments to on-premises servers\nBlue/green deployments to ECS.\nBlue/green deployments to on-premises servers.\nIncorrect\nCodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless\nLambda functions, or Amazon ECS services.\nCodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket\nrepositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can\nuse CodeDeploy.\n- AWS CodePipeline\nmmm ON\n]",
        "options": [
          "C: ategory: CDA - Deployment B",
          "c: urrent application deployment process of a company is tedious and is prone to errors. They asked a developer to set up CodeDeploy as",
          "d: eployment service, which can automate their application deployments on their hybrid cloud architecture.",
          "c: h of the following deployment types does CodeDeploy support? (Select TWO.)",
          "a: ce deployments to AWS Lambda.",
          "d: eployments to ECS.",
          "a: ce deployments to on-premises servers",
          "B: lue/green deployments to ECS.",
          "B: lue/green deployments to on-premises servers.",
          "c: orrect",
          "C: odeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless",
          "a: mbda functions, or Amazon ECS services.",
          "C: odeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket",
          "C: odeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can",
          "C: odeDeploy.",
          "A: WS CodePipeline"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "7. QUESTION\nCategory: CDA - Deployment B\nThe current application deployment process of a company is tedious and is prone to errors. They asked a developer to set up CodeDeploy as\ntheir deployment service, which can automate their application deployments on their hybrid cloud architecture.\nWhich of the following deployment types does CodeDeploy support? (Select TWO.)\nIn-place deployments to AWS Lambda.\nRolling deployments to ECS.\nIn-place deployments to on-premises servers\nBlue/green deployments to ECS.\nBlue/green deployments to on-premises servers.\nIncorrect\nCodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless\nLambda functions, or Amazon ECS services.\nCodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket\nrepositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can\nuse CodeDeploy.\n- AWS CodePipeline\nmmm ON\n]"
      },
      "tags": {
        "services": [
          "EC2",
          "Lambda",
          "ECS",
          "S3",
          "CodeDeploy",
          "CodePipeline"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 085720.png",
      "parsed": {
        "question": "®\nCategory: CDA - Troubleshooting and Optimization Ei\nA company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront\ndistribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by\nvalidating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of\nlogin attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks.\nWhich solution would reduce the load on the Fargate tasks in the most operationally efficient manner?\nCreate a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.\nCreate a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.\nCreate a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.\n® Enable auto-scaling on the Fargate tasks.\nIncorrect\nCloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is\ndesigned for operations that can be processed with low latency at the edge locations of AWS, such as:\n— Cache key normalization — You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an\noptimal cache key, which can improve your cache hit ratio.\n— Header manipulation - You can insert, modify, or delete HTTP headers in the request or response. For example, you can adda True-\nClient-IP header to every request.\n- Status code modification and body generation - You can evaluate headers and respond back to viewers with customized content.\nn",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization Ei",
          "A: company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront",
          "d: istribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by",
          "a: lidating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of",
          "a: ttempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks.",
          "c: h solution would reduce the load on the Fargate tasks in the most operationally efficient manner?",
          "C: reate a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.",
          "C: reate a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.",
          "C: reate a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.",
          "a: ble auto-scaling on the Fargate tasks.",
          "c: orrect",
          "C: loudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is",
          "d: esigned for operations that can be processed with low latency at the edge locations of AWS, such as:",
          "C: ache key normalization — You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an",
          "a: l cache key, which can improve your cache hit ratio.",
          "a: der manipulation - You can insert, modify, or delete HTTP headers in the request or response. For example, you can adda True-",
          "C: lient-IP header to every request.",
          "a: tus code modification and body generation - You can evaluate headers and respond back to viewers with customized content."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "10. QUESTION\n®\nCategory: CDA - Troubleshooting and Optimization Ei\nA company has a latency-sensitive service running on AWS Fargate, which is fronted by an Application Load Balancer (ALB). A CloudFront\ndistribution uses the ALB as its origin and presents a custom domain for clients to access the service. The service authenticates requests by\nvalidating the JSON Web Token (JWT) obtained from the Authorization header sent by clients. Lately, there has been a significant influx of\nlogin attempts from unauthenticated users, which increases the CPU utilization of the Fargate tasks.\nWhich solution would reduce the load on the Fargate tasks in the most operationally efficient manner?\nCreate a Lambda function that performs JWT validation. Configure the ALB to route login requests to the Lambda function.\nCreate a CloudFront function for JWT validation. Attach it to the Viewer Request event of the CloudFront distribution.\nCreate a Lambda@Edge function for JWT validation. Attach it to the Origin Response event of the CloudFront distribution.\n® Enable auto-scaling on the Fargate tasks.\nIncorrect\nCloudFront Functions allows you to write lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations. This feature is\ndesigned for operations that can be processed with low latency at the edge locations of AWS, such as:\n— Cache key normalization — You can transform HTTP request attributes (headers, query strings, cookies, even the URL path) to create an\noptimal cache key, which can improve your cache hit ratio.\n— Header manipulation - You can insert, modify, or delete HTTP headers in the request or response. For example, you can adda True-\nClient-IP header to every request.\n- Status code modification and body generation - You can evaluate headers and respond back to viewers with customized content.\nn"
      },
      "tags": {
        "services": [
          "Lambda",
          "Fargate",
          "CloudFront",
          "ALB",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "scaling",
          "authorization"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 085803.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA leading technology company is building a serverless application in AWS using the C++ programming language. The application will use\nDynamoDB as its data store, Lambda as its compute service, and API Gateway as its API Proxy. You are tasked to handle the deployment of the\ncompute resources to AWS.\nWhich of the following steps should you implement to properly deploy the serverless application?\nCreate a Lambda function with the C++ code and directly upload it to AWS.\nUpload the deployment package to S3 and then use CloudFormation to deploy Lambda function with a reference to the S3\nURL of the package.\nCreate a new layer which contains the Custom Runtime for C++ and then launch a Lambda function which uses that runtime.\n® Use AWS Serverless Application Model (AWS SAM) to deploy the Lambda function.\nIncorrect\nYou can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function's handler method\nwhen the function is invoked. You can include a runtime in your function's deployment package in the form of an executable file\nnamed bootstrap .\nA runtime is responsible for running the function's setup code, reading the handler name from an environment variable, and reading invocation\nevents from the Lambda runtime API. The runtime passes the event data to the function handler, and posts the response from the handler back to\nLambda.\nYour custom runtime runs in the standard Lambda execution environment. It can be a shell script, a script in a language that’s included in Amazon\nLinux, or a binary executable file that's compiled in Amazon Linux.",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: leading technology company is building a serverless application in AWS using the C++ programming language. The application will use",
          "D: ynamoDB as its data store, Lambda as its compute service, and API Gateway as its API Proxy. You are tasked to handle the deployment of the",
          "c: ompute resources to AWS.",
          "c: h of the following steps should you implement to properly deploy the serverless application?",
          "C: reate a Lambda function with the C++ code and directly upload it to AWS.",
          "a: d the deployment package to S3 and then use CloudFormation to deploy Lambda function with a reference to the S3",
          "a: ckage.",
          "C: reate a new layer which contains the Custom Runtime for C++ and then launch a Lambda function which uses that runtime.",
          "A: WS Serverless Application Model (AWS SAM) to deploy the Lambda function.",
          "c: orrect",
          "c: an implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function's handler method",
          "c: tion is invoked. You can include a runtime in your function's deployment package in the form of an executable file",
          "a: med bootstrap .",
          "A: runtime is responsible for running the function's setup code, reading the handler name from an environment variable, and reading invocation",
          "a: mbda runtime API. The runtime passes the event data to the function handler, and posts the response from the handler back to",
          "a: mbda.",
          "c: ustom runtime runs in the standard Lambda execution environment. It can be a shell script, a script in a language that’s included in Amazon",
          "a: binary executable file that's compiled in Amazon Linux."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "11. QUESTION\nCategory: CDA - Development with AWS Services\nA leading technology company is building a serverless application in AWS using the C++ programming language. The application will use\nDynamoDB as its data store, Lambda as its compute service, and API Gateway as its API Proxy. You are tasked to handle the deployment of the\ncompute resources to AWS.\nWhich of the following steps should you implement to properly deploy the serverless application?\nCreate a Lambda function with the C++ code and directly upload it to AWS.\nUpload the deployment package to S3 and then use CloudFormation to deploy Lambda function with a reference to the S3\nURL of the package.\nCreate a new layer which contains the Custom Runtime for C++ and then launch a Lambda function which uses that runtime.\n® Use AWS Serverless Application Model (AWS SAM) to deploy the Lambda function.\nIncorrect\nYou can implement an AWS Lambda runtime in any programming language. A runtime is a program that runs a Lambda function's handler method\nwhen the function is invoked. You can include a runtime in your function's deployment package in the form of an executable file\nnamed bootstrap .\nA runtime is responsible for running the function's setup code, reading the handler name from an environment variable, and reading invocation\nevents from the Lambda runtime API. The runtime passes the event data to the function handler, and posts the response from the handler back to\nLambda.\nYour custom runtime runs in the standard Lambda execution environment. It can be a shell script, a script in a language that’s included in Amazon\nLinux, or a binary executable file that's compiled in Amazon Linux."
      },
      "tags": {
        "services": [
          "Lambda",
          "S3",
          "DynamoDB",
          "API Gateway",
          "CloudFormation",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 090030.png",
      "parsed": {
        "question": "n\nCategory: CDA - Deployment\nA developer is building an online game in AWS which will be using a NoSQL database with DynamoDB. Each player data has an average size of\n3.5 KB and it is expected that the game will send 150 eventually consistent read requests per second.\nHow may Read Capacity Units (RCU) should the developer provision to the table?\n75\n150\n300\nIncorrect\nOne read request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size.\nTransactional read requests require 2 read request units to perform one read for items up to 4 KB.\nIf you need to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units\nrequired depends on the item size, and whether you want an eventually consistent or strongly consistent read.\nCapacity calculator\nAvg. item size [4 | KB",
        "options": [
          "c: EDO",
          "C: ategory: CDA - Deployment",
          "A: developer is building an online game in AWS which will be using a NoSQL database with DynamoDB. Each player data has an average size of",
          "B: and it is expected that the game will send 150 eventually consistent read requests per second.",
          "a: y Read Capacity Units (RCU) should the developer provision to the table?",
          "c: orrect",
          "a: d request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size.",
          "a: nsactional read requests require 2 read request units to perform one read for items up to 4 KB.",
          "d: to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units",
          "d: depends on the item size, and whether you want an eventually consistent or strongly consistent read.",
          "C: apacity calculator",
          "A: vg. item size [4 | KB"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "cEDO\n12. QUESTION n\nCategory: CDA - Deployment\nA developer is building an online game in AWS which will be using a NoSQL database with DynamoDB. Each player data has an average size of\n3.5 KB and it is expected that the game will send 150 eventually consistent read requests per second.\nHow may Read Capacity Units (RCU) should the developer provision to the table?\n75\n150\n300\nIncorrect\nOne read request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size.\nTransactional read requests require 2 read request units to perform one read for items up to 4 KB.\nIf you need to read an item that is larger than 4 KB, DynamoDB needs additional read request units. The total number of read request units\nrequired depends on the item size, and whether you want an eventually consistent or strongly consistent read.\nCapacity calculator\nAvg. item size [4 | KB"
      },
      "tags": {
        "services": [
          "DynamoDB"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 090116.png",
      "parsed": {
        "question": "uired to handle 150 eventually consistent read requests with an average item size of 3.5 KB, you simply have to\ndo the following steps:\nStep #1 Get the Average Item Size by rounding up to 4 KB\n\n= 3.5 KB = 4 KB (rounded up)\nStep #2 Get the RCU per Item by dividing the Average Item Size by 8 KB\n\n=4KB/8KB\n\n=05\nStep #3 Multiply the RCU per item to the number of items to be written per second\n\n=150x 0.5\n\n= 75 eventually consistent read requests\nHence, the correct answer is 75.\n150 is incorrect because this is the value for strongly consistent read requests based on the given RCU. Take note that for Step #2, you have to\ndivide the Average Item Size by 8 KB and not by 4 KB, if you are calculating for eventual consistency.\n300 is incorrect because this is the value for transactional read requests based on the given RCU. Take note that for Step #2, you have to divide\nthe Average Item Size by 8 KB and not by 2 KB, if you are calculating for eventual consistency.\n600 is incorrect because this is the value for transactional write requests, which is irrelevant since only the RCU is provided in the scenario.\nReferences:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowltWorks.ReadWriteCapacityMode.html",
        "options": [
          "a: ding an item larger than 4 KB consumes more read capacity units. For example, a strongly consistent read of an item that is 8 KB (4 KB x 2)",
          "c: onsumes 2 read capacity units. An eventually consistent read on that same item consumes only 1 read capacity unit.",
          "a: ds are rounded up to the next 4 KB multiple. For example, reading a 3,500-byte item consumes the same throughput as reading a",
          "B: item. E",
          "b: er of RCU required to handle 150 eventually consistent read requests with an average item size of 3.5 KB, you simply have to",
          "d: o the following steps:",
          "A: verage Item Size by rounding up to 4 KB",
          "B: = 4 KB (rounded up)",
          "C: U per Item by dividing the Average Item Size by 8 KB",
          "B: /8KB",
          "C: U per item to the number of items to be written per second",
          "a: lly consistent read requests",
          "c: e, the correct answer is 75.",
          "c: orrect because this is the value for strongly consistent read requests based on the given RCU. Take note that for Step #2, you have to",
          "d: ivide the Average Item Size by 8 KB and not by 4 KB, if you are calculating for eventual consistency.",
          "c: orrect because this is the value for transactional read requests based on the given RCU. Take note that for Step #2, you have to divide",
          "A: verage Item Size by 8 KB and not by 2 KB, if you are calculating for eventual consistency.",
          "c: orrect because this is the value for transactional write requests, which is irrelevant since only the RCU is provided in the scenario.",
          "c: es:",
          "d: ocs.aws.amazon.com/amazondynamodb/latest/developerguide/HowltWorks.ReadWriteCapacityMode.html"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "Reading an item larger than 4 KB consumes more read capacity units. For example, a strongly consistent read of an item that is 8 KB (4 KB x 2)\nconsumes 2 read capacity units. An eventually consistent read on that same item consumes only 1 read capacity unit.\nItem sizes for reads are rounded up to the next 4 KB multiple. For example, reading a 3,500-byte item consumes the same throughput as reading a\n4 KB item. E\nTo get the number of RCU required to handle 150 eventually consistent read requests with an average item size of 3.5 KB, you simply have to\ndo the following steps:\nStep #1 Get the Average Item Size by rounding up to 4 KB\n\n= 3.5 KB = 4 KB (rounded up)\nStep #2 Get the RCU per Item by dividing the Average Item Size by 8 KB\n\n=4KB/8KB\n\n=05\nStep #3 Multiply the RCU per item to the number of items to be written per second\n\n=150x 0.5\n\n= 75 eventually consistent read requests\nHence, the correct answer is 75.\n150 is incorrect because this is the value for strongly consistent read requests based on the given RCU. Take note that for Step #2, you have to\ndivide the Average Item Size by 8 KB and not by 4 KB, if you are calculating for eventual consistency.\n300 is incorrect because this is the value for transactional read requests based on the given RCU. Take note that for Step #2, you have to divide\nthe Average Item Size by 8 KB and not by 2 KB, if you are calculating for eventual consistency.\n600 is incorrect because this is the value for transactional write requests, which is irrelevant since only the RCU is provided in the scenario.\nReferences:\nhttps://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowltWorks.ReadWriteCapacityMode.html"
      },
      "tags": {
        "services": [
          "DynamoDB",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    }
  ],
  "summary": {
    "totalQuestions": 44,
    "correctCount": 44,
    "incorrectCount": 0,
    "accuracy": 100,
    "serviceBreakdown": {
      "EC2": {
        "total": 16,
        "correct": 16,
        "incorrect": 0,
        "accuracy": 100
      },
      "Elastic Beanstalk": {
        "total": 5,
        "correct": 5,
        "incorrect": 0,
        "accuracy": 100
      },
      "ECS": {
        "total": 8,
        "correct": 8,
        "incorrect": 0,
        "accuracy": 100
      },
      "Fargate": {
        "total": 4,
        "correct": 4,
        "incorrect": 0,
        "accuracy": 100
      },
      "Lambda": {
        "total": 20,
        "correct": 20,
        "incorrect": 0,
        "accuracy": 100
      },
      "S3": {
        "total": 9,
        "correct": 9,
        "incorrect": 0,
        "accuracy": 100
      },
      "Step Functions": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "Config": {
        "total": 19,
        "correct": 19,
        "incorrect": 0,
        "accuracy": 100
      },
      "SAM": {
        "total": 9,
        "correct": 9,
        "incorrect": 0,
        "accuracy": 100
      },
      "EKS": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "API Gateway": {
        "total": 6,
        "correct": 6,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudWatch": {
        "total": 5,
        "correct": 5,
        "incorrect": 0,
        "accuracy": 100
      },
      "Route 53": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "ELB": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "X-Ray": {
        "total": 3,
        "correct": 3,
        "incorrect": 0,
        "accuracy": 100
      },
      "CodeDeploy": {
        "total": 5,
        "correct": 5,
        "incorrect": 0,
        "accuracy": 100
      },
      "DynamoDB": {
        "total": 10,
        "correct": 10,
        "incorrect": 0,
        "accuracy": 100
      },
      "EBS": {
        "total": 4,
        "correct": 4,
        "incorrect": 0,
        "accuracy": 100
      },
      "RDS": {
        "total": 12,
        "correct": 12,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudFront": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "WAF": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "IAM": {
        "total": 5,
        "correct": 5,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudFormation": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudTrail": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "Kinesis": {
        "total": 7,
        "correct": 7,
        "incorrect": 0,
        "accuracy": 100
      },
      "ECR": {
        "total": 5,
        "correct": 5,
        "incorrect": 0,
        "accuracy": 100
      },
      "CodePipeline": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "Cognito": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "Amplify": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "KMS": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "ElastiCache": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "SQS": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "ALB": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      }
    },
    "domainBreakdown": {
      "Deployment": {
        "total": 18,
        "correct": 18,
        "incorrect": 0,
        "accuracy": 100
      },
      "Development with AWS Services": {
        "total": 31,
        "correct": 31,
        "incorrect": 0,
        "accuracy": 100
      },
      "Troubleshooting and Optimization": {
        "total": 31,
        "correct": 31,
        "incorrect": 0,
        "accuracy": 100
      },
      "Security": {
        "total": 12,
        "correct": 12,
        "incorrect": 0,
        "accuracy": 100
      }
    },
    "weakAreas": [],
    "strongAreas": [
      {
        "service": "EC2",
        "accuracy": 100,
        "questionsReviewed": 16
      },
      {
        "service": "Elastic Beanstalk",
        "accuracy": 100,
        "questionsReviewed": 5
      },
      {
        "service": "ECS",
        "accuracy": 100,
        "questionsReviewed": 8
      },
      {
        "service": "Fargate",
        "accuracy": 100,
        "questionsReviewed": 4
      },
      {
        "service": "Lambda",
        "accuracy": 100,
        "questionsReviewed": 20
      },
      {
        "service": "S3",
        "accuracy": 100,
        "questionsReviewed": 9
      },
      {
        "service": "Config",
        "accuracy": 100,
        "questionsReviewed": 19
      },
      {
        "service": "SAM",
        "accuracy": 100,
        "questionsReviewed": 9
      },
      {
        "service": "API Gateway",
        "accuracy": 100,
        "questionsReviewed": 6
      },
      {
        "service": "CloudWatch",
        "accuracy": 100,
        "questionsReviewed": 5
      },
      {
        "service": "X-Ray",
        "accuracy": 100,
        "questionsReviewed": 3
      },
      {
        "service": "CodeDeploy",
        "accuracy": 100,
        "questionsReviewed": 5
      },
      {
        "service": "DynamoDB",
        "accuracy": 100,
        "questionsReviewed": 10
      },
      {
        "service": "EBS",
        "accuracy": 100,
        "questionsReviewed": 4
      },
      {
        "service": "RDS",
        "accuracy": 100,
        "questionsReviewed": 12
      },
      {
        "service": "CloudFront",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "IAM",
        "accuracy": 100,
        "questionsReviewed": 5
      },
      {
        "service": "CloudFormation",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "Kinesis",
        "accuracy": 100,
        "questionsReviewed": 7
      },
      {
        "service": "ECR",
        "accuracy": 100,
        "questionsReviewed": 5
      },
      {
        "service": "CodePipeline",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "KMS",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "ElastiCache",
        "accuracy": 100,
        "questionsReviewed": 2
      }
    ],
    "topKeywords": [
      {
        "keyword": "deployment",
        "count": 12
      },
      {
        "keyword": "serverless",
        "count": 8
      },
      {
        "keyword": "security",
        "count": 6
      },
      {
        "keyword": "container",
        "count": 5
      },
      {
        "keyword": "performance",
        "count": 5
      },
      {
        "keyword": "IAM role",
        "count": 4
      },
      {
        "keyword": "synchronous",
        "count": 3
      },
      {
        "keyword": "monitoring",
        "count": 3
      },
      {
        "keyword": "authentication",
        "count": 3
      },
      {
        "keyword": "policy",
        "count": 3
      },
      {
        "keyword": "scaling",
        "count": 3
      },
      {
        "keyword": "asynchronous",
        "count": 2
      },
      {
        "keyword": "caching",
        "count": 2
      },
      {
        "keyword": "canary",
        "count": 2
      },
      {
        "keyword": "encryption",
        "count": 2
      }
    ],
    "recommendations": [
      "🎯 Excellent performance! You're exam-ready. Focus on scenario-based practice"
    ]
  },
  "heatmap": [
    {
      "service": "EC2",
      "accuracy": 100,
      "total": 16,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Elastic Beanstalk",
      "accuracy": 100,
      "total": 5,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ECS",
      "accuracy": 100,
      "total": 8,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Fargate",
      "accuracy": 100,
      "total": 4,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Lambda",
      "accuracy": 100,
      "total": 20,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "S3",
      "accuracy": 100,
      "total": 9,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Step Functions",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Config",
      "accuracy": 100,
      "total": 19,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "SAM",
      "accuracy": 100,
      "total": 9,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "EKS",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "API Gateway",
      "accuracy": 100,
      "total": 6,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudWatch",
      "accuracy": 100,
      "total": 5,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Route 53",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ELB",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "X-Ray",
      "accuracy": 100,
      "total": 3,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CodeDeploy",
      "accuracy": 100,
      "total": 5,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "DynamoDB",
      "accuracy": 100,
      "total": 10,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "EBS",
      "accuracy": 100,
      "total": 4,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "RDS",
      "accuracy": 100,
      "total": 12,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudFront",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "WAF",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "IAM",
      "accuracy": 100,
      "total": 5,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudFormation",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudTrail",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Kinesis",
      "accuracy": 100,
      "total": 7,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ECR",
      "accuracy": 100,
      "total": 5,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CodePipeline",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Cognito",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Amplify",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "KMS",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ElastiCache",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "SQS",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ALB",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    }
  ],
  "quickStats": {
    "accuracy": "100.0%",
    "correct": 44,
    "incorrect": 0,
    "totalServices": 33,
    "weakAreaCount": 0
  }
}