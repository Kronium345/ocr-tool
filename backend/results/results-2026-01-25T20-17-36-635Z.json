{
  "success": true,
  "processedCount": 44,
  "timestamp": "2026-01-25T20:17:36.635Z",
  "questions": [
    {
      "file": "Screenshot 2026-01-17 090222.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services .\nFor application deployments, a company is using CloudFormation templates, which are regularly updated to map the latest AMI IDs. A ii\ndeveloper was assigned to automate the process since the current set up takes a lot of time to execute on a regular basis.\nWhich of the following is the MOST suitable solution that the developer should implement to satisfy this requirement?\nSet up your Systems Manager State Manager to store the latest AMI IDs and integrate it with your CloudFormation template.\n® Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation\ntemplate.\nSet up CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you\ndecide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.\nIntegrate CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding\ndeployments.\nIntegrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.\nIncorrect\nYou can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other\nparameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the\nParameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value.\nCloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.\nIf the parameter referenced in the template does not exist in Systems Manager, there will be synchronous validation error that will be thrown. Also,\nif you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed\nagainst SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager.",
        "options": [
          "C: ategory: CDA - Development with AWS Services .",
          "a: pplication deployments, a company is using CloudFormation templates, which are regularly updated to map the latest AMI IDs. A ii",
          "d: eveloper was assigned to automate the process since the current set up takes a lot of time to execute on a regular basis.",
          "c: h of the following is the MOST suitable solution that the developer should implement to satisfy this requirement?",
          "a: nager State Manager to store the latest AMI IDs and integrate it with your CloudFormation template.",
          "C: all the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation",
          "a: te.",
          "C: loudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you",
          "d: ecide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.",
          "a: te CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding",
          "d: eployments.",
          "a: te AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.",
          "c: orrect",
          "c: an use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other",
          "a: rameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the",
          "a: rameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value.",
          "C: loudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.",
          "a: rameter referenced in the template does not exist in Systems Manager, there will be synchronous validation error that will be thrown. Also,",
          "a: ve defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed",
          "a: gainst SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "14. QUESTION\nCategory: CDA - Development with AWS Services .\nFor application deployments, a company is using CloudFormation templates, which are regularly updated to map the latest AMI IDs. A ii\ndeveloper was assigned to automate the process since the current set up takes a lot of time to execute on a regular basis.\nWhich of the following is the MOST suitable solution that the developer should implement to satisfy this requirement?\nSet up your Systems Manager State Manager to store the latest AMI IDs and integrate it with your CloudFormation template.\n® Call the update-stack API in CloudFormation whenever you decide to update the EC2 instances in your CloudFormation\ntemplate.\nSet up CloudFormation with Systems Manager Parameter Store to retrieve the latest AMI IDs for your template. Whenever you\ndecide to update the EC2 instances, call the update-stack API in CloudFormation in your CloudFormation template.\nIntegrate CloudFormation with AWS Service Catalog to fetch the latest AMI IDs and automatically use them for succeeding\ndeployments.\nIntegrate AWS Service Catalog with AWS Config to automatically fetch the latest AMI and use it for succeeding deployments.\nIncorrect\nYou can use the existing Parameters section of your CloudFormation template to define Systems Manager parameters, along with other\nparameters. Systems Manager parameters are a unique type that is different from existing parameters because they refer to actual values in the\nParameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value.\nCloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.\nIf the parameter referenced in the template does not exist in Systems Manager, there will be synchronous validation error that will be thrown. Also,\nif you have defined any parameter value validations (AllowedValues, AllowedPattern, etc.) for Systems Manager parameters, they will be performed\nagainst SSM keys which are given as input values for template parameters, not actual values stored in Systems Manager."
      },
      "tags": {
        "services": [
          "EC2",
          "Systems Manager",
          "Parameter Store",
          "CloudFormation",
          "Config",
          "Systems Manager"
        ],
        "domains": [
          "Security",
          "Deployment"
        ],
        "keywords": [
          "deployment",
          "synchronous"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 090254.png",
      "parsed": {
        "question": "®\nCategory: CDA - Development with AWS Services Ei\nA web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item\none at a time. The network overhead of these transactions causes degradation in the application's performance. You were instructed by your\nmanager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or\nmultithreading.\nWhich of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?\nUse DynamoDB Batch Operations API for GET, PUT, and DELETE operations.\n® Enable DynamoDB Streams.\nRefactor the application to use DynamoDB transactional read and write APIs .\nUpgrade the EC2 instances to a higher instance type.\nIncorrect\nFor applications that need to read or write multiple items, DynamoDB provides the BatchGetItem and BatchWriteItem operations. Using\nthese operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the\nindividual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading.\nae\naws dynamodb batch-get-item \\\n--request-items file://request-items.json\nThe arguments for --request-items are stored in the file request-items. json:",
        "options": [
          "C: ategory: CDA - Development with AWS Services Ei",
          "A: web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item",
          "a: t a time. The network overhead of these transactions causes degradation in the application's performance. You were instructed by your",
          "a: nager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or",
          "a: ding.",
          "c: h of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?",
          "D: ynamoDB Batch Operations API for GET, PUT, and DELETE operations.",
          "a: ble DynamoDB Streams.",
          "a: ctor the application to use DynamoDB transactional read and write APIs .",
          "a: de the EC2 instances to a higher instance type.",
          "c: orrect",
          "a: pplications that need to read or write multiple items, DynamoDB provides the BatchGetItem and BatchWriteItem operations. Using",
          "a: tions can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the",
          "d: ividual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading.",
          "a: e",
          "a: ws dynamodb batch-get-item \\",
          "a: rguments for --request-items are stored in the file request-items. json:"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "15. QUESTION\n®\nCategory: CDA - Development with AWS Services Ei\nA web application running in Amazon Elastic Beanstalk reads and writes a large number of related items in DynamoDB and processes each item\none at a time. The network overhead of these transactions causes degradation in the application's performance. You were instructed by your\nmanager to quickly refactor the application but without introducing major code changes such as implementing concurrency management or\nmultithreading.\nWhich of the following solutions is the EASIEST method to implement that will improve the application performance in a cost-effective manner?\nUse DynamoDB Batch Operations API for GET, PUT, and DELETE operations.\n® Enable DynamoDB Streams.\nRefactor the application to use DynamoDB transactional read and write APIs .\nUpgrade the EC2 instances to a higher instance type.\nIncorrect\nFor applications that need to read or write multiple items, DynamoDB provides the BatchGetItem and BatchWriteItem operations. Using\nthese operations can reduce the number of network round trips from your application to DynamoDB. In addition, DynamoDB performs the\nindividual read or write operations in parallel. Your applications benefit from this parallelism without having to manage concurrency or threading.\nae\naws dynamodb batch-get-item \\\n--request-items file://request-items.json\nThe arguments for --request-items are stored in the file request-items. json:"
      },
      "tags": {
        "services": [
          "EC2",
          "Elastic Beanstalk",
          "DynamoDB"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "performance"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 090341.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services [\nA company is developing a distributed system which will use a Lambda function that will be invoked asynchronously. In the event of failure, the\nfunction must be retried twice before sending the unprocessed events to an Amazon SQS queue through the use of Dead Letter Queue (DLQ).\nWhich of the following is the correct way to implement a DLQ in Lambda?\nSpecify the Amazon Resource Name of the SQS Queue in the Lambda function's DeadLetterconfig parameter.\nSpecify the AWS Service Namespace of the SQS Queue in the Lambda function's DeadLetterconfig parameter.\n® Specify the AWS Service Namespace of the SQS Queue in the AWS: :Lambda::Function resource of the CloudFormation\ntemplate that you'll use for deploying the function.\nSpecify the Amazon Resource Name of the SQS Queue in the Transform section of the AWS SAM template that you'll use\nfor deploying the function.\nIncorrect\nAny Lambda function invoked asynchronously is retried twice before the event is discarded. If the retries fail and you're unsure why, use Dead\nLetter Queues (DLQ) to direct unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure.\nClentupiosdsan —\nSins: am EEE\nfe imotettunbnai\nRE RAETEE |",
        "options": [
          "C: ategory: CDA - Development with AWS Services [",
          "A: company is developing a distributed system which will use a Lambda function that will be invoked asynchronously. In the event of failure, the",
          "c: tion must be retried twice before sending the unprocessed events to an Amazon SQS queue through the use of Dead Letter Queue (DLQ).",
          "c: h of the following is the correct way to implement a DLQ in Lambda?",
          "c: ify the Amazon Resource Name of the SQS Queue in the Lambda function's DeadLetterconfig parameter.",
          "c: ify the AWS Service Namespace of the SQS Queue in the Lambda function's DeadLetterconfig parameter.",
          "c: ify the AWS Service Namespace of the SQS Queue in the AWS: :Lambda::Function resource of the CloudFormation",
          "a: te that you'll use for deploying the function.",
          "c: ify the Amazon Resource Name of the SQS Queue in the Transform section of the AWS SAM template that you'll use",
          "d: eploying the function.",
          "c: orrect",
          "A: ny Lambda function invoked asynchronously is retried twice before the event is discarded. If the retries fail and you're unsure why, use Dead",
          "D: LQ) to direct unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure.",
          "C: lentupiosdsan —",
          "a: m EEE",
          "b: nai",
          "A: ETEE |"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "16. QUESTION\nCategory: CDA - Development with AWS Services [\nA company is developing a distributed system which will use a Lambda function that will be invoked asynchronously. In the event of failure, the\nfunction must be retried twice before sending the unprocessed events to an Amazon SQS queue through the use of Dead Letter Queue (DLQ).\nWhich of the following is the correct way to implement a DLQ in Lambda?\nSpecify the Amazon Resource Name of the SQS Queue in the Lambda function's DeadLetterconfig parameter.\nSpecify the AWS Service Namespace of the SQS Queue in the Lambda function's DeadLetterconfig parameter.\n® Specify the AWS Service Namespace of the SQS Queue in the AWS: :Lambda::Function resource of the CloudFormation\ntemplate that you'll use for deploying the function.\nSpecify the Amazon Resource Name of the SQS Queue in the Transform section of the AWS SAM template that you'll use\nfor deploying the function.\nIncorrect\nAny Lambda function invoked asynchronously is retried twice before the event is discarded. If the retries fail and you're unsure why, use Dead\nLetter Queues (DLQ) to direct unprocessed events to an Amazon SQS queue or an Amazon SNS topic to analyze the failure.\nClentupiosdsan —\nSins: am EEE\nfe imotettunbnai\nRE RAETEE |"
      },
      "tags": {
        "services": [
          "Lambda",
          "SQS",
          "SNS",
          "CloudFormation",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "queue",
          "topic",
          "asynchronous",
          "synchronous"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 090453.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services .\nA commercial bank is developing an online auction application with a DynamoDB database that will allow customers to bid for real estate ki\nproperties from the comforts of their homes. The application should allow the minimum acceptable price established by the bank prior to the\nauction. The opening bid entered by the staff must be at least the minimum bid and the new bids submitted by the customers should be\ngreater than the current bid. The application logic has already been implemented but the DynamoDB database calls should also be tailored to\nmeet the requirements.\nWhich of the following is the MOST effective solution that will satisfy the requirement in this scenario?\n® Use an optimistic locking strategy in your database calls to ensure that the new bid submitted by the customer is greater than\nthe current bid.\nEnable DynamoDB Transactions to automatically check the minimum acceptable price as well as the current and new bid\nprice.\nUse DynamoDB Streams and a Lambda function to track the current bid price and compare against all of the new bids\nsubmitted by the customers.\nConfigure the database calls of the application to use conditional updates and conditional writes with a condition expression\nthat will check if the new bid submitted by the customer is greater than the current bid.\nIncorrect\nBy default, the DynamoDB write operations ( PutItem , UpdateItem , DeleteItem ) are unconditional: each of these operations will\noverwrite an existing item that has the specified primary key.\nDynamoDB optionally supports conditional writes for these operations. A conditional write will succeed only if the item attributes meet one or more\nexpected conditions. Otherwise, it returns an error.",
        "options": [
          "C: ategory: CDA - Development with AWS Services .",
          "A: commercial bank is developing an online auction application with a DynamoDB database that will allow customers to bid for real estate ki",
          "c: omforts of their homes. The application should allow the minimum acceptable price established by the bank prior to the",
          "a: uction. The opening bid entered by the staff must be at least the minimum bid and the new bids submitted by the customers should be",
          "a: ter than the current bid. The application logic has already been implemented but the DynamoDB database calls should also be tailored to",
          "c: h of the following is the MOST effective solution that will satisfy the requirement in this scenario?",
          "a: n optimistic locking strategy in your database calls to ensure that the new bid submitted by the customer is greater than",
          "c: urrent bid.",
          "a: ble DynamoDB Transactions to automatically check the minimum acceptable price as well as the current and new bid",
          "c: e.",
          "D: ynamoDB Streams and a Lambda function to track the current bid price and compare against all of the new bids",
          "b: mitted by the customers.",
          "C: onfigure the database calls of the application to use conditional updates and conditional writes with a condition expression",
          "a: t will check if the new bid submitted by the customer is greater than the current bid.",
          "c: orrect",
          "B: y default, the DynamoDB write operations ( PutItem , UpdateItem , DeleteItem ) are unconditional: each of these operations will",
          "a: n existing item that has the specified primary key.",
          "D: ynamoDB optionally supports conditional writes for these operations. A conditional write will succeed only if the item attributes meet one or more",
          "c: ted conditions. Otherwise, it returns an error."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "18. QUESTION\nCategory: CDA - Development with AWS Services .\nA commercial bank is developing an online auction application with a DynamoDB database that will allow customers to bid for real estate ki\nproperties from the comforts of their homes. The application should allow the minimum acceptable price established by the bank prior to the\nauction. The opening bid entered by the staff must be at least the minimum bid and the new bids submitted by the customers should be\ngreater than the current bid. The application logic has already been implemented but the DynamoDB database calls should also be tailored to\nmeet the requirements.\nWhich of the following is the MOST effective solution that will satisfy the requirement in this scenario?\n® Use an optimistic locking strategy in your database calls to ensure that the new bid submitted by the customer is greater than\nthe current bid.\nEnable DynamoDB Transactions to automatically check the minimum acceptable price as well as the current and new bid\nprice.\nUse DynamoDB Streams and a Lambda function to track the current bid price and compare against all of the new bids\nsubmitted by the customers.\nConfigure the database calls of the application to use conditional updates and conditional writes with a condition expression\nthat will check if the new bid submitted by the customer is greater than the current bid.\nIncorrect\nBy default, the DynamoDB write operations ( PutItem , UpdateItem , DeleteItem ) are unconditional: each of these operations will\noverwrite an existing item that has the specified primary key.\nDynamoDB optionally supports conditional writes for these operations. A conditional write will succeed only if the item attributes meet one or more\nexpected conditions. Otherwise, it returns an error."
      },
      "tags": {
        "services": [
          "Lambda",
          "DynamoDB",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 092528.png",
      "parsed": {
        "question": "uIZ1 December 24, 2026\nAWS Certified Developer\nAssociate DVA-C02 . age .\n; Randomized Test —- AWS Certified Developer Associate\nPractice Exams 2026 p\n40 of 65 questions answered correctly\nsl Your Progress\nRandomizediTest You have reached 40 of 65 point(s), (61.54%)\n=2 AWS Certified Developer Associate Prac\nCategories\n[& Randomized Test - AWS Certified Devel\nCDA - Deployment\nPractice Exams - Timed Mode CDA - Development with AWS Services\nCDA - Security\nPractice Exams — Review Mode D) CDA - Troubleshooting and Optimization\nPractice Exams/= Section-Based Sorry, you failed the test. Carefully read our detailed explanations including the references and cheat sheets then try again. &\nBONUS SECTION — FLASHCAR... To view your record of all previous attempts:",
        "options": [
          "A: LS",
          "d: awolowol + 2",
          "D: DOJO Ov",
          "B: ack to Course",
          "D: ecember 24, 2026",
          "A: WS Certified Developer",
          "A: ssociate DVA-C02 . age .",
          "a: ndomized Test —- AWS Certified Developer Associate",
          "a: ctice Exams 2026 p",
          "a: nswered correctly",
          "a: ndomizediTest You have reached 40 of 65 point(s), (61.54%)",
          "A: WS Certified Developer Associate Prac",
          "C: ategories",
          "a: ndomized Test - AWS Certified Devel",
          "C: DA - Deployment",
          "a: ctice Exams - Timed Mode CDA - Development with AWS Services",
          "C: DA - Security",
          "a: ctice Exams — Review Mode D) CDA - Troubleshooting and Optimization",
          "a: ctice Exams/= Section-Based Sorry, you failed the test. Carefully read our detailed explanations including the references and cheat sheets then try again. &",
          "B: ONUS SECTION — FLASHCAR... To view your record of all previous attempts:"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "s including the references and cheat sheets then try again. &\nBONUS SECTION — FLASHCAR... To view your record of all previous attempts:",
        "rawText": "TUTORIALS\n= dawolowol + 2\n= (ID DOJO Ov\n< Back to Course\nQuIZ1 December 24, 2026\nAWS Certified Developer\nAssociate DVA-C02 . age .\n; Randomized Test —- AWS Certified Developer Associate\nPractice Exams 2026 p\n40 of 65 questions answered correctly\nsl Your Progress\nRandomizediTest You have reached 40 of 65 point(s), (61.54%)\n=2 AWS Certified Developer Associate Prac\nCategories\n[& Randomized Test - AWS Certified Devel\nCDA - Deployment\nPractice Exams - Timed Mode CDA - Development with AWS Services\nCDA - Security\nPractice Exams — Review Mode D) CDA - Troubleshooting and Optimization\nPractice Exams/= Section-Based Sorry, you failed the test. Carefully read our detailed explanations including the references and cheat sheets then try again. &\nBONUS SECTION — FLASHCAR... To view your record of all previous attempts:"
      },
      "tags": {
        "services": [],
        "domains": [],
        "keywords": [
          "security",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 094234.png",
      "parsed": {
        "question": "[\nCategory: CDA - Troubleshooting and Optimization\nA financial company has a cryptocurrency application that has been hosted in Elastic Beanstalk for a couple of months. Recently, the\napplication's performance has been degrading, so you decided to check the CPU and memory utilization of the underlying EC2 instances in\nCloudWatch. You can see the CPU utilization of the instances but not the memory utilization.\nWhich of the following is the MOST likely cause of this issue?\nThe detailed monitoring is not enabled in CloudWatch.\nCloudWatch does not track memory utilization by default.\n® The .ebextensions/xray-daemon.config file in Elastic Beanstalk is missing.\nX-Ray Daemon is not installed on the EC2 instances.\nIncorrect\nAmazon CloudWatch agent enables you to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The\nagent supports both Windows Server and Linux and allows you to select the metrics to be collected, including sub-resource metrics such as per-\nCPU core.\nDashboard unucsystem |v Q Xx IK < 1neuzasnves 3\nAarms\n«Linux System > Filesystem instanced, MountPath\nFilesystem ~ Instanceld ~ InstanceName ~  MountPath ~ Metric Name -\nOK [4] Idevivdal d0e7410D pythonsampie-dev / DiskSpaceAvallable\nBiling Idevirvaat 1a0e74100 pythonsampie-dev / DiskSpaceUsed\nLogs ¥ /devivdal d0e7410D pythonsampie-dev. fi DiskSpaceUtization\n|| Metrics",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: financial company has a cryptocurrency application that has been hosted in Elastic Beanstalk for a couple of months. Recently, the",
          "a: pplication's performance has been degrading, so you decided to check the CPU and memory utilization of the underlying EC2 instances in",
          "C: loudWatch. You can see the CPU utilization of the instances but not the memory utilization.",
          "c: h of the following is the MOST likely cause of this issue?",
          "d: etailed monitoring is not enabled in CloudWatch.",
          "C: loudWatch does not track memory utilization by default.",
          "b: extensions/xray-daemon.config file in Elastic Beanstalk is missing.",
          "a: y Daemon is not installed on the EC2 instances.",
          "c: orrect",
          "A: mazon CloudWatch agent enables you to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The",
          "a: gent supports both Windows Server and Linux and allows you to select the metrics to be collected, including sub-resource metrics such as per-",
          "C: PU core.",
          "D: ashboard unucsystem |v Q Xx IK < 1neuzasnves 3",
          "A: arms",
          "a: nced, MountPath",
          "a: nceld ~ InstanceName ~  MountPath ~ Metric Name -",
          "d: evivdal d0e7410D pythonsampie-dev / DiskSpaceAvallable",
          "B: iling Idevirvaat 1a0e74100 pythonsampie-dev / DiskSpaceUsed",
          "d: evivdal d0e7410D pythonsampie-dev. fi DiskSpaceUtization",
          "c: s"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "19. QUESTION [\nCategory: CDA - Troubleshooting and Optimization\nA financial company has a cryptocurrency application that has been hosted in Elastic Beanstalk for a couple of months. Recently, the\napplication's performance has been degrading, so you decided to check the CPU and memory utilization of the underlying EC2 instances in\nCloudWatch. You can see the CPU utilization of the instances but not the memory utilization.\nWhich of the following is the MOST likely cause of this issue?\nThe detailed monitoring is not enabled in CloudWatch.\nCloudWatch does not track memory utilization by default.\n® The .ebextensions/xray-daemon.config file in Elastic Beanstalk is missing.\nX-Ray Daemon is not installed on the EC2 instances.\nIncorrect\nAmazon CloudWatch agent enables you to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The\nagent supports both Windows Server and Linux and allows you to select the metrics to be collected, including sub-resource metrics such as per-\nCPU core.\nDashboard unucsystem |v Q Xx IK < 1neuzasnves 3\nAarms\n«Linux System > Filesystem instanced, MountPath\nFilesystem ~ Instanceld ~ InstanceName ~  MountPath ~ Metric Name -\nOK [4] Idevivdal d0e7410D pythonsampie-dev / DiskSpaceAvallable\nBiling Idevirvaat 1a0e74100 pythonsampie-dev / DiskSpaceUsed\nLogs ¥ /devivdal d0e7410D pythonsampie-dev. fi DiskSpaceUtization\n|| Metrics"
      },
      "tags": {
        "services": [
          "EC2",
          "Elastic Beanstalk",
          "X-Ray",
          "CloudWatch",
          "Config",
          "SAM"
        ],
        "domains": [
          "Security",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "performance",
          "monitoring"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 094325.png",
      "parsed": {
        "question": "Category: CDA - Security E\nA developer is designing a multi-tiered system which utilizes various AWS resources. The application will be hosted in Elastic Beanstalk, which\nuses an RDS database and an S3 bucket that is configured to use Server-Side Encryption with Customer-Provided Encryption Keys (SSE-C). In\nthis configuration, Amazon S3 does not store the encryption key you provide but instead, stores a randomly salted hash-based message\nauthentication code (HMAC) value of the encryption key in order to validate future requests.\nWhich of the following is a valid consideration that the developer should keep in mind when implementing this architecture?\nIf you lose the encryption key, you lose the object.\nThe salted HMAC value can be used to derive the value of the encryption key.\n@® The salted HMAC value can be used to decrypt the contents of the encrypted object.\nIf you lose the encryption key, the salted HMAC value can be used to decrypt the object.\nIncorrect\nServer-side encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you\nto set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it\nwrites to disks, and decryption, when you access your objects. Therefore, you don’t need to maintain any code to perform data encryption and\ndecryption. The only thing you do is manage the encryption keys you provide.\nEncrypted Object CED\n4 a al \\ |\n—",
        "options": [
          "C: ategory: CDA - Security E",
          "A: developer is designing a multi-tiered system which utilizes various AWS resources. The application will be hosted in Elastic Beanstalk, which",
          "a: n RDS database and an S3 bucket that is configured to use Server-Side Encryption with Customer-Provided Encryption Keys (SSE-C). In",
          "c: onfiguration, Amazon S3 does not store the encryption key you provide but instead, stores a randomly salted hash-based message",
          "a: uthentication code (HMAC) value of the encryption key in order to validate future requests.",
          "c: h of the following is a valid consideration that the developer should keep in mind when implementing this architecture?",
          "c: ryption key, you lose the object.",
          "a: lted HMAC value can be used to derive the value of the encryption key.",
          "a: lted HMAC value can be used to decrypt the contents of the encrypted object.",
          "c: ryption key, the salted HMAC value can be used to decrypt the object.",
          "c: orrect",
          "d: e encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you",
          "c: ryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it",
          "d: isks, and decryption, when you access your objects. Therefore, you don’t need to maintain any code to perform data encryption and",
          "d: ecryption. The only thing you do is manage the encryption keys you provide.",
          "c: rypted Object CED",
          "a: al \\ |"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "21. QUESTION\nCategory: CDA - Security E\nA developer is designing a multi-tiered system which utilizes various AWS resources. The application will be hosted in Elastic Beanstalk, which\nuses an RDS database and an S3 bucket that is configured to use Server-Side Encryption with Customer-Provided Encryption Keys (SSE-C). In\nthis configuration, Amazon S3 does not store the encryption key you provide but instead, stores a randomly salted hash-based message\nauthentication code (HMAC) value of the encryption key in order to validate future requests.\nWhich of the following is a valid consideration that the developer should keep in mind when implementing this architecture?\nIf you lose the encryption key, you lose the object.\nThe salted HMAC value can be used to derive the value of the encryption key.\n@® The salted HMAC value can be used to decrypt the contents of the encrypted object.\nIf you lose the encryption key, the salted HMAC value can be used to decrypt the object.\nIncorrect\nServer-side encryption is about protecting data at rest. Using server-side encryption with customer-provided encryption keys (SSE-C) allows you\nto set your own encryption keys. With the encryption key you provide as part of your request, Amazon S3 manages both the encryption, as it\nwrites to disks, and decryption, when you access your objects. Therefore, you don’t need to maintain any code to perform data encryption and\ndecryption. The only thing you do is manage the encryption keys you provide.\nEncrypted Object CED\n4 a al \\ |\n—"
      },
      "tags": {
        "services": [
          "Elastic Beanstalk",
          "S3",
          "RDS",
          "Config",
          "ECR"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment"
        ],
        "keywords": [
          "security",
          "encryption",
          "authentication"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 094409.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that\nis used by different departments. The developer used the iam create-account-alias --account-alias finance-dept AWS CLI\ncommand to create a user-friendly identifier for the finance department.\nFor faster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests,\nAWS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having\nto re-instrument the application code is required as well.\nWhich of the following options is the MOST suitable solution that the developer implements?\nUse the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and\nconfigure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the\ndownstream API calls.\n® Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure\nthe AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.\nUse the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and\nconfigure the Amazon CloudWatch Evidently to trace all the downstream API calls.\nUse the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the\nAWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.\nIncorrect\nThe SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. The AWS Distro for OpenTelemetry is part",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that",
          "d: by different departments. The developer used the iam create-account-alias --account-alias finance-dept AWS CLI",
          "c: ommand to create a user-friendly identifier for the finance department.",
          "a: ster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests,",
          "A: WS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having",
          "a: pplication code is required as well.",
          "c: h of the following options is the MOST suitable solution that the developer implements?",
          "a: nce-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and",
          "c: onfigure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the",
          "d: ownstream API calls.",
          "a: nce-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure",
          "A: WS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.",
          "a: ws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and",
          "c: onfigure the Amazon CloudWatch Evidently to trace all the downstream API calls.",
          "a: nce-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the",
          "A: WS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.",
          "c: orrect",
          "D: Ks included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. The AWS Distro for OpenTelemetry is part"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "24. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA company has an AWS account with an ID of 061218980612 and has a centralized Java web application hosted in AWS Elastic Beanstalk that\nis used by different departments. The developer used the iam create-account-alias --account-alias finance-dept AWS CLI\ncommand to create a user-friendly identifier for the finance department.\nFor faster troubleshooting, the application must also be configured to easily trace all its downstream requests, such as Apache HTTP requests,\nAWS SDK requests, and SQL queries made using a JDBC driver. The ability to send traces to multiple different tracing backends without having\nto re-instrument the application code is required as well.\nWhich of the following options is the MOST suitable solution that the developer implements?\nUse the https://finance-dept.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and\nconfigure an IAM Roles Anywhere trust model in Elastic Beanstalk with a proper source identity prefix to trace all the\ndownstream API calls.\n® Use the https://finance-dept.aws.amazon.com/console sign-in page URL for the AWS account. Install and configure\nthe AWS X-Ray auto-instrumentation Java agent to trace all the downstream API calls.\nUse the https://061218980612.aws.signin.amazon.com/console sign-in page URL for the AWS account. Set up and\nconfigure the Amazon CloudWatch Evidently to trace all the downstream API calls.\nUse the https://finance-dept.signin.aws.amazon.com/console sign-in page URL for the AWS account. Install the\nAWS Distro for OpenTelemetry Collector and set up the AWS Distro for OpenTelemetry to trace all the downstream API calls.\nIncorrect\nThe SDKs included with X-Ray are part of a tightly integrated instrumentation solution offered by AWS. The AWS Distro for OpenTelemetry is part"
      },
      "tags": {
        "services": [
          "Elastic Beanstalk",
          "IAM",
          "X-Ray",
          "CloudWatch",
          "Config"
        ],
        "domains": [
          "Security",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "IAM role"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 094539.png",
      "parsed": {
        "question": "i\nCategory: CDA - Development with AWS Services\nA mobile game has a serverless backend in AWS which is composed of Lambda, API Gateway, and DynamoDB. It writes 100 items per second\nto the DynamoDB table and the size is 1.5 KB per item. The table has a provisioned WCU of 100 but the write requests are still being throttled\nby DynamoDB.\nWhat is the MOST suitable solution in order to rectify this throttling issue?\nEnable DynamoDB Accelerator (DAX).\nImplement database caching with an ElastiCache cluster.\n® Use strong consistency in the write operations.\nIncrease the WCU to 200.\nIncorrect\nA write capacity unit (WCU) represents one write per second, for an item up to 1 KB in size.\nFor example, suppose that you create a table with 10 write capacity units. This allows you to perform 10 writes per second, for items up to 1 KB in\nsize per second. Item sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same\nthroughput as writing a 1 KB item.\nIf you need to write an item that is larger than 1 KB, DynamoDB must consume additional write capacity units. Transactional write requests require\n2 write capacity units to perform one write per second for items up to 1 KB. The total number of write capacity units required depends on the item\nsize. For example, if your item size is 2 KB, you require 2 write capacity units to sustain one standard write request per second or 4 write capacity\nunits for a transactional write request.",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: mobile game has a serverless backend in AWS which is composed of Lambda, API Gateway, and DynamoDB. It writes 100 items per second",
          "D: ynamoDB table and the size is 1.5 KB per item. The table has a provisioned WCU of 100 but the write requests are still being throttled",
          "b: y DynamoDB.",
          "a: t is the MOST suitable solution in order to rectify this throttling issue?",
          "a: ble DynamoDB Accelerator (DAX).",
          "d: atabase caching with an ElastiCache cluster.",
          "c: onsistency in the write operations.",
          "c: rease the WCU to 200.",
          "c: orrect",
          "A: write capacity unit (WCU) represents one write per second, for an item up to 1 KB in size.",
          "a: mple, suppose that you create a table with 10 write capacity units. This allows you to perform 10 writes per second, for items up to 1 KB in",
          "c: ond. Item sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same",
          "a: s writing a 1 KB item.",
          "d: to write an item that is larger than 1 KB, DynamoDB must consume additional write capacity units. Transactional write requests require",
          "c: apacity units to perform one write per second for items up to 1 KB. The total number of write capacity units required depends on the item",
          "a: mple, if your item size is 2 KB, you require 2 write capacity units to sustain one standard write request per second or 4 write capacity",
          "a: transactional write request."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "EEE EE EEE\n28. QUESTION i\nCategory: CDA - Development with AWS Services\nA mobile game has a serverless backend in AWS which is composed of Lambda, API Gateway, and DynamoDB. It writes 100 items per second\nto the DynamoDB table and the size is 1.5 KB per item. The table has a provisioned WCU of 100 but the write requests are still being throttled\nby DynamoDB.\nWhat is the MOST suitable solution in order to rectify this throttling issue?\nEnable DynamoDB Accelerator (DAX).\nImplement database caching with an ElastiCache cluster.\n® Use strong consistency in the write operations.\nIncrease the WCU to 200.\nIncorrect\nA write capacity unit (WCU) represents one write per second, for an item up to 1 KB in size.\nFor example, suppose that you create a table with 10 write capacity units. This allows you to perform 10 writes per second, for items up to 1 KB in\nsize per second. Item sizes for writes are rounded up to the next 1 KB multiple. For example, writing a 500-byte item consumes the same\nthroughput as writing a 1 KB item.\nIf you need to write an item that is larger than 1 KB, DynamoDB must consume additional write capacity units. Transactional write requests require\n2 write capacity units to perform one write per second for items up to 1 KB. The total number of write capacity units required depends on the item\nsize. For example, if your item size is 2 KB, you require 2 write capacity units to sustain one standard write request per second or 4 write capacity\nunits for a transactional write request."
      },
      "tags": {
        "services": [
          "Lambda",
          "DynamoDB",
          "ElastiCache",
          "API Gateway",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "caching",
          "throttling"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 094648.png",
      "parsed": {
        "question": "Category: CDA - Security\nYou are developing a new batch job for the enterprise application suite in your company, which is hosted in an Auto Scaling group of EC2 [\ninstances behind an ELB. The application is using an S3 bucket configured with Server-Side Encryption with AWS KMS Keys (SSE-KMS). The\nbatch job must upload files to the bucket using the default AWS KMS key to protect the data at rest.\nWhat should you do to satisfy this requirement with the LEAST amount of configuration?\nInclude the x-amz-server-side-encryption header with a value of aws:kms in your upload request.\nInclude the x-amz-server-side-encryption header with a value of AEs256 in your upload request.\nInclude the x-amz-server-side-encryption-customer-algorithm , x-amz-server-side-encryption-customer-key ,\nand x-amz-server-side-encryption-customer-key-MD5 headers with appropriate values in the upload request.\n® Include the x-amz-server-side-encryption header with a value of aws:kms as well asthe x-amz-server-side-\nencryption-aws-kms-key-id header containing the ID of the default AWS KMS key in your upload request.\nIncorrect\nServer-side encryption is about protecting data at rest. AWS Key Management Service (AWS KMS) is a service that combines secure, highly\navailable hardware and software to provide a key management system scaled for the cloud. AWS KMS uses KMS keys to encrypt your Amazon S3\nobjects. You use AWS KMS via the AWS Management Console or AWS KMS APIs to centrally create encryption keys, define the policies that\ncontrol how keys can be used, and audit key usage to prove they are being used correctly. You can use these keys to protect your data in Amazon\nS3 buckets.\nol a a",
        "options": [
          "C: ategory: CDA - Security",
          "a: re developing a new batch job for the enterprise application suite in your company, which is hosted in an Auto Scaling group of EC2 [",
          "a: nces behind an ELB. The application is using an S3 bucket configured with Server-Side Encryption with AWS KMS Keys (SSE-KMS). The",
          "b: atch job must upload files to the bucket using the default AWS KMS key to protect the data at rest.",
          "a: t should you do to satisfy this requirement with the LEAST amount of configuration?",
          "c: lude the x-amz-server-side-encryption header with a value of aws:kms in your upload request.",
          "c: lude the x-amz-server-side-encryption header with a value of AEs256 in your upload request.",
          "c: lude the x-amz-server-side-encryption-customer-algorithm , x-amz-server-side-encryption-customer-key ,",
          "a: nd x-amz-server-side-encryption-customer-key-MD5 headers with appropriate values in the upload request.",
          "c: lude the x-amz-server-side-encryption header with a value of aws:kms as well asthe x-amz-server-side-",
          "c: ryption-aws-kms-key-id header containing the ID of the default AWS KMS key in your upload request.",
          "c: orrect",
          "d: e encryption is about protecting data at rest. AWS Key Management Service (AWS KMS) is a service that combines secure, highly",
          "a: vailable hardware and software to provide a key management system scaled for the cloud. AWS KMS uses KMS keys to encrypt your Amazon S3",
          "b: jects. You use AWS KMS via the AWS Management Console or AWS KMS APIs to centrally create encryption keys, define the policies that",
          "c: ontrol how keys can be used, and audit key usage to prove they are being used correctly. You can use these keys to protect your data in Amazon",
          "b: uckets.",
          "a: a"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "30. QUESTION\nCategory: CDA - Security\nYou are developing a new batch job for the enterprise application suite in your company, which is hosted in an Auto Scaling group of EC2 [\ninstances behind an ELB. The application is using an S3 bucket configured with Server-Side Encryption with AWS KMS Keys (SSE-KMS). The\nbatch job must upload files to the bucket using the default AWS KMS key to protect the data at rest.\nWhat should you do to satisfy this requirement with the LEAST amount of configuration?\nInclude the x-amz-server-side-encryption header with a value of aws:kms in your upload request.\nInclude the x-amz-server-side-encryption header with a value of AEs256 in your upload request.\nInclude the x-amz-server-side-encryption-customer-algorithm , x-amz-server-side-encryption-customer-key ,\nand x-amz-server-side-encryption-customer-key-MD5 headers with appropriate values in the upload request.\n® Include the x-amz-server-side-encryption header with a value of aws:kms as well asthe x-amz-server-side-\nencryption-aws-kms-key-id header containing the ID of the default AWS KMS key in your upload request.\nIncorrect\nServer-side encryption is about protecting data at rest. AWS Key Management Service (AWS KMS) is a service that combines secure, highly\navailable hardware and software to provide a key management system scaled for the cloud. AWS KMS uses KMS keys to encrypt your Amazon S3\nobjects. You use AWS KMS via the AWS Management Console or AWS KMS APIs to centrally create encryption keys, define the policies that\ncontrol how keys can be used, and audit key usage to prove they are being used correctly. You can use these keys to protect your data in Amazon\nS3 buckets.\nol a a"
      },
      "tags": {
        "services": [
          "EC2",
          "S3",
          "ELB",
          "KMS",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Security"
        ],
        "keywords": [
          "scaling",
          "security",
          "encryption"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 095231.png",
      "parsed": {
        "question": "",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: company has an application that is using CloudFront to serve their static contents to their users around the globe. They are receiving a",
          "b: er of bad reviews from their customers lately because it takes a lot of time to log into their website. Sometimes, their users are also [",
          "c: h is why the developer was instructed to fix this problem immediately.",
          "c: h of the following combination of options should the developer use together to set up a cost-effective solution for this scenario? (Select",
          "a: unch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to",
          "a: sily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model",
          "A: M) service to improve the overall application performance.",
          "C: onfigure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as",
          "c: ond origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code",
          "a: ilure responses.",
          "a: unch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy",
          "c: oming traffic to the region with the best latency to the user.",
          "A: dd a cache-Control max-age directive to your objects in CloudFront and specify the longest practical value for max-",
          "a: ge to increase the cache hit ratio of your CloudFront distribution.",
          "C: ustomize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your",
          "a: mbda functions to execute the authentication process in AWS locations closer to the users.",
          "c: orrect",
          "a: mbda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer",
          "c: tinne rin in reenances ta ClandErant avante withal it nravicianine ar mananing carvere Yall can tea | amhda fiinectinne +a"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "SWE TUN\nCategory: CDA - Development with AWS Services\nA company has an application that is using CloudFront to serve their static contents to their users around the globe. They are receiving a\nnumber of bad reviews from their customers lately because it takes a lot of time to log into their website. Sometimes, their users are also [\ngetting HTTP 504 errors which is why the developer was instructed to fix this problem immediately.\nWhich of the following combination of options should the developer use together to set up a cost-effective solution for this scenario? (Select\nTWO.)\nLaunch your application to multiple and geographically disperse VPCs on various AWS regions then create a transit VPC to\neasily connect all your resources. Use several Lambda functions in each region using the AWS Serverless Application Model\n(SAM) service to improve the overall application performance.\nConfigure an origin failover by creating an origin group with two origins. Specify one as the primary origin and the other as\nthe second origin which CloudFront automatically switches to when the primary origin returns specific HTTP status code\nfailure responses.\nLaunch your application to multiple AWS regions to serve your global users. Use a Route 53 record with latency routing policy\nto route incoming traffic to the region with the best latency to the user.\nAdd a cache-Control max-age directive to your objects in CloudFront and specify the longest practical value for max-\nage to increase the cache hit ratio of your CloudFront distribution.\nCustomize the content that the CloudFront web distribution delivers to your users using Lambda@Edge, which allows your\nLambda functions to execute the authentication process in AWS locations closer to the users.\nIncorrect\nLambda@Edge lets you run Lambda functions to customize the content that CloudFront delivers, executing the functions in AWS locations closer\n+n the viewer The flinectinne rin in reenances ta ClandErant avante withal it nravicianine ar mananing carvere Yall can tea | amhda fiinectinne +a"
      },
      "tags": {
        "services": [
          "Lambda",
          "EBS",
          "VPC",
          "CloudFront",
          "Route 53",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "performance",
          "authentication",
          "policy",
          "VPC"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 095342.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA web application is currently deployed on an On-Demand Linux EC2 instance that connects to an Amazon RDS database. Users have\nfrequently reported that the application crashes intermittently. The support team has reviewed the logs in CloudWatch but has been unable to\nidentify the root cause. To enhance troubleshooting, the team needs to monitor additional metrics, including memory utilization, swap usage,\nand the count of idle and active processes running on the instance.\nWhich solution would be the MOST appropriate to implement in this situation?\nUse detailed monitoring in CloudWatch.\nUse AWS CloudShell to consolidate all metrics in a single dashboard.\n® Install the AWS X-Ray daemon on the EC2 instance.\nInstall the Amazon CloudWatch Logs agent to the EC2 instance.\nIncorrect\nYou can use the CloudWatch agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent\nsupports both Windows Server and Linux, and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU\ncore. Aside from the usual metrics, it also tracks the memory, swap, and disk space utilization metrics of your server.\nDashboard Unux System v Q X IK € 1n12c2Meia 3H\nAlarms\n4 LUinux System > Filesystem Instanceld, MountPath\nFilesystem ~  Instanceld + InstanceName ~  MountPath + Metric Name -\noK © | © /gevinvdat 1-0e7410D pythonsampie-dev ’ DiskSpaceAvallable\nBilling 1 idevivdal -d0e7410D pythonsampie-dev ’ DiskSpaceUsed\n1 as BR Adeshnsdad 1 AReT ANN Ee —, 2 EE ——",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: web application is currently deployed on an On-Demand Linux EC2 instance that connects to an Amazon RDS database. Users have",
          "d: that the application crashes intermittently. The support team has reviewed the logs in CloudWatch but has been unable to",
          "d: entify the root cause. To enhance troubleshooting, the team needs to monitor additional metrics, including memory utilization, swap usage,",
          "a: nd the count of idle and active processes running on the instance.",
          "c: h solution would be the MOST appropriate to implement in this situation?",
          "d: etailed monitoring in CloudWatch.",
          "A: WS CloudShell to consolidate all metrics in a single dashboard.",
          "a: ll the AWS X-Ray daemon on the EC2 instance.",
          "a: ll the Amazon CloudWatch Logs agent to the EC2 instance.",
          "c: orrect",
          "c: an use the CloudWatch agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent",
          "b: oth Windows Server and Linux, and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU",
          "c: ore. Aside from the usual metrics, it also tracks the memory, swap, and disk space utilization metrics of your server.",
          "D: ashboard Unux System v Q X IK € 1n12c2Meia 3H",
          "A: larms",
          "a: nceld, MountPath",
          "a: nceld + InstanceName ~  MountPath + Metric Name -",
          "d: at 1-0e7410D pythonsampie-dev ’ DiskSpaceAvallable",
          "B: illing 1 idevivdal -d0e7410D pythonsampie-dev ’ DiskSpaceUsed",
          "a: s BR Adeshnsdad 1 AReT ANN Ee —, 2 EE ——"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "33. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA web application is currently deployed on an On-Demand Linux EC2 instance that connects to an Amazon RDS database. Users have\nfrequently reported that the application crashes intermittently. The support team has reviewed the logs in CloudWatch but has been unable to\nidentify the root cause. To enhance troubleshooting, the team needs to monitor additional metrics, including memory utilization, swap usage,\nand the count of idle and active processes running on the instance.\nWhich solution would be the MOST appropriate to implement in this situation?\nUse detailed monitoring in CloudWatch.\nUse AWS CloudShell to consolidate all metrics in a single dashboard.\n® Install the AWS X-Ray daemon on the EC2 instance.\nInstall the Amazon CloudWatch Logs agent to the EC2 instance.\nIncorrect\nYou can use the CloudWatch agent to collect both system metrics and log files from Amazon EC2 instances and on-premises servers. The agent\nsupports both Windows Server and Linux, and enables you to select the metrics to be collected, including sub-resource metrics such as per-CPU\ncore. Aside from the usual metrics, it also tracks the memory, swap, and disk space utilization metrics of your server.\nDashboard Unux System v Q X IK € 1n12c2Meia 3H\nAlarms\n4 LUinux System > Filesystem Instanceld, MountPath\nFilesystem ~  Instanceld + InstanceName ~  MountPath + Metric Name -\noK © | © /gevinvdat 1-0e7410D pythonsampie-dev ’ DiskSpaceAvallable\nBilling 1 idevivdal -d0e7410D pythonsampie-dev ’ DiskSpaceUsed\n1 as BR Adeshnsdad 1 AReT ANN Ee —, 2 EE ——"
      },
      "tags": {
        "services": [
          "EC2",
          "RDS",
          "X-Ray",
          "CloudWatch",
          "SAM"
        ],
        "domains": [
          "Security",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "monitoring"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 095547.png",
      "parsed": {
        "question": "Category: CDA - Security\nA developer runs a shell script that uses the aws s3 cp CLI to upload a large file to an S3 bucket. The S3 bucket is configured with Server- E\nside encryption with AWS Key Management Service (SSE-KMS). An Access Denied error always shows up whenever the developer uploads\na file with a size of 100 GB or more. However, whenever he uploads a smaller file, the request succeeds.\nWhich of the following are possible reasons why this issue is happening? (Select TWO.)\nThe developer does not have the kms:Encrypt permission.\nThe developer does not have the kms:Decrypt permission.\nThe developer's IAM permission has an attached inline policy that restricts him from uploading a file to S3 with a size of 100\nGB or more.\nThe AWS CLI S3 commands perform a multipart upload when the file is large.\nThe maximum size that can be encrypted in KMS is only 100 GB.\nIncorrect\nIf you are getting an Access Denied error when trying to upload a large file to your S3 bucket with an upload request that includes an AWS\nKMS key, then you have to confirm that you have permission to perform kms:Decrypt actions on the AWS KMS key that you're using to\nencrypt the object.\nEY n n am—",
        "options": [
          "C: ategory: CDA - Security",
          "A: developer runs a shell script that uses the aws s3 cp CLI to upload a large file to an S3 bucket. The S3 bucket is configured with Server- E",
          "d: e encryption with AWS Key Management Service (SSE-KMS). An Access Denied error always shows up whenever the developer uploads",
          "a: file with a size of 100 GB or more. However, whenever he uploads a smaller file, the request succeeds.",
          "c: h of the following are possible reasons why this issue is happening? (Select TWO.)",
          "d: eveloper does not have the kms:Encrypt permission.",
          "d: eveloper does not have the kms:Decrypt permission.",
          "d: eveloper's IAM permission has an attached inline policy that restricts him from uploading a file to S3 with a size of 100",
          "B: or more.",
          "A: WS CLI S3 commands perform a multipart upload when the file is large.",
          "a: ximum size that can be encrypted in KMS is only 100 GB.",
          "c: orrect",
          "a: re getting an Access Denied error when trying to upload a large file to your S3 bucket with an upload request that includes an AWS",
          "a: ve to confirm that you have permission to perform kms:Decrypt actions on the AWS KMS key that you're using to",
          "c: rypt the object.",
          "a: m—"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "34. QUESTION\nCategory: CDA - Security\nA developer runs a shell script that uses the aws s3 cp CLI to upload a large file to an S3 bucket. The S3 bucket is configured with Server- E\nside encryption with AWS Key Management Service (SSE-KMS). An Access Denied error always shows up whenever the developer uploads\na file with a size of 100 GB or more. However, whenever he uploads a smaller file, the request succeeds.\nWhich of the following are possible reasons why this issue is happening? (Select TWO.)\nThe developer does not have the kms:Encrypt permission.\nThe developer does not have the kms:Decrypt permission.\nThe developer's IAM permission has an attached inline policy that restricts him from uploading a file to S3 with a size of 100\nGB or more.\nThe AWS CLI S3 commands perform a multipart upload when the file is large.\nThe maximum size that can be encrypted in KMS is only 100 GB.\nIncorrect\nIf you are getting an Access Denied error when trying to upload a large file to your S3 bucket with an upload request that includes an AWS\nKMS key, then you have to confirm that you have permission to perform kms:Decrypt actions on the AWS KMS key that you're using to\nencrypt the object.\nEY n n am—"
      },
      "tags": {
        "services": [
          "S3",
          "IAM",
          "KMS",
          "Config",
          "ECR"
        ],
        "domains": [
          "Development with AWS Services",
          "Security"
        ],
        "keywords": [
          "security",
          "encryption",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 095835.png",
      "parsed": {
        "question": "®\nCategory: CDA - Deployment i:\nA single docker container environment is hosted in Elastic Beanstalk. Your manager instructed you to ensure that the compute resources\nmaintain full capacity during deployments to avoid any degradation of the service or possible down time.\nWhich of the following deployment methods should you use to satisfy the given requirement? (Select TWO.)\nRolling\nImmutable\nRolling with additional batch\nAll at once\nIncorrect\nIn ElasticBeanstalk, you can choose from a variety of deployment methods:\nAll at once - Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while\nthe deployment occurs.\nRolling — Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's\ncapacity by the number of instances in a batch.\nRolling with additional batch — Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the\ndeployment process.",
        "options": [
          "C: ategory: CDA - Deployment i:",
          "A: single docker container environment is hosted in Elastic Beanstalk. Your manager instructed you to ensure that the compute resources",
          "a: intain full capacity during deployments to avoid any degradation of the service or possible down time.",
          "c: h of the following deployment methods should you use to satisfy the given requirement? (Select TWO.)",
          "a: ble",
          "a: dditional batch",
          "A: ll at once",
          "c: orrect",
          "a: sticBeanstalk, you can choose from a variety of deployment methods:",
          "A: ll at once - Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while",
          "d: eployment occurs.",
          "D: eploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's",
          "c: apacity by the number of instances in a batch.",
          "a: dditional batch — Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the",
          "d: eployment process."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "39. QUESTION\n®\nCategory: CDA - Deployment i:\nA single docker container environment is hosted in Elastic Beanstalk. Your manager instructed you to ensure that the compute resources\nmaintain full capacity during deployments to avoid any degradation of the service or possible down time.\nWhich of the following deployment methods should you use to satisfy the given requirement? (Select TWO.)\nRolling\nImmutable\nRolling with additional batch\nAll at once\nIncorrect\nIn ElasticBeanstalk, you can choose from a variety of deployment methods:\nAll at once - Deploy the new version to all instances simultaneously. All instances in your environment are out of service for a short time while\nthe deployment occurs.\nRolling — Deploy the new version in batches. Each batch is taken out of service during the deployment phase, reducing your environment's\ncapacity by the number of instances in a batch.\nRolling with additional batch — Deploy the new version in batches, but first launch a new batch of instances to ensure full capacity during the\ndeployment process."
      },
      "tags": {
        "services": [
          "Elastic Beanstalk"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": [
          "container",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 095934.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an E\nSQS queue. The system works well but there are times that the consumers process the same message twice. Upon investigation, you found\nout that if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers,\nwhich causes duplicate processing.\nWhich of the following is the BEST solution that the developer should implement to meet this requirement?\nConfigure the queue to use short polling by setting the wWaitTimeSeconds parameter of the ReceiveMessage request to\n0.\nSet the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.\nPostpone the delivery of new messages by using a delay queue.\n® Configure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.\nIncorrect\nThe visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a\nmessage.\nWhen a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn’t automatically delete\nthe message. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example,\ndue to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after\nreceiving and processing it.\nReceiveMessage ReceiveMessage\nRequest ReceiveMessage ReceiveMessage Request\n] . Request Request .",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an E",
          "b: ut there are times that the consumers process the same message twice. Upon investigation, you found",
          "a: t if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers,",
          "c: h causes duplicate processing.",
          "c: h of the following is the BEST solution that the developer should implement to meet this requirement?",
          "C: onfigure the queue to use short polling by setting the wWaitTimeSeconds parameter of the ReceiveMessage request to",
          "b: ility timeout to the maximum time that it takes your application to process and delete a message from the queue.",
          "d: elivery of new messages by using a delay queue.",
          "C: onfigure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.",
          "c: orrect",
          "b: ility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a",
          "a: ge.",
          "a: consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn’t automatically delete",
          "a: ge. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example,",
          "d: ue to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after",
          "c: eiving and processing it.",
          "c: eiveMessage ReceiveMessage",
          "c: eiveMessage ReceiveMessage Request"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "41. QUESTION\nCategory: CDA - Development with AWS Services\nA batch application is hosted in an Auto Scaling group of On-Demand EC2 instances which consumes and processes the messages from an E\nSQS queue. The system works well but there are times that the consumers process the same message twice. Upon investigation, you found\nout that if the consumer takes a long time to process the message, that exact same message becomes available again to other consumers,\nwhich causes duplicate processing.\nWhich of the following is the BEST solution that the developer should implement to meet this requirement?\nConfigure the queue to use short polling by setting the wWaitTimeSeconds parameter of the ReceiveMessage request to\n0.\nSet the visibility timeout to the maximum time that it takes your application to process and delete a message from the queue.\nPostpone the delivery of new messages by using a delay queue.\n® Configure the queue to use long polling by setting the Receive Message Wait Time parameter to a value greater than 0.\nIncorrect\nThe visibility timeout is a period of time during which Amazon SQS prevents other consuming components from receiving and processing a\nmessage.\nWhen a consumer receives and processes a message from a queue, the message remains in the queue. Amazon SQS doesn’t automatically delete\nthe message. Because Amazon SQS is a distributed system, there's no guarantee that the consumer actually receives the message (for example,\ndue to a connectivity issue, or due to an issue in the consumer application). Thus, the consumer must delete the message from the queue after\nreceiving and processing it.\nReceiveMessage ReceiveMessage\nRequest ReceiveMessage ReceiveMessage Request\n] . Request Request ."
      },
      "tags": {
        "services": [
          "EC2",
          "SQS",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment"
        ],
        "keywords": [
          "scaling",
          "queue"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100013.png",
      "parsed": {
        "question": "[\nCategory: CDA - Development with AWS Services\nA software engineer is developing a serverless application which will use a DynamoDB database. One of the requirements is that each write\nrequest should return the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were\naffected by the operation.\nWhat should be done to accomplish this feature?\nAdd the ReturnConsumedCapacity parameter with a value of INDEXES in every write request.\n® Addthe Returnvalues parameter with a value of INDEXES in every write request.\nAdd the Returnvalues parameter with a value of TOTAL in every write request.\nAdd the ReturnConsumedCapacity parameter with a value of TOTAL in every write request.\nIncorrect\nTo create, update, or delete an item in a DynamoDB table, use one of the following operations:\n- PutItem\n— UpdateItem\n- Deleteltem\nFor each of these operations, you need to specify the entire primary key, not just part of it. For example, if a table has a composite primary key\n(partition key and sort key), you must supply a value for the partition key and a value for the sort key.\n— To return the number of write capacity units consumed by any of these operations, set the ReturnConsumedCapacity parameter to one of",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: software engineer is developing a serverless application which will use a DynamoDB database. One of the requirements is that each write",
          "d: return the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were",
          "a: ffected by the operation.",
          "a: t should be done to accomplish this feature?",
          "A: dd the ReturnConsumedCapacity parameter with a value of INDEXES in every write request.",
          "A: ddthe Returnvalues parameter with a value of INDEXES in every write request.",
          "A: dd the Returnvalues parameter with a value of TOTAL in every write request.",
          "A: dd the ReturnConsumedCapacity parameter with a value of TOTAL in every write request.",
          "c: orrect",
          "c: reate, update, or delete an item in a DynamoDB table, use one of the following operations:",
          "d: ateItem",
          "D: eleteltem",
          "a: ch of these operations, you need to specify the entire primary key, not just part of it. For example, if a table has a composite primary key",
          "a: rtition key and sort key), you must supply a value for the partition key and a value for the sort key.",
          "b: er of write capacity units consumed by any of these operations, set the ReturnConsumedCapacity parameter to one of"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "42. QUESTION [\nCategory: CDA - Development with AWS Services\nA software engineer is developing a serverless application which will use a DynamoDB database. One of the requirements is that each write\nrequest should return the total number of write capacity units consumed, with subtotals for the table and any secondary indexes that were\naffected by the operation.\nWhat should be done to accomplish this feature?\nAdd the ReturnConsumedCapacity parameter with a value of INDEXES in every write request.\n® Addthe Returnvalues parameter with a value of INDEXES in every write request.\nAdd the Returnvalues parameter with a value of TOTAL in every write request.\nAdd the ReturnConsumedCapacity parameter with a value of TOTAL in every write request.\nIncorrect\nTo create, update, or delete an item in a DynamoDB table, use one of the following operations:\n- PutItem\n— UpdateItem\n- Deleteltem\nFor each of these operations, you need to specify the entire primary key, not just part of it. For example, if a table has a composite primary key\n(partition key and sort key), you must supply a value for the partition key and a value for the sort key.\n— To return the number of write capacity units consumed by any of these operations, set the ReturnConsumedCapacity parameter to one of"
      },
      "tags": {
        "services": [
          "DynamoDB"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100106.png",
      "parsed": {
        "question": "Category: CDA - Deployment\nYou are using AWS Serverless Application Model (AWS SAM) to build and deploy applications in your serverless infrastructure. Your manager\ninstructed you to create a CloudFormation template that includes your SAM script and other service configurations. This template will be used\nto launch a similar infrastructure in another region.\nWhat should you do in order to accomplish this task?\nAdd a Parameters section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to\nuse.\n® Add a Resources section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to\nuse.\nAdd a Transform section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to\nuse.\nAdd a mappings section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.\nIncorrect\nAWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing\nthose resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you\nwant, and AWS CloudFormation takes care of provisioning and configuring those resources for you.\nA Cloudformation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. The template includes several sections\nfor you to define your infrastructure code.\nFor serverless applications (also referred to as Lambda-based applications), the Transform section specifies the version of the AWS",
        "options": [
          "C: ategory: CDA - Deployment",
          "a: re using AWS Serverless Application Model (AWS SAM) to build and deploy applications in your serverless infrastructure. Your manager",
          "c: ted you to create a CloudFormation template that includes your SAM script and other service configurations. This template will be used",
          "a: unch a similar infrastructure in another region.",
          "a: t should you do in order to accomplish this task?",
          "A: dd a Parameters section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to",
          "A: dd a Resources section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to",
          "A: dd a Transform section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to",
          "A: dd a mappings section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.",
          "c: orrect",
          "A: WS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing",
          "c: es and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you",
          "a: nt, and AWS CloudFormation takes care of provisioning and configuring those resources for you.",
          "A: Cloudformation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. The template includes several sections",
          "d: efine your infrastructure code.",
          "a: pplications (also referred to as Lambda-based applications), the Transform section specifies the version of the AWS"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "44. QUESTION\nCategory: CDA - Deployment\nYou are using AWS Serverless Application Model (AWS SAM) to build and deploy applications in your serverless infrastructure. Your manager\ninstructed you to create a CloudFormation template that includes your SAM script and other service configurations. This template will be used\nto launch a similar infrastructure in another region.\nWhat should you do in order to accomplish this task?\nAdd a Parameters section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to\nuse.\n® Add a Resources section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to\nuse.\nAdd a Transform section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to\nuse.\nAdd a mappings section in the template to specify the version of the AWS Serverless Application Model (AWS SAM) to use.\nIncorrect\nAWS CloudFormation is a service that helps you model and set up your Amazon Web Services resources so that you can spend less time managing\nthose resources and more time focusing on your applications that run in AWS. You create a template that describes all the AWS resources that you\nwant, and AWS CloudFormation takes care of provisioning and configuring those resources for you.\nA Cloudformation template is a JSON- or YAML-formatted text file that describes your AWS infrastructure. The template includes several sections\nfor you to define your infrastructure code.\nFor serverless applications (also referred to as Lambda-based applications), the Transform section specifies the version of the AWS"
      },
      "tags": {
        "services": [
          "Lambda",
          "CloudFormation",
          "Config",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100140.png",
      "parsed": {
        "question": "uest's method name\" I\n\"headers\": {String containing incoming request headers}\n\"multivValueHeaders\": {List of strings containing incoming request headers}\n\"queryStringParameters\": {query string parameters }\n\"multivalueQueryStringParameters\": {List of query string parameters}\n\"pathParameters\": {path parameters}\n\"stageVariables\": {Applicable stage variables}\n\"requestContext\": {Request context, including authorizer-returned key-value pairs}\n\"body\": \"A JSON string of the request payload.\"\n\"isBase64Encoded\": \"A boolean flag to indicate if the applicable request payload is Base64-encode\"\n}\nWhich of the following options is the MOST appropriate method to use to meet this requirement?\nLambda proxy integration\nLambda custom integration\n@® HTTP Proxy integration\nHTTP custom integration\nIncorrect\nYou choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the\nintegration endpoint. For a Lambda function, you can have two types of integration:\n— 1 amhbhda nrovv intearatinon",
        "options": [
          "c: e\": \"Resource path\",",
          "a: th\": \"Path parameter\",",
          "d: \": \"Incoming request's method name\" I",
          "a: ders\": {String containing incoming request headers}",
          "a: lueHeaders\": {List of strings containing incoming request headers}",
          "a: rameters\": {query string parameters }",
          "a: lueQueryStringParameters\": {List of query string parameters}",
          "a: thParameters\": {path parameters}",
          "a: geVariables\": {Applicable stage variables}",
          "C: ontext\": {Request context, including authorizer-returned key-value pairs}",
          "b: ody\": \"A JSON string of the request payload.\"",
          "B: ase64Encoded\": \"A boolean flag to indicate if the applicable request payload is Base64-encode\"",
          "c: h of the following options is the MOST appropriate method to use to meet this requirement?",
          "a: mbda proxy integration",
          "a: mbda custom integration",
          "a: tion",
          "c: ustom integration",
          "c: orrect",
          "c: hoose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the",
          "a: tion endpoint. For a Lambda function, you can have two types of integration:",
          "a: mhbhda nrovv intearatinon"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "\"resource\": \"Resource path\",\n\"path\": \"Path parameter\",\n\"httpMethod\": \"Incoming request's method name\" I\n\"headers\": {String containing incoming request headers}\n\"multivValueHeaders\": {List of strings containing incoming request headers}\n\"queryStringParameters\": {query string parameters }\n\"multivalueQueryStringParameters\": {List of query string parameters}\n\"pathParameters\": {path parameters}\n\"stageVariables\": {Applicable stage variables}\n\"requestContext\": {Request context, including authorizer-returned key-value pairs}\n\"body\": \"A JSON string of the request payload.\"\n\"isBase64Encoded\": \"A boolean flag to indicate if the applicable request payload is Base64-encode\"\n}\nWhich of the following options is the MOST appropriate method to use to meet this requirement?\nLambda proxy integration\nLambda custom integration\n@® HTTP Proxy integration\nHTTP custom integration\nIncorrect\nYou choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the\nintegration endpoint. For a Lambda function, you can have two types of integration:\n— 1 amhbhda nrovv intearatinon"
      },
      "tags": {
        "services": [
          "Lambda"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100246.png",
      "parsed": {
        "question": "uest for 1 hour.\nThe request will fail if the x-amz-meta-custom-header header is not included.\nThis configuration authorizes the user to perform actions on the S3 bucket.\nIncorrect\nCross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a\n| different domain With CORS sunnort vou can build rich client-side web aoblications with Amazon S22 and selectively allow cross-oriain access to",
        "options": [
          "c: oding=\"UTF-8\"?2>",
          "C: ORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">",
          "C: ORSRule>",
          "A: llowedOrigin>https://tutorialsdojo.com</AllowedOrigin>",
          "A: llowedMethod>GET</AllowedMethod>",
          "A: llowedMethod>PUT</AllowedMethod>",
          "A: llowedMethod>POST</AllowedMethod>",
          "A: llowedMethod>DELETE</AllowedMethod>",
          "A: llowedHeader>*</AllowedHeader>",
          "a: der>ETag</ExposeHeader>",
          "a: der>x-amz-meta-custom-header</ExposeHeader>",
          "a: xAgeSeconds>3600</MaxAgeSeconds>",
          "C: ORSRule>",
          "C: ORSConfiguration>",
          "c: h of the following statements are TRUE with regards to this S3 configuration? (Select TWO.)",
          "a: llows a user to view, add, remove or update objects inside the S3 bucket from the domain tutorialsdojo.com.",
          "A: ll HTTP Methods are allowed.",
          "c: ause the browser to cache the response of the preflight OPTIONS request for 1 hour.",
          "a: il if the x-amz-meta-custom-header header is not included.",
          "c: onfiguration authorizes the user to perform actions on the S3 bucket.",
          "c: orrect",
          "C: ross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a",
          "d: ifferent domain With CORS sunnort vou can build rich client-side web aoblications with Amazon S22 and selectively allow cross-oriain access to"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "<?xml version=\"1.0\" encoding=\"UTF-8\"?2>\n<CORSConfiguration xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n<CORSRule>\n<AllowedOrigin>https://tutorialsdojo.com</AllowedOrigin>\n<AllowedMethod>GET</AllowedMethod>\n<AllowedMethod>PUT</AllowedMethod>\n<AllowedMethod>POST</AllowedMethod>\n<AllowedMethod>DELETE</AllowedMethod>\n<AllowedHeader>*</AllowedHeader>\n<ExposeHeader>ETag</ExposeHeader>\n<ExposeHeader>x-amz-meta-custom-header</ExposeHeader>\n<MaxAgeSeconds>3600</MaxAgeSeconds>\n</CORSRule>\n</CORSConfiguration>\nWhich of the following statements are TRUE with regards to this S3 configuration? (Select TWO.)\nIt allows a user to view, add, remove or update objects inside the S3 bucket from the domain tutorialsdojo.com.\nAll HTTP Methods are allowed.\nThis will cause the browser to cache the response of the preflight OPTIONS request for 1 hour.\nThe request will fail if the x-amz-meta-custom-header header is not included.\nThis configuration authorizes the user to perform actions on the S3 bucket.\nIncorrect\nCross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a\n| different domain With CORS sunnort vou can build rich client-side web aoblications with Amazon S22 and selectively allow cross-oriain access to"
      },
      "tags": {
        "services": [
          "S3",
          "RDS",
          "Config"
        ],
        "domains": [
          "Development with AWS Services"
        ],
        "keywords": [
          "CORS"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100310.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nThe read and write operations to an Amazon DynamoDB table are throttled, causing errors in a stateful application that maintains user E\nsessions. Despite checking Amazon CloudWatch metrics, the consumed capacity units have not exceeded the provisioned capacity. Upon\nfurther investigation, it is found that a \"hot partition” is being accessed more frequently than others by downstream services.\nWhat should be done to resolve this issue with MINIMAL cost? (Select TWO.)\nUse DynamoDB Accelerator (DAX).\nIncrease the amount of read or write capacity for your table.\nImplement error retries and exponential backoff.\nImplement read sharding to distribute workloads evenly.\nRefactor your application to distribute your read and write operations as evenly as possible across your table.\nIncorrect\nPartitions are usually throttled when they are accessed by your downstream applications much more frequently than other partitions (that is, a\n“hot” partition), or when workloads rely on short periods of time with high usage (a “burst” of read or write activity). This problem can be more\npronounced in stateful applications, where maintaining session data or transactions may cause repeated access to the same partition. To avoid hot\npartitions and throttling, you must optimize your table and partition structure.\nDynamoDB adaptive capacity automatically boosts throughput capacity to high-traffic partitions. However, each partition is still subject to the hard\nlimit. This means that adaptive capacity can’t solve larger issues with your table or partition design. To avoid hot partitions and throttling, optimize\nyour table and partition structure.\n: CEE EER — ° ©",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "a: d and write operations to an Amazon DynamoDB table are throttled, causing errors in a stateful application that maintains user E",
          "D: espite checking Amazon CloudWatch metrics, the consumed capacity units have not exceeded the provisioned capacity. Upon",
          "a: tion, it is found that a \"hot partition” is being accessed more frequently than others by downstream services.",
          "a: t should be done to resolve this issue with MINIMAL cost? (Select TWO.)",
          "D: ynamoDB Accelerator (DAX).",
          "c: rease the amount of read or write capacity for your table.",
          "a: nd exponential backoff.",
          "a: d sharding to distribute workloads evenly.",
          "a: ctor your application to distribute your read and write operations as evenly as possible across your table.",
          "c: orrect",
          "a: rtitions are usually throttled when they are accessed by your downstream applications much more frequently than other partitions (that is, a",
          "a: rtition), or when workloads rely on short periods of time with high usage (a “burst” of read or write activity). This problem can be more",
          "c: ed in stateful applications, where maintaining session data or transactions may cause repeated access to the same partition. To avoid hot",
          "a: rtitions and throttling, you must optimize your table and partition structure.",
          "D: ynamoDB adaptive capacity automatically boosts throughput capacity to high-traffic partitions. However, each partition is still subject to the hard",
          "a: ns that adaptive capacity can’t solve larger issues with your table or partition design. To avoid hot partitions and throttling, optimize",
          "a: ble and partition structure.",
          "C: EE EER — ° ©"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "49. QUESTION\nCategory: CDA - Development with AWS Services\nThe read and write operations to an Amazon DynamoDB table are throttled, causing errors in a stateful application that maintains user E\nsessions. Despite checking Amazon CloudWatch metrics, the consumed capacity units have not exceeded the provisioned capacity. Upon\nfurther investigation, it is found that a \"hot partition” is being accessed more frequently than others by downstream services.\nWhat should be done to resolve this issue with MINIMAL cost? (Select TWO.)\nUse DynamoDB Accelerator (DAX).\nIncrease the amount of read or write capacity for your table.\nImplement error retries and exponential backoff.\nImplement read sharding to distribute workloads evenly.\nRefactor your application to distribute your read and write operations as evenly as possible across your table.\nIncorrect\nPartitions are usually throttled when they are accessed by your downstream applications much more frequently than other partitions (that is, a\n“hot” partition), or when workloads rely on short periods of time with high usage (a “burst” of read or write activity). This problem can be more\npronounced in stateful applications, where maintaining session data or transactions may cause repeated access to the same partition. To avoid hot\npartitions and throttling, you must optimize your table and partition structure.\nDynamoDB adaptive capacity automatically boosts throughput capacity to high-traffic partitions. However, each partition is still subject to the hard\nlimit. This means that adaptive capacity can’t solve larger issues with your table or partition design. To avoid hot partitions and throttling, optimize\nyour table and partition structure.\n: CEE EER — ° ©"
      },
      "tags": {
        "services": [
          "DynamoDB",
          "CloudWatch",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "throttling",
          "stateful"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100333.png",
      "parsed": {
        "question": "Category: CDA - Security\nA developer is tasked with automating the deployment of a new microservice in an ECS cluster using AWS CodeDeploy. The developer is E\nwriting the AppSpec file to instruct CodeDeploy on how to handle the deployment.\nWhich sets of properties are REQUIRED in the resources section to successfully deploy the microservice? (Select THREE.)\nContainerName\nTaskDefinition\nNetworkConfiguration\ntargetversion\nContainerPort\nIncorrect\nAn AppSpec file is a YAML or JSON formatted file used by AWS CodeDeploy to manage and direct the deployment process. It provides instructions\nregarding how the deployment service should handle application content and lifecycle event hooks. Depending on the type of deployment, the\ncontent and structure of the AppSpec file will differ, tailored to the specific requirements and nature of each service.\nFor ECS deployments, the resources section specifies the Amazon ECS service to deploy and has the following structure:",
        "options": [
          "C: ategory: CDA - Security",
          "A: developer is tasked with automating the deployment of a new microservice in an ECS cluster using AWS CodeDeploy. The developer is E",
          "A: ppSpec file to instruct CodeDeploy on how to handle the deployment.",
          "c: h sets of properties are REQUIRED in the resources section to successfully deploy the microservice? (Select THREE.)",
          "C: ontainerName",
          "a: skDefinition",
          "C: onfiguration",
          "a: rgetversion",
          "C: ontainerPort",
          "c: orrect",
          "A: n AppSpec file is a YAML or JSON formatted file used by AWS CodeDeploy to manage and direct the deployment process. It provides instructions",
          "a: rding how the deployment service should handle application content and lifecycle event hooks. Depending on the type of deployment, the",
          "c: ontent and structure of the AppSpec file will differ, tailored to the specific requirements and nature of each service.",
          "C: S deployments, the resources section specifies the Amazon ECS service to deploy and has the following structure:"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "50. QUESTION\nCategory: CDA - Security\nA developer is tasked with automating the deployment of a new microservice in an ECS cluster using AWS CodeDeploy. The developer is E\nwriting the AppSpec file to instruct CodeDeploy on how to handle the deployment.\nWhich sets of properties are REQUIRED in the resources section to successfully deploy the microservice? (Select THREE.)\nContainerName\nTaskDefinition\nNetworkConfiguration\ntargetversion\nContainerPort\nIncorrect\nAn AppSpec file is a YAML or JSON formatted file used by AWS CodeDeploy to manage and direct the deployment process. It provides instructions\nregarding how the deployment service should handle application content and lifecycle event hooks. Depending on the type of deployment, the\ncontent and structure of the AppSpec file will differ, tailored to the specific requirements and nature of each service.\nFor ECS deployments, the resources section specifies the Amazon ECS service to deploy and has the following structure:"
      },
      "tags": {
        "services": [
          "ECS",
          "CodeDeploy",
          "Config"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": [
          "container",
          "security",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100420.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization B\nA clickstream application uses Amazon Kinesis Data Stream for real-time processing. PutRecord API calls are being used by the producer to\nsend data to the stream. However, there are cases where the producer intermittently restarted while doing the processing, which resulted in\nsending the same data twice to the stream. This inadvertently causes duplication of entries in the data stream, which affects the processing of\nthe consumers.\nWhich of the following should you implement to resolve this issue?\n® Split shards of the data stream.\nAdd more shards.\nEmbed a primary key within the record.\nMerge shards of the data stream.\nIncorrect\nThere are two primary reasons why records may be delivered more than one time to your Amazon Kinesis Data Streams application: producer\nretries and consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times.\n=\n=E EVES\n= —\n[FE [SM\nn",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization B",
          "A: clickstream application uses Amazon Kinesis Data Stream for real-time processing. PutRecord API calls are being used by the producer to",
          "d: data to the stream. However, there are cases where the producer intermittently restarted while doing the processing, which resulted in",
          "d: ing the same data twice to the stream. This inadvertently causes duplication of entries in the data stream, which affects the processing of",
          "c: onsumers.",
          "c: h of the following should you implement to resolve this issue?",
          "a: rds of the data stream.",
          "A: dd more shards.",
          "b: ed a primary key within the record.",
          "a: rds of the data stream.",
          "c: orrect",
          "a: re two primary reasons why records may be delivered more than one time to your Amazon Kinesis Data Streams application: producer",
          "a: nd consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "51. QUESTION\nCategory: CDA - Troubleshooting and Optimization B\nA clickstream application uses Amazon Kinesis Data Stream for real-time processing. PutRecord API calls are being used by the producer to\nsend data to the stream. However, there are cases where the producer intermittently restarted while doing the processing, which resulted in\nsending the same data twice to the stream. This inadvertently causes duplication of entries in the data stream, which affects the processing of\nthe consumers.\nWhich of the following should you implement to resolve this issue?\n® Split shards of the data stream.\nAdd more shards.\nEmbed a primary key within the record.\nMerge shards of the data stream.\nIncorrect\nThere are two primary reasons why records may be delivered more than one time to your Amazon Kinesis Data Streams application: producer\nretries and consumer retries. Your application must anticipate and appropriately handle processing individual records multiple times.\n=\n=E EVES\n= —\n[FE [SM\nn"
      },
      "tags": {
        "services": [
          "RDS",
          "Kinesis",
          "SAM"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100528.png",
      "parsed": {
        "question": "uirements specified\nin the task definition. such as CPU and memory. Similarly. when vou scale down the task count, Amazon ECS must determine which tasks to",
        "options": [
          "a: cementStrategy”: [",
          "d: \": \"attribute:ecs.availability-zone\",",
          "a: d\"",
          "a: cementStrategy”: [",
          "a: ndom\"",
          "c: orrect",
          "a: task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified",
          "a: sk definition. such as CPU and memory. Similarly. when vou scale down the task count, Amazon ECS must determine which tasks to"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "o\n\"placementStrategy”: [\n{\n\"field\": \"attribute:ecs.availability-zone\",\n\"type\": \"spread\"\n}\n1\n\"placementStrategy”: [\n{\n\"type\": \"random\"\n}\n1\nIncorrect\nWhen a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified\nin the task definition. such as CPU and memory. Similarly. when vou scale down the task count, Amazon ECS must determine which tasks to"
      },
      "tags": {
        "services": [
          "EC2",
          "ECS"
        ],
        "domains": [],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100551.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nYou are developing a high-traffic online stocks trading application, which will be hosted in an ECS Cluster and will be accessed by thousands\nof investors for intraday stocks trading. Each task of the cluster should be evenly placed across multiple Availability Zones to avoid any service\ndisruptions.\nWhich of the following is the MOST suitable placementStrategy configuration that you should use in your task definition?\n\"placementStrategy”: [\n{\n\"field\": \"memory\",\n® )\n\"type\": \"binpack\"",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "a: re developing a high-traffic online stocks trading application, which will be hosted in an ECS Cluster and will be accessed by thousands",
          "a: day stocks trading. Each task of the cluster should be evenly placed across multiple Availability Zones to avoid any service",
          "d: isruptions.",
          "c: h of the following is the MOST suitable placementStrategy configuration that you should use in your task definition?",
          "a: cementStrategy”: [",
          "d: \": \"memory\",",
          "b: inpack\""
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "54. QUESTION\nCategory: CDA - Development with AWS Services\nYou are developing a high-traffic online stocks trading application, which will be hosted in an ECS Cluster and will be accessed by thousands\nof investors for intraday stocks trading. Each task of the cluster should be evenly placed across multiple Availability Zones to avoid any service\ndisruptions.\nWhich of the following is the MOST suitable placementStrategy configuration that you should use in your task definition?\n\"placementStrategy”: [\n{\n\"field\": \"memory\",\n® )\n\"type\": \"binpack\""
      },
      "tags": {
        "services": [
          "ECS",
          "Config"
        ],
        "domains": [],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100605.png",
      "parsed": {
        "question": "",
        "options": [
          "d: awolowo1 + © ps",
          "B: y default, tasks are randomly placed with RunTask or spread across Availability Zones with CreateService. Spread is typically used to achieve",
          "a: vailability by making sure that multiple copies of a task are scheduled across multiple instances based on attributes such as Availability",
          "A: task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be i",
          "c: ified when either running a task or creating a new service.",
          "A: mazon ECS supports the following task placement strategies:",
          "b: inpack - Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.",
          "a: ndom - Place tasks randomly.",
          "a: d - Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, instanceld, or host.",
          "C: 0 CJ",
          "a: : us-west-2b",
          "a: d strategy, contrary to the binpack strategy, tries to put your tasks on as many different instances as possible. It is typically used to",
          "a: chieve high availability and mitigate risks, by making sure that you don’t put all your task-eggs in the same instance-baskets. Spread across",
          "A: vailability Zones, therefore, is the default placement strategy used for services.",
          "a: d strategy, you must also indicate a field parameter. It is used to indicate the bins that you are considering. The accepted",
          "a: lues are instancelD, host, or a custom attribute key:value pairs such as attribute:ecs.availability-zone to balance tasks across zones. There are",
          "a: l AWS attributes that start with the ecs prefix, but you can be creative and create your own attributes.",
          "c: e, the task placement configuration which has a value of \"field\": \"attribute:ecs.availability-zone\", \"type\": \"spread\" is"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "dawolowo1 + © ps\nBy default, tasks are randomly placed with RunTask or spread across Availability Zones with CreateService. Spread is typically used to achieve\nhigh availability by making sure that multiple copies of a task are scheduled across multiple instances based on attributes such as Availability\nZones.\n°\nA task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be i\nspecified when either running a task or creating a new service.\nAmazon ECS supports the following task placement strategies:\nbinpack - Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.\nrandom - Place tasks randomly.\nspread - Place tasks evenly based on the specified value. Accepted values are attribute key-value pairs, instanceld, or host.\n[J 1\n“Io [420\n[] [\n[J o |\nJ\n[ Ll]\nOo C0 CJ\n|\nus-west-2a : us-west-2b\nThe spread strategy, contrary to the binpack strategy, tries to put your tasks on as many different instances as possible. It is typically used to\nachieve high availability and mitigate risks, by making sure that you don’t put all your task-eggs in the same instance-baskets. Spread across\nAvailability Zones, therefore, is the default placement strategy used for services.\nWhen using the spread strategy, you must also indicate a field parameter. It is used to indicate the bins that you are considering. The accepted\nvalues are instancelD, host, or a custom attribute key:value pairs such as attribute:ecs.availability-zone to balance tasks across zones. There are\nseveral AWS attributes that start with the ecs prefix, but you can be creative and create your own attributes.\nHence, the task placement configuration which has a value of \"field\": \"attribute:ecs.availability-zone\", \"type\": \"spread\" is"
      },
      "tags": {
        "services": [
          "ECS",
          "Config",
          "SAM"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": [
          "high availability"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100642.png",
      "parsed": {
        "question": "utput section of the stack’s template. To",
        "options": [
          "d: Type: Api",
          "a: th: /hello",
          "d: get",
          "d: Api:",
          "D: escription: \"API Gateway endpoint URL\"",
          "a: lue: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/tdojo/hello/\"",
          "c: h changes should be done so that the newly created API endpoint can be referenced to other stacks?",
          "A: dd the awWs::TInclude transform in the original template to directly import the HelloWorldFunction resource to other",
          "a: tes.",
          "c: lude the Export property in the original template’s outputs section. Then use the Fn::TImportvalue function in",
          "a: tes to retrieve the exported value.",
          "c: lude the Export property in the original template’s outputs section. Then use the Ref function in other templates",
          "d: value.",
          "c: ify HelloWorldapi as parameter when using the Fn::Importvalue function in other templates.",
          "c: orrect",
          "a: re information between stacks, export a stack’s output values. Other stacks that are in the same AWS account and region can import the",
          "d: values. For example, you might have a single networking stack that exports the IDs of a subnet and security group for public web",
          "a: cks with a public webserver can easily import those networking resources. You don’t need to hard code resource IDs in the stack’s",
          "a: te or pass IDs as input parameters. To export a stack’s output value, use the Export field in the Qutput section of the stack’s template. To"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "EE\nEvents:\nHelloWorld:\nType: Api\nProperties: E\nPath: /hello\nMethod: get\nOutputs:\n\nHelloWorldApi:\nDescription: \"API Gateway endpoint URL\"\nValue: !Sub \"https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/tdojo/hello/\"\n\nWhich changes should be done so that the newly created API endpoint can be referenced to other stacks?\n\n® Add the awWs::TInclude transform in the original template to directly import the HelloWorldFunction resource to other\ntemplates.\nInclude the Export property in the original template’s outputs section. Then use the Fn::TImportvalue function in\nother templates to retrieve the exported value.\nInclude the Export property in the original template’s outputs section. Then use the Ref function in other templates\nto retrieve the exported value.\nSpecify HelloWorldapi as parameter when using the Fn::Importvalue function in other templates.\n\nIncorrect\n\nTo share information between stacks, export a stack’s output values. Other stacks that are in the same AWS account and region can import the\n\nexported values. For example, you might have a single networking stack that exports the IDs of a subnet and security group for public web\n\nservers. Stacks with a public webserver can easily import those networking resources. You don’t need to hard code resource IDs in the stack’s\n\nJ template or pass IDs as input parameters. To export a stack’s output value, use the Export field in the Qutput section of the stack’s template. To"
      },
      "tags": {
        "services": [
          "EBS",
          "API Gateway",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Deployment"
        ],
        "keywords": [
          "serverless",
          "security",
          "subnet",
          "security group"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100734.png",
      "parsed": {
        "question": "|\nCategory: CDA - Development with AWS Services\nA global financial company has hundreds of users from all over the world who regularly upload terabytes of transactional data to a centralized\nAmazon S3 bucket. Users from different parts of the globe are experiencing delays in uploading data, which in turn affects processing times.\nThe goal is to improve data throughput and ensure consistently fast data transfer to the S3 bucket regardless of the user's location.\nWhich of the following should be used to satisfy the above requirement?\n® Amazon CloudFront\nAWS Transfer for SFTP\nAWS Direct Connect\nS3 Transfer Acceleration\nIncorrect\nAmazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3\nbucket. Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location,\ndata is routed to your Amazon S3 bucket over an optimized network path.\nDIRECT SNOWBALL STORAGE TECHNOLOGY KINESIS TRANSFER\nCONNECT GATEWAY PARTNERSHIPS FIREHOSE ACCELERATION\ndy TN yy dy dy",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: global financial company has hundreds of users from all over the world who regularly upload terabytes of transactional data to a centralized",
          "A: mazon S3 bucket. Users from different parts of the globe are experiencing delays in uploading data, which in turn affects processing times.",
          "a: l is to improve data throughput and ensure consistently fast data transfer to the S3 bucket regardless of the user's location.",
          "c: h of the following should be used to satisfy the above requirement?",
          "A: mazon CloudFront",
          "A: WS Transfer for SFTP",
          "A: WS Direct Connect",
          "a: nsfer Acceleration",
          "c: orrect",
          "A: mazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3",
          "b: ucket. Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location,",
          "d: ata is routed to your Amazon S3 bucket over an optimized network path.",
          "D: IRECT SNOWBALL STORAGE TECHNOLOGY KINESIS TRANSFER",
          "C: ONNECT GATEWAY PARTNERSHIPS FIREHOSE ACCELERATION",
          "d: y TN yy dy dy"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "58. QUESTION |\nCategory: CDA - Development with AWS Services\nA global financial company has hundreds of users from all over the world who regularly upload terabytes of transactional data to a centralized\nAmazon S3 bucket. Users from different parts of the globe are experiencing delays in uploading data, which in turn affects processing times.\nThe goal is to improve data throughput and ensure consistently fast data transfer to the S3 bucket regardless of the user's location.\nWhich of the following should be used to satisfy the above requirement?\n® Amazon CloudFront\nAWS Transfer for SFTP\nAWS Direct Connect\nS3 Transfer Acceleration\nIncorrect\nAmazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and your Amazon S3\nbucket. Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location,\ndata is routed to your Amazon S3 bucket over an optimized network path.\nDIRECT SNOWBALL STORAGE TECHNOLOGY KINESIS TRANSFER\nCONNECT GATEWAY PARTNERSHIPS FIREHOSE ACCELERATION\ndy TN yy dy dy"
      },
      "tags": {
        "services": [
          "S3",
          "CloudFront",
          "Direct Connect",
          "Kinesis"
        ],
        "domains": [
          "Development with AWS Services"
        ],
        "keywords": []
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100755.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nA developer is creating a React application whose source code is hosted in GitHub. To help ensure proper functionality and identify any Ul [\nissues before going live, the developer must perform end-to-end (E2E) testing using Cypress.\nWhich combination of actions should the developer take? (Select Two)\nUpdate the amplify.yml file with appropriate configuration settings for Cypress.\nConnect the Github repository to AWS Amplify Hosting\nUpdate the amplifyconfiguration.json with appropriate configuration settings for Cypress.\nInclude the location of the Cypress configuration file in the aws-exports.js file.\nCreate an application in AWS Amplify Studio. Clone the application's source code in a local environment and run amplify\npull --appId APP _ID --envName ENV NAME\nIncorrect\nAWS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack\napplications on AWS.\nAmplify provides two services:\n-Amplify Hosting — provides a git-based workflow for hosting full-stack serverless web apps with continuous deployment.\n-Amplify Studio - a visual development environment that simplifies the creation of scalable, full-stack web and mobile apps. Use can use\n= Amplify Studio to build your frontend Ul with a set of ready-to-use Ul components, create an app backend with AWS resources, and then",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "A: developer is creating a React application whose source code is hosted in GitHub. To help ensure proper functionality and identify any Ul [",
          "b: efore going live, the developer must perform end-to-end (E2E) testing using Cypress.",
          "c: h combination of actions should the developer take? (Select Two)",
          "d: ate the amplify.yml file with appropriate configuration settings for Cypress.",
          "C: onnect the Github repository to AWS Amplify Hosting",
          "d: ate the amplifyconfiguration.json with appropriate configuration settings for Cypress.",
          "c: lude the location of the Cypress configuration file in the aws-exports.js file.",
          "C: reate an application in AWS Amplify Studio. Clone the application's source code in a local environment and run amplify",
          "a: ppId APP _ID --envName ENV NAME",
          "c: orrect",
          "A: WS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack",
          "a: pplications on AWS.",
          "A: mplify provides two services:",
          "A: mplify Hosting — provides a git-based workflow for hosting full-stack serverless web apps with continuous deployment.",
          "A: mplify Studio - a visual development environment that simplifies the creation of scalable, full-stack web and mobile apps. Use can use",
          "A: mplify Studio to build your frontend Ul with a set of ready-to-use Ul components, create an app backend with AWS resources, and then"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "59. QUESTION\nCategory: CDA - Development with AWS Services\nA developer is creating a React application whose source code is hosted in GitHub. To help ensure proper functionality and identify any Ul [\nissues before going live, the developer must perform end-to-end (E2E) testing using Cypress.\nWhich combination of actions should the developer take? (Select Two)\nUpdate the amplify.yml file with appropriate configuration settings for Cypress.\nConnect the Github repository to AWS Amplify Hosting\nUpdate the amplifyconfiguration.json with appropriate configuration settings for Cypress.\nInclude the location of the Cypress configuration file in the aws-exports.js file.\nCreate an application in AWS Amplify Studio. Clone the application's source code in a local environment and run amplify\npull --appId APP _ID --envName ENV NAME\nIncorrect\nAWS Amplify is a set of purpose-built tools and features that enables frontend web and mobile developers to quickly and easily build full-stack\napplications on AWS.\nAmplify provides two services:\n-Amplify Hosting — provides a git-based workflow for hosting full-stack serverless web apps with continuous deployment.\n-Amplify Studio - a visual development environment that simplifies the creation of scalable, full-stack web and mobile apps. Use can use\n= Amplify Studio to build your frontend Ul with a set of ready-to-use Ul components, create an app backend with AWS resources, and then"
      },
      "tags": {
        "services": [
          "Config",
          "Amplify"
        ],
        "domains": [],
        "keywords": [
          "serverless",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100840.png",
      "parsed": {
        "question": "E\nCategory: CDA - Deployment\nAn application architect manages several AWS accounts for staging, testing, and production environments, which are used by several\ndevelopment teams. For application deployments, the developers use the similar base CloudFormation template for their applications.\nWhich of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal\neffort?\n® Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts.\nCreate and manage stacks on multiple AWS accounts using CloudFormation Change Sets.\nUpdate the stacks on multiple AWS accounts using CloudFormation StackSets.\nDefine and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.\nIncorrect\nAWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts\nand regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the\ntemplate as the basis for provisioning stacks into selected target accounts across specified regions.\nBN —\nBN ——\nBH —\nStack set",
        "options": [
          "A: ________ 4 4 4",
          "C: ategory: CDA - Deployment",
          "A: n application architect manages several AWS accounts for staging, testing, and production environments, which are used by several",
          "d: evelopment teams. For application deployments, the developers use the similar base CloudFormation template for their applications.",
          "c: h of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal",
          "A: WS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts.",
          "C: reate and manage stacks on multiple AWS accounts using CloudFormation Change Sets.",
          "d: ate the stacks on multiple AWS accounts using CloudFormation StackSets.",
          "D: efine and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.",
          "c: orrect",
          "A: WS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts",
          "a: nd regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the",
          "a: te as the basis for provisioning stacks into selected target accounts across specified regions.",
          "B: N —",
          "B: N ——",
          "B: H —",
          "a: ck set"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "A ________ 4 4 4\n61. QUESTION E\nCategory: CDA - Deployment\nAn application architect manages several AWS accounts for staging, testing, and production environments, which are used by several\ndevelopment teams. For application deployments, the developers use the similar base CloudFormation template for their applications.\nWhich of the following can allow the developer to effectively manage the updates on this template across all AWS accounts with minimal\neffort?\n® Use AWS CodePipeline to automate the deployment of CloudFormation templates across multiple accounts.\nCreate and manage stacks on multiple AWS accounts using CloudFormation Change Sets.\nUpdate the stacks on multiple AWS accounts using CloudFormation StackSets.\nDefine and manage stack instances on multiple AWS Accounts using CloudFormation Stack Instances.\nIncorrect\nAWS CloudFormation StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts\nand regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the\ntemplate as the basis for provisioning stacks into selected target accounts across specified regions.\nBN —\nBN ——\nBH —\nStack set"
      },
      "tags": {
        "services": [
          "CodePipeline",
          "CloudFormation"
        ],
        "domains": [
          "Deployment"
        ],
        "keywords": [
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 100923.png",
      "parsed": {
        "question": "|\nCategory: CDA - Deployment\nAn EBS-backed EC2 instance has been recently reported to contain a malware that could spread to your other instances. To fix this security\nvulnerability, you will need to attach its root EBS volume to a new EC2 instance which hosts a security program that can scan viruses, worms,\nTrojan horses, or spyware.\nWhat steps would you take to detach the root volume from the compromised EC2 instance?\nStop the instance then detach the volume.\n@® Unmount the volume, stop the instance, and then detach.\nUnmount the volume from the OS and then detach.\nDetach the volume from the AWS Console. AWS takes care of unmounting the volume for you.\nIncorrect\nYou can detach an Amazon EBS volume from an instance explicitly or by terminating the instance. However, if the instance is running, you must\nfirst unmount the volume from the instance.\nIf an EBS volume is the root device of an instance, you must stop the instance before you can detach the volume.\nThe options that say unmount the volume from the OS and then detach and unmount the volume, stop the instance, and then detach are both\nincorrect because you can’t unmount the root volume on a running instance.\nThe option that says: Detach the volume from the AWS Console. AWS takes care of unmounting the volume for you is incorrect because\nunmounting the volume is not managed by AWS.\n]",
        "options": [
          "A: 4 4 A 4",
          "C: ategory: CDA - Deployment",
          "A: n EBS-backed EC2 instance has been recently reported to contain a malware that could spread to your other instances. To fix this security",
          "a: bility, you will need to attach its root EBS volume to a new EC2 instance which hosts a security program that can scan viruses, worms,",
          "a: n horses, or spyware.",
          "a: t steps would you take to detach the root volume from the compromised EC2 instance?",
          "a: nce then detach the volume.",
          "a: nce, and then detach.",
          "a: nd then detach.",
          "D: etach the volume from the AWS Console. AWS takes care of unmounting the volume for you.",
          "c: orrect",
          "c: an detach an Amazon EBS volume from an instance explicitly or by terminating the instance. However, if the instance is running, you must",
          "a: nce.",
          "a: n EBS volume is the root device of an instance, you must stop the instance before you can detach the volume.",
          "a: t say unmount the volume from the OS and then detach and unmount the volume, stop the instance, and then detach are both",
          "c: orrect because you can’t unmount the root volume on a running instance.",
          "a: t says: Detach the volume from the AWS Console. AWS takes care of unmounting the volume for you is incorrect because",
          "a: naged by AWS."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "A 4 4 A 4\n63. QUESTION |\nCategory: CDA - Deployment\nAn EBS-backed EC2 instance has been recently reported to contain a malware that could spread to your other instances. To fix this security\nvulnerability, you will need to attach its root EBS volume to a new EC2 instance which hosts a security program that can scan viruses, worms,\nTrojan horses, or spyware.\nWhat steps would you take to detach the root volume from the compromised EC2 instance?\nStop the instance then detach the volume.\n@® Unmount the volume, stop the instance, and then detach.\nUnmount the volume from the OS and then detach.\nDetach the volume from the AWS Console. AWS takes care of unmounting the volume for you.\nIncorrect\nYou can detach an Amazon EBS volume from an instance explicitly or by terminating the instance. However, if the instance is running, you must\nfirst unmount the volume from the instance.\nIf an EBS volume is the root device of an instance, you must stop the instance before you can detach the volume.\nThe options that say unmount the volume from the OS and then detach and unmount the volume, stop the instance, and then detach are both\nincorrect because you can’t unmount the root volume on a running instance.\nThe option that says: Detach the volume from the AWS Console. AWS takes care of unmounting the volume for you is incorrect because\nunmounting the volume is not managed by AWS.\n]"
      },
      "tags": {
        "services": [
          "EC2",
          "EBS"
        ],
        "domains": [],
        "keywords": [
          "security",
          "deployment"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 102254.png",
      "parsed": {
        "question": "Category: CDA - Security\n®\n\nA multinational investment bank has a hybrid cloud architecture with AWS. To improve the security of the applications, the company decided to\nuse AWS Key Management Service (KMS) with customer-managed keys to create and manage the encryption keys across a wide range of AWS\nservices. A software developer has been assigned to integrate AWS KMS with the financial applications of the company.\nWhich of the following are the recommended steps to locally encrypt data using AWS KMS that should be followed? (Select TWO.)\n\nUse the GenerateDataKeyWithoutPlaintext operation to get a data encryption key then use the plaintext data key in the\n\nresponse to encrypt data locally.\n\nUse the GenerateDataKey operation to get a data encryption key then use the plaintext data key in the response to encrypt\n\ndata locally.\n\nErase the encrypted data key from memory and store the plaintext data key alongside the locally encrypted data.\n\nErase the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data.\n\nEncrypt data locally using the Encrypt operation.",
        "options": [
          "C: ategory: CDA - Security",
          "A: multinational investment bank has a hybrid cloud architecture with AWS. To improve the security of the applications, the company decided to",
          "A: WS Key Management Service (KMS) with customer-managed keys to create and manage the encryption keys across a wide range of AWS",
          "c: es. A software developer has been assigned to integrate AWS KMS with the financial applications of the company.",
          "c: h of the following are the recommended steps to locally encrypt data using AWS KMS that should be followed? (Select TWO.)",
          "a: teDataKeyWithoutPlaintext operation to get a data encryption key then use the plaintext data key in the",
          "c: rypt data locally.",
          "a: teDataKey operation to get a data encryption key then use the plaintext data key in the response to encrypt",
          "d: ata locally.",
          "a: se the encrypted data key from memory and store the plaintext data key alongside the locally encrypted data.",
          "a: se the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data.",
          "c: rypt data locally using the Encrypt operation."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "2. QUESTION\nCategory: CDA - Security\n®\n\nA multinational investment bank has a hybrid cloud architecture with AWS. To improve the security of the applications, the company decided to\nuse AWS Key Management Service (KMS) with customer-managed keys to create and manage the encryption keys across a wide range of AWS\nservices. A software developer has been assigned to integrate AWS KMS with the financial applications of the company.\nWhich of the following are the recommended steps to locally encrypt data using AWS KMS that should be followed? (Select TWO.)\n\nUse the GenerateDataKeyWithoutPlaintext operation to get a data encryption key then use the plaintext data key in the\n\nresponse to encrypt data locally.\n\nUse the GenerateDataKey operation to get a data encryption key then use the plaintext data key in the response to encrypt\n\ndata locally.\n\nErase the encrypted data key from memory and store the plaintext data key alongside the locally encrypted data.\n\nErase the plaintext data key from memory and store the encrypted data key alongside the locally encrypted data.\n\nEncrypt data locally using the Encrypt operation."
      },
      "tags": {
        "services": [
          "KMS"
        ],
        "domains": [
          "Security"
        ],
        "keywords": [
          "security",
          "encryption"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 102347.png",
      "parsed": {
        "question": "Category: CDA - Development with AWS Services\nYou are designing the DynamoDB table that will be used by your Node.js application. It will have to handle 10 writes per second and then 20\neventually consistent reads per second where all the items have a size of 2 KB for both operations.\nWhich of the following are the most optimal WCU and RCU that you should provision to the table?\n20 RCU and 20 WCU\n® 40RCU and 20 WCU\n10 RCU and 20 WCU\n40 RCU and 40 WCU\nIncorrect\nWhen you create a new provisioned table in DynamoDB, you must specify its provisioned throughput capacity—the amount of read and write\nactivity that the table will be able to support. DynamoDB uses this information to reserve sufficient system resources to meet your throughput\nrequirements.\nYou can optionally allow DynamoDB auto-scaling to manage your table's throughput capacity. However, you still must provide initial settings for\nread and write capacity when you create the table. DynamoDB auto scaling uses these initial settings as a starting point and then adjusts them\ndynamically in response to your application's requirements. You specify throughput requirements in terms of capacity units—the amount of data\nyour application needs to read or write per second. You can modify these settings later, if needed, or enable DynamoDB auto-scaling to modify\nthem automatically.\nCapacity calculator\nAvg. item size KB",
        "options": [
          "C: ategory: CDA - Development with AWS Services",
          "a: re designing the DynamoDB table that will be used by your Node.js application. It will have to handle 10 writes per second and then 20",
          "a: lly consistent reads per second where all the items have a size of 2 KB for both operations.",
          "c: h of the following are the most optimal WCU and RCU that you should provision to the table?",
          "C: U and 20 WCU",
          "C: U and 20 WCU",
          "C: U and 20 WCU",
          "C: U and 40 WCU",
          "c: orrect",
          "c: reate a new provisioned table in DynamoDB, you must specify its provisioned throughput capacity—the amount of read and write",
          "a: ctivity that the table will be able to support. DynamoDB uses this information to reserve sufficient system resources to meet your throughput",
          "c: an optionally allow DynamoDB auto-scaling to manage your table's throughput capacity. However, you still must provide initial settings for",
          "a: d and write capacity when you create the table. DynamoDB auto scaling uses these initial settings as a starting point and then adjusts them",
          "d: ynamically in response to your application's requirements. You specify throughput requirements in terms of capacity units—the amount of data",
          "a: pplication needs to read or write per second. You can modify these settings later, if needed, or enable DynamoDB auto-scaling to modify",
          "a: utomatically.",
          "C: apacity calculator",
          "A: vg. item size KB"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "3. QUESTION\nCategory: CDA - Development with AWS Services\nYou are designing the DynamoDB table that will be used by your Node.js application. It will have to handle 10 writes per second and then 20\neventually consistent reads per second where all the items have a size of 2 KB for both operations.\nWhich of the following are the most optimal WCU and RCU that you should provision to the table?\n20 RCU and 20 WCU\n® 40RCU and 20 WCU\n10 RCU and 20 WCU\n40 RCU and 40 WCU\nIncorrect\nWhen you create a new provisioned table in DynamoDB, you must specify its provisioned throughput capacity—the amount of read and write\nactivity that the table will be able to support. DynamoDB uses this information to reserve sufficient system resources to meet your throughput\nrequirements.\nYou can optionally allow DynamoDB auto-scaling to manage your table's throughput capacity. However, you still must provide initial settings for\nread and write capacity when you create the table. DynamoDB auto scaling uses these initial settings as a starting point and then adjusts them\ndynamically in response to your application's requirements. You specify throughput requirements in terms of capacity units—the amount of data\nyour application needs to read or write per second. You can modify these settings later, if needed, or enable DynamoDB auto-scaling to modify\nthem automatically.\nCapacity calculator\nAvg. item size KB"
      },
      "tags": {
        "services": [
          "DynamoDB"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "scaling"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 102417.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization E\nA company has an application hosted in an ECS Cluster that heavily uses an RDS database. A developer needs to closely monitor how the\ndifferent processes on a DB instance use the CPU, such as the percentage of the CPU bandwidth or the total memory consumed by each\nprocess to ensure application performance.\nWhich of the following is the MOST suitable solution that the developer should implement?\nUse Enhanced Monitoring in RDS.\nDevelop a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of\nthe RDS instance.\n@® Use CloudWatch to track the CPU Utilization of your database.\nTrack the cpus and MEMS metrics which are readily available in the Amazon RDS console.\nIncorrect\nAmazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB\ninstance using the console or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By\ndefault, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the\nCloudWatch Logs, change the retention for the RDSOSMetrics log group in the CloudWatch console.\nProcess List\nQ Filter process list\n_",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization E",
          "A: company has an application hosted in an ECS Cluster that heavily uses an RDS database. A developer needs to closely monitor how the",
          "d: ifferent processes on a DB instance use the CPU, such as the percentage of the CPU bandwidth or the total memory consumed by each",
          "c: ess to ensure application performance.",
          "c: h of the following is the MOST suitable solution that the developer should implement?",
          "a: nced Monitoring in RDS.",
          "D: evelop a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of",
          "D: S instance.",
          "C: loudWatch to track the CPU Utilization of your database.",
          "a: ck the cpus and MEMS metrics which are readily available in the Amazon RDS console.",
          "c: orrect",
          "A: mazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB",
          "a: nce using the console or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By",
          "d: efault, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the",
          "C: loudWatch Logs, change the retention for the RDSOSMetrics log group in the CloudWatch console.",
          "c: ess List",
          "c: ess list"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "4. QUESTION\nCategory: CDA - Troubleshooting and Optimization E\nA company has an application hosted in an ECS Cluster that heavily uses an RDS database. A developer needs to closely monitor how the\ndifferent processes on a DB instance use the CPU, such as the percentage of the CPU bandwidth or the total memory consumed by each\nprocess to ensure application performance.\nWhich of the following is the MOST suitable solution that the developer should implement?\nUse Enhanced Monitoring in RDS.\nDevelop a shell script that collects and publishes custom metrics to CloudWatch which tracks the real-time CPU Utilization of\nthe RDS instance.\n@® Use CloudWatch to track the CPU Utilization of your database.\nTrack the cpus and MEMS metrics which are readily available in the Amazon RDS console.\nIncorrect\nAmazon RDS provides metrics in real time for the operating system (OS) that your DB instance runs on. You can view the metrics for your DB\ninstance using the console or consume the Enhanced Monitoring JSON output from CloudWatch Logs in a monitoring system of your choice. By\ndefault, Enhanced Monitoring metrics are stored in the CloudWatch Logs for 30 days. To modify the amount of time the metrics are stored in the\nCloudWatch Logs, change the retention for the RDSOSMetrics log group in the CloudWatch console.\nProcess List\nQ Filter process list\n_"
      },
      "tags": {
        "services": [
          "ECS",
          "RDS",
          "CloudWatch"
        ],
        "domains": [
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "performance",
          "monitoring"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 102712.png",
      "parsed": {
        "question": "§\nCategory: CDA - Security\nA software engineer is building a serverless application in AWS consisting of Lambda, API Gateway, and DynamoDB. She needs to implement a\ncustom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML to determine the caller's identity.\n\nWhich of the features of API Gateway is the MOST suitable one that she should use to build this feature?\nCross-Origin Resource Sharing (CORS)\nResource Policy\n® Cross-Account Lambda Authorizer\nLambda Authorizers\nIncorrect\nA Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of\nyour API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.\nThere are two types of Lambda authorizers:\n— A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web\nToken (JWT) or an OAuth token.\n— A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of\nheaders, query string parameters, stageVariables, and $context variables.\nSearch [2] a T Mm (m] g Qa | B [2] [= © > 0B @QID 17/01/",
        "options": [
          "D: &",
          "C: ategory: CDA - Security",
          "A: software engineer is building a serverless application in AWS consisting of Lambda, API Gateway, and DynamoDB. She needs to implement a",
          "c: ustom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML to determine the caller's identity.",
          "c: h of the features of API Gateway is the MOST suitable one that she should use to build this feature?",
          "C: ross-Origin Resource Sharing (CORS)",
          "c: e Policy",
          "C: ross-Account Lambda Authorizer",
          "a: mbda Authorizers",
          "c: orrect",
          "A: Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of",
          "A: PI's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.",
          "a: re two types of Lambda authorizers:",
          "A: token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web",
          "a: n OAuth token.",
          "A: request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of",
          "a: ders, query string parameters, stageVariables, and $context variables.",
          "a: rch [2] a T Mm (m] g Qa | B [2] [= © > 0B @QID 17/01/"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "ED &\n8. QUESTION §\nCategory: CDA - Security\nA software engineer is building a serverless application in AWS consisting of Lambda, API Gateway, and DynamoDB. She needs to implement a\ncustom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML to determine the caller's identity.\n\nWhich of the features of API Gateway is the MOST suitable one that she should use to build this feature?\nCross-Origin Resource Sharing (CORS)\nResource Policy\n® Cross-Account Lambda Authorizer\nLambda Authorizers\nIncorrect\nA Lambda authorizer is an API Gateway feature that uses a Lambda function to control access to your API. When a client makes a request to one of\nyour API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.\nThere are two types of Lambda authorizers:\n— A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web\nToken (JWT) or an OAuth token.\n— A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of\nheaders, query string parameters, stageVariables, and $context variables.\nSearch [2] a T Mm (m] g Qa | B [2] [= © > 0B @QID 17/01/"
      },
      "tags": {
        "services": [
          "Lambda",
          "DynamoDB",
          "API Gateway",
          "IAM",
          "SAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "serverless",
          "security",
          "CORS",
          "authentication",
          "authorization",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 102758.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization .\nIn the next financial year, a company has decided to develop a completely new version of its legacy application that will utilize Node.js and i\nGraphQL. The new architecture aims to offer an end-to-end view of requests as they traverse the application and display a map of the\nunderlying components.\nTo achieve this, the application will be hosted in an Auto Scaling group (ASG) of Linux EC2 instances behind an Application Load Balancer\n(ALB) and must be instrumented to send trace data to the AWS X-Ray.\nWhich of the following options is the MOST suitable way to satisfy this requirement?\nEnable AWS Web Application Firewall (WAF) on the ALB to monitor web requests.\nUse a user data script to install the X-Ray daemon.\nEnable AWS X-Ray tracing on the ASG's launch template.\n® Refactor your application to send segment documents directly to X-Ray by using the PutTracesegments API.\nIncorrect\nThe AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the\nSDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the\ndaemon alongside your application.\nTo properly instrument your application hosted in an EC2 instance, you have to install the X-Ray daemon by using a user data script. This will\ninstall and run the daemon automatically when you launch the instance. To use the daemon on Amazon EC2, create a new instance profile role or\nadd the managed policy to an existing one. This will grant the daemon permission to upload trace data to X-Ray.\nRun the AWS X-Ray daemon",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization .",
          "a: ncial year, a company has decided to develop a completely new version of its legacy application that will utilize Node.js and i",
          "a: phQL. The new architecture aims to offer an end-to-end view of requests as they traverse the application and display a map of the",
          "d: erlying components.",
          "a: chieve this, the application will be hosted in an Auto Scaling group (ASG) of Linux EC2 instances behind an Application Load Balancer",
          "A: LB) and must be instrumented to send trace data to the AWS X-Ray.",
          "c: h of the following options is the MOST suitable way to satisfy this requirement?",
          "a: ble AWS Web Application Firewall (WAF) on the ALB to monitor web requests.",
          "a: user data script to install the X-Ray daemon.",
          "a: ble AWS X-Ray tracing on the ASG's launch template.",
          "a: ctor your application to send segment documents directly to X-Ray by using the PutTracesegments API.",
          "c: orrect",
          "A: WS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the",
          "D: K sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the",
          "d: aemon alongside your application.",
          "a: pplication hosted in an EC2 instance, you have to install the X-Ray daemon by using a user data script. This will",
          "a: ll and run the daemon automatically when you launch the instance. To use the daemon on Amazon EC2, create a new instance profile role or",
          "a: dd the managed policy to an existing one. This will grant the daemon permission to upload trace data to X-Ray.",
          "A: WS X-Ray daemon"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "10. QUESTION\nCategory: CDA - Troubleshooting and Optimization .\nIn the next financial year, a company has decided to develop a completely new version of its legacy application that will utilize Node.js and i\nGraphQL. The new architecture aims to offer an end-to-end view of requests as they traverse the application and display a map of the\nunderlying components.\nTo achieve this, the application will be hosted in an Auto Scaling group (ASG) of Linux EC2 instances behind an Application Load Balancer\n(ALB) and must be instrumented to send trace data to the AWS X-Ray.\nWhich of the following options is the MOST suitable way to satisfy this requirement?\nEnable AWS Web Application Firewall (WAF) on the ALB to monitor web requests.\nUse a user data script to install the X-Ray daemon.\nEnable AWS X-Ray tracing on the ASG's launch template.\n® Refactor your application to send segment documents directly to X-Ray by using the PutTracesegments API.\nIncorrect\nThe AWS X-Ray SDK does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the\nSDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches. Use a script to run the\ndaemon alongside your application.\nTo properly instrument your application hosted in an EC2 instance, you have to install the X-Ray daemon by using a user data script. This will\ninstall and run the daemon automatically when you launch the instance. To use the daemon on Amazon EC2, create a new instance profile role or\nadd the managed policy to an existing one. This will grant the daemon permission to upload trace data to X-Ray.\nRun the AWS X-Ray daemon"
      },
      "tags": {
        "services": [
          "EC2",
          "ALB",
          "WAF",
          "X-Ray"
        ],
        "domains": [
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "scaling",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 103014.png",
      "parsed": {
        "question": "E\nCategory: CDA - Deployment\nA developer is writing a CloudFormation template which will be used to deploy a simple Lambda function to AWS. The function to be deployed\nis made in Python with just 3 lines of codes which can be written inline in the template.\nWhich parameter of the AWS: :Lambda::Function resource should the developer use to place the Python code in the template?\nZipFile\nHandler\nCode\nIncorrect\nTo create a Lambda function you first create a Lambda function deployment package, a .zip or .jar file consisting of your code and any\ndependencies. When creating the zip, include only the code and its dependencies, not the containing folder. You will then need to set the\nappropriate security permissions for the zip package.\nIf you are using a CloudFormation template, you can configure the AWS: :Lambda::Function resource which creates a Lambda function. To\ncreate a function, you need a deployment package and an execution role. The deployment package contains your function code. The execution\nrole grants the function permission to use AWS services, such as Amazon CloudWatch Logs for log streaming and AWS X-Ray for request tracing.\nManagedPolicyArns:\n— arn:aws:iam::aws:policy/AWSLambdaExecute\n— arn:aws:iam::aws:policy/AmazonS3FullAccess",
        "options": [
          "C: EEEERERRR EE EER",
          "C: ategory: CDA - Deployment",
          "A: developer is writing a CloudFormation template which will be used to deploy a simple Lambda function to AWS. The function to be deployed",
          "a: de in Python with just 3 lines of codes which can be written inline in the template.",
          "c: h parameter of the AWS: :Lambda::Function resource should the developer use to place the Python code in the template?",
          "a: ndler",
          "C: ode",
          "c: orrect",
          "c: reate a Lambda function you first create a Lambda function deployment package, a .zip or .jar file consisting of your code and any",
          "d: ependencies. When creating the zip, include only the code and its dependencies, not the containing folder. You will then need to set the",
          "a: ppropriate security permissions for the zip package.",
          "a: re using a CloudFormation template, you can configure the AWS: :Lambda::Function resource which creates a Lambda function. To",
          "c: reate a function, you need a deployment package and an execution role. The deployment package contains your function code. The execution",
          "a: nts the function permission to use AWS services, such as Amazon CloudWatch Logs for log streaming and AWS X-Ray for request tracing.",
          "a: nagedPolicyArns:",
          "a: rn:aws:iam::aws:policy/AWSLambdaExecute",
          "a: rn:aws:iam::aws:policy/AmazonS3FullAccess"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "CEEEERERRR EE EER\n15. QUESTION E\nCategory: CDA - Deployment\nA developer is writing a CloudFormation template which will be used to deploy a simple Lambda function to AWS. The function to be deployed\nis made in Python with just 3 lines of codes which can be written inline in the template.\nWhich parameter of the AWS: :Lambda::Function resource should the developer use to place the Python code in the template?\nZipFile\nHandler\nCode\nIncorrect\nTo create a Lambda function you first create a Lambda function deployment package, a .zip or .jar file consisting of your code and any\ndependencies. When creating the zip, include only the code and its dependencies, not the containing folder. You will then need to set the\nappropriate security permissions for the zip package.\nIf you are using a CloudFormation template, you can configure the AWS: :Lambda::Function resource which creates a Lambda function. To\ncreate a function, you need a deployment package and an execution role. The deployment package contains your function code. The execution\nrole grants the function permission to use AWS services, such as Amazon CloudWatch Logs for log streaming and AWS X-Ray for request tracing.\nManagedPolicyArns:\n— arn:aws:iam::aws:policy/AWSLambdaExecute\n— arn:aws:iam::aws:policy/AmazonS3FullAccess"
      },
      "tags": {
        "services": [
          "Lambda",
          "S3",
          "IAM",
          "X-Ray",
          "CloudFormation",
          "CloudWatch",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "security",
          "deployment",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 103152.png",
      "parsed": {
        "question": "Category: CDA - Troubleshooting and Optimization\nA developer is planning to use the AWS Elastic Beanstalk console to run the AWS X-Ray daemon on the EC2 instances in her application\nenvironment. She will use X-Ray to construct a service map to help identify issues with her application and to provide insight on which\napplication component to optimize. The environment is using a default Elastic Beanstalk instance profile.\nWhich IAM managed policy does Elastic Beanstalk use for the X-Ray daemon to upload data to X-Ray?\n@® AwWsXrayFullAccess\nAWSXRayDaemonWriteAccess\nAWSXrayReadOnlyAccess\nAWSXRayElasticBeanstalkWriteAccess\nIncorrect\nYou can use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. IAM\ncontrols access to the X-Ray service at the API level to enforce permissions uniformly, regardless of which client (console, AWS SDK, AWS CLI)\nyour users employ. To use the X-Ray console to view service maps and segments, you only need read permissions. To enable console access, add\nthe AWSXrayReadOnlyAccess managed policy to your IAM user. For local development and testing, create an IAM user with read and write\npermissions. Generate access keys for the user and store them in the standard AWS SDK location. You can use these credentials with the X-Ray\ndaemon, the AWS CLI, and the AWS SDK.\nAWSXRayDaemonWriteAccess\nAllow the AWS X-Ray Daemon to relay raw trace segments data to the service's API and retrieve sampling data (rules, targets, etc.) to be used by the X-Ray SDK.\npenal ey",
        "options": [
          "C: ategory: CDA - Troubleshooting and Optimization",
          "A: developer is planning to use the AWS Elastic Beanstalk console to run the AWS X-Ray daemon on the EC2 instances in her application",
          "a: y to construct a service map to help identify issues with her application and to provide insight on which",
          "a: pplication component to optimize. The environment is using a default Elastic Beanstalk instance profile.",
          "c: h IAM managed policy does Elastic Beanstalk use for the X-Ray daemon to upload data to X-Ray?",
          "A: wWsXrayFullAccess",
          "A: WSXRayDaemonWriteAccess",
          "A: WSXrayReadOnlyAccess",
          "A: WSXRayElasticBeanstalkWriteAccess",
          "c: orrect",
          "c: an use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. IAM",
          "c: ontrols access to the X-Ray service at the API level to enforce permissions uniformly, regardless of which client (console, AWS SDK, AWS CLI)",
          "a: y console to view service maps and segments, you only need read permissions. To enable console access, add",
          "A: WSXrayReadOnlyAccess managed policy to your IAM user. For local development and testing, create an IAM user with read and write",
          "a: te access keys for the user and store them in the standard AWS SDK location. You can use these credentials with the X-Ray",
          "d: aemon, the AWS CLI, and the AWS SDK.",
          "A: WSXRayDaemonWriteAccess",
          "A: llow the AWS X-Ray Daemon to relay raw trace segments data to the service's API and retrieve sampling data (rules, targets, etc.) to be used by the X-Ray SDK.",
          "a: l ey"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "20. QUESTION\nCategory: CDA - Troubleshooting and Optimization\nA developer is planning to use the AWS Elastic Beanstalk console to run the AWS X-Ray daemon on the EC2 instances in her application\nenvironment. She will use X-Ray to construct a service map to help identify issues with her application and to provide insight on which\napplication component to optimize. The environment is using a default Elastic Beanstalk instance profile.\nWhich IAM managed policy does Elastic Beanstalk use for the X-Ray daemon to upload data to X-Ray?\n@® AwWsXrayFullAccess\nAWSXRayDaemonWriteAccess\nAWSXrayReadOnlyAccess\nAWSXRayElasticBeanstalkWriteAccess\nIncorrect\nYou can use AWS Identity and Access Management (IAM) to grant X-Ray permissions to users and compute resources in your account. IAM\ncontrols access to the X-Ray service at the API level to enforce permissions uniformly, regardless of which client (console, AWS SDK, AWS CLI)\nyour users employ. To use the X-Ray console to view service maps and segments, you only need read permissions. To enable console access, add\nthe AWSXrayReadOnlyAccess managed policy to your IAM user. For local development and testing, create an IAM user with read and write\npermissions. Generate access keys for the user and store them in the standard AWS SDK location. You can use these credentials with the X-Ray\ndaemon, the AWS CLI, and the AWS SDK.\nAWSXRayDaemonWriteAccess\nAllow the AWS X-Ray Daemon to relay raw trace segments data to the service's API and retrieve sampling data (rules, targets, etc.) to be used by the X-Ray SDK.\npenal ey"
      },
      "tags": {
        "services": [
          "EC2",
          "Elastic Beanstalk",
          "IAM",
          "X-Ray",
          "SAM"
        ],
        "domains": [
          "Security",
          "Deployment",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 103237.png",
      "parsed": {
        "question": "Category: CDA - Security\nA product design firm has adopted a remote work policy and wants to provide employees with access to a suite of CAD software through EC2\nSpot instances. These instances will be deployed using a CloudFormation template. The development team must be able to securely obtain\nsoftware license keys in the template each time it is needed.\nWhich solution meets this requirement while offering the most secure and cost-effective approach?\nStore the license key as a securestring in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic\nreference to retrieve the secret in the CloudFormation template.\n® Store the license key as a secret in AWS Secrets Manager. Use the secretsmanager dynamic reference to retrieve the\nsecret in the CloudFormation template.\nPass the license key in the Parameter section of the CloudFormation template during stack creation. Enable the NoEcho\nattribute on the parameter.\nEmbed the license keys in the Mapping section of the CloudFormation template. Let users choose the correct license key\nusing the Parameter section. Enable the NoEkcho attribute on the parameter.\nIncorrect\nDynamic references provide a compact, powerful way for you to reference external values that are stored and managed in other services, such as\nthe AWS Systems Manager Parameter Store or AWS Secrets Manager. When you use a dynamic reference, CloudFormation retrieves the value of\nthe specified reference when necessary during stack and change set operations and passes the value to the appropriate resource.\nCloudFormation does not store the actual reference value.\nThe following snippet shows how you can use the ssm-secure dynamic reference to retrieve an IAM user’s password from the Parameter Store\nfor console login. IAMUserPassword pertains to the parameter name followed by the version number.",
        "options": [
          "C: ategory: CDA - Security",
          "A: product design firm has adopted a remote work policy and wants to provide employees with access to a suite of CAD software through EC2",
          "a: nces. These instances will be deployed using a CloudFormation template. The development team must be able to securely obtain",
          "a: re license keys in the template each time it is needed.",
          "c: h solution meets this requirement while offering the most secure and cost-effective approach?",
          "c: ense key as a securestring in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic",
          "c: e to retrieve the secret in the CloudFormation template.",
          "c: ense key as a secret in AWS Secrets Manager. Use the secretsmanager dynamic reference to retrieve the",
          "c: ret in the CloudFormation template.",
          "a: ss the license key in the Parameter section of the CloudFormation template during stack creation. Enable the NoEcho",
          "a: ttribute on the parameter.",
          "b: ed the license keys in the Mapping section of the CloudFormation template. Let users choose the correct license key",
          "a: rameter section. Enable the NoEkcho attribute on the parameter.",
          "c: orrect",
          "D: ynamic references provide a compact, powerful way for you to reference external values that are stored and managed in other services, such as",
          "A: WS Systems Manager Parameter Store or AWS Secrets Manager. When you use a dynamic reference, CloudFormation retrieves the value of",
          "c: ified reference when necessary during stack and change set operations and passes the value to the appropriate resource.",
          "C: loudFormation does not store the actual reference value.",
          "c: an use the ssm-secure dynamic reference to retrieve an IAM user’s password from the Parameter Store",
          "c: onsole login. IAMUserPassword pertains to the parameter name followed by the version number."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "21. QUESTION\nCategory: CDA - Security\nA product design firm has adopted a remote work policy and wants to provide employees with access to a suite of CAD software through EC2\nSpot instances. These instances will be deployed using a CloudFormation template. The development team must be able to securely obtain\nsoftware license keys in the template each time it is needed.\nWhich solution meets this requirement while offering the most secure and cost-effective approach?\nStore the license key as a securestring in AWS Systems Manager (SSM) Parameter Store. Use the ssm-secure dynamic\nreference to retrieve the secret in the CloudFormation template.\n® Store the license key as a secret in AWS Secrets Manager. Use the secretsmanager dynamic reference to retrieve the\nsecret in the CloudFormation template.\nPass the license key in the Parameter section of the CloudFormation template during stack creation. Enable the NoEcho\nattribute on the parameter.\nEmbed the license keys in the Mapping section of the CloudFormation template. Let users choose the correct license key\nusing the Parameter section. Enable the NoEkcho attribute on the parameter.\nIncorrect\nDynamic references provide a compact, powerful way for you to reference external values that are stored and managed in other services, such as\nthe AWS Systems Manager Parameter Store or AWS Secrets Manager. When you use a dynamic reference, CloudFormation retrieves the value of\nthe specified reference when necessary during stack and change set operations and passes the value to the appropriate resource.\nCloudFormation does not store the actual reference value.\nThe following snippet shows how you can use the ssm-secure dynamic reference to retrieve an IAM user’s password from the Parameter Store\nfor console login. IAMUserPassword pertains to the parameter name followed by the version number."
      },
      "tags": {
        "services": [
          "EC2",
          "IAM",
          "Secrets Manager",
          "Systems Manager",
          "Parameter Store",
          "CloudFormation",
          "Systems Manager",
          "ECR"
        ],
        "domains": [
          "Security",
          "Deployment"
        ],
        "keywords": [
          "security",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 103500.png",
      "parsed": {
        "question": "K\nCategory: CDA - Development with AWS Services\nA developer has a Node.js function running in AWS Lambda. Currently, the code initializes a database connection to an Amazon RDS database\nevery time the Lambda function is executed, and closes the connection before the function ends.\nWhat feature in AWS Lambda will allow the developer to reuse the already existing database connection instead of initializing it each time the\nfunction is run?\nExecution context\n@® Environment variables\nAWS Lambda is not capable of maintaining existing database connections due to its transient data store.\nEvent source mapping\nIncorrect\nWhen AWS Lambda executes your Lambda function, it provisions and manages the resources needed to run your Lambda function. When you\ncreate a Lambda function, you specify configuration information, such as the amount of memory and maximum execution time that you want to\nallow for your Lambda function. When a Lambda function is invoked, AWS Lambda launches an execution context based on the configuration\nsettings you provide.\nThe execution context is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as\ndatabase connections or HTTP endpoints. This affords subsequent invocations better performance because there is no need to “cold-start” or\ninitialize those external dependencies. After a Lambda function is executed, AWS Lambda maintains the execution context for some time in\nanticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes, and",
        "options": [
          "A: ______ 4 4 Nd",
          "C: ategory: CDA - Development with AWS Services",
          "A: developer has a Node.js function running in AWS Lambda. Currently, the code initializes a database connection to an Amazon RDS database",
          "a: mbda function is executed, and closes the connection before the function ends.",
          "a: t feature in AWS Lambda will allow the developer to reuse the already existing database connection instead of initializing it each time the",
          "c: tion is run?",
          "c: ution context",
          "a: riables",
          "A: WS Lambda is not capable of maintaining existing database connections due to its transient data store.",
          "c: e mapping",
          "c: orrect",
          "A: WS Lambda executes your Lambda function, it provisions and manages the resources needed to run your Lambda function. When you",
          "c: reate a Lambda function, you specify configuration information, such as the amount of memory and maximum execution time that you want to",
          "a: llow for your Lambda function. When a Lambda function is invoked, AWS Lambda launches an execution context based on the configuration",
          "d: e.",
          "c: ution context is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as",
          "d: atabase connections or HTTP endpoints. This affords subsequent invocations better performance because there is no need to “cold-start” or",
          "a: lize those external dependencies. After a Lambda function is executed, AWS Lambda maintains the execution context for some time in",
          "a: nticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes, and"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "A ______ 4 4 Nd\n28. QUESTION K\nCategory: CDA - Development with AWS Services\nA developer has a Node.js function running in AWS Lambda. Currently, the code initializes a database connection to an Amazon RDS database\nevery time the Lambda function is executed, and closes the connection before the function ends.\nWhat feature in AWS Lambda will allow the developer to reuse the already existing database connection instead of initializing it each time the\nfunction is run?\nExecution context\n@® Environment variables\nAWS Lambda is not capable of maintaining existing database connections due to its transient data store.\nEvent source mapping\nIncorrect\nWhen AWS Lambda executes your Lambda function, it provisions and manages the resources needed to run your Lambda function. When you\ncreate a Lambda function, you specify configuration information, such as the amount of memory and maximum execution time that you want to\nallow for your Lambda function. When a Lambda function is invoked, AWS Lambda launches an execution context based on the configuration\nsettings you provide.\nThe execution context is a temporary runtime environment that initializes any external dependencies of your Lambda function code, such as\ndatabase connections or HTTP endpoints. This affords subsequent invocations better performance because there is no need to “cold-start” or\ninitialize those external dependencies. After a Lambda function is executed, AWS Lambda maintains the execution context for some time in\nanticipation of another Lambda function invocation. In effect, the service freezes the execution context after a Lambda function completes, and"
      },
      "tags": {
        "services": [
          "Lambda",
          "RDS",
          "Config"
        ],
        "domains": [
          "Development with AWS Services",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "performance"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 103533.png",
      "parsed": {
        "question": "i\nCategory: CDA - Development with AWS Services\nYou are planning to launch a Lambda function integrated with API Gateway. It is required to specify how the incoming request data is mapped\nto the integration request and how the resulting integration response data is mapped to the method response.\n\nWhich of the following options is the MOST appropriate method use to meet this requirement?\n\n@® HTTP Proxy integration\n\nLambda custom integration\n\nLambda proxy integration\n\nHTTP custom integration\n\nIncorrect\n\nYou choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the\nintegration endpoint. For a Lambda function, you can have two types of integration:\n\n— Lambda proxy integration\n\n— Lambda custom integration\nIn Lambda proxy integration, the setup is simple. If your API does not require content encoding or caching, you only need to set the integration’s\nHTTP method to POST, the integration endpoint URI to the ARN of the Lambda function invocation action of a specific Lambda function, and the\ncredential to an IAM role with permissions to allow API Gateway to call the Lambda function on your behalf.",
        "options": [
          "C: hi) wd",
          "C: ategory: CDA - Development with AWS Services",
          "a: re planning to launch a Lambda function integrated with API Gateway. It is required to specify how the incoming request data is mapped",
          "a: tion request and how the resulting integration response data is mapped to the method response.",
          "c: h of the following options is the MOST appropriate method use to meet this requirement?",
          "a: tion",
          "a: mbda custom integration",
          "a: mbda proxy integration",
          "c: ustom integration",
          "c: orrect",
          "c: hoose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the",
          "a: tion endpoint. For a Lambda function, you can have two types of integration:",
          "a: mbda proxy integration",
          "a: mbda custom integration",
          "a: mbda proxy integration, the setup is simple. If your API does not require content encoding or caching, you only need to set the integration’s",
          "d: to POST, the integration endpoint URI to the ARN of the Lambda function invocation action of a specific Lambda function, and the",
          "c: redential to an IAM role with permissions to allow API Gateway to call the Lambda function on your behalf."
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "Chi) wd\n30. QUESTION i\nCategory: CDA - Development with AWS Services\nYou are planning to launch a Lambda function integrated with API Gateway. It is required to specify how the incoming request data is mapped\nto the integration request and how the resulting integration response data is mapped to the method response.\n\nWhich of the following options is the MOST appropriate method use to meet this requirement?\n\n@® HTTP Proxy integration\n\nLambda custom integration\n\nLambda proxy integration\n\nHTTP custom integration\n\nIncorrect\n\nYou choose an API integration type according to the types of integration endpoint you work with and how you want data to pass to and from the\nintegration endpoint. For a Lambda function, you can have two types of integration:\n\n— Lambda proxy integration\n\n— Lambda custom integration\nIn Lambda proxy integration, the setup is simple. If your API does not require content encoding or caching, you only need to set the integration’s\nHTTP method to POST, the integration endpoint URI to the ARN of the Lambda function invocation action of a specific Lambda function, and the\ncredential to an IAM role with permissions to allow API Gateway to call the Lambda function on your behalf."
      },
      "tags": {
        "services": [
          "Lambda",
          "API Gateway",
          "IAM"
        ],
        "domains": [
          "Development with AWS Services",
          "Security",
          "Troubleshooting and Optimization"
        ],
        "keywords": [
          "caching",
          "IAM role"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 124328.png",
      "parsed": {
        "question": "|\nCategory: CDA - Security\nA web application is currently using an on-premises Microsoft SQL Server 2019 Enterprise Edition database. Your manager instructed you to\nmigrate the application to Elastic Beanstalk and the database to RDS for SQL Server. For additional security, you must configure your database\nto automatically encrypt the actual data before it is written to storage, and automatically decrypt data when the data is read from storage.\nWhich of the following services will you use to achieve this?\nEnable Transparent Data Encryption (TDE).\nUse Microsoft SQL Server Windows Authentication.\nEnable RDS Encryption.\n® Use IAM DB Authentication.\nIncorrect\nAmazon RDS supports using Transparent Data Encryption (TDE) to encrypt stored data on your DB instances running Microsoft SQL Server. TDE\nautomatically encrypts data before it is written to storage, and automatically decrypts data when the data is read from storage.\nAmazon RDS supports TDE for the following SQL Server versions and editions:\n— SQL Server 2019 Standard and Enterprise Editions\n— SQL Server 2017 Enterprise Edition\n—_cnl Sarver 201R8 Enternrice Edition",
        "options": [
          "A: A 4 aN 4",
          "C: ategory: CDA - Security",
          "A: web application is currently using an on-premises Microsoft SQL Server 2019 Enterprise Edition database. Your manager instructed you to",
          "a: te the application to Elastic Beanstalk and the database to RDS for SQL Server. For additional security, you must configure your database",
          "a: utomatically encrypt the actual data before it is written to storage, and automatically decrypt data when the data is read from storage.",
          "c: h of the following services will you use to achieve this?",
          "a: ble Transparent Data Encryption (TDE).",
          "c: rosoft SQL Server Windows Authentication.",
          "a: ble RDS Encryption.",
          "A: M DB Authentication.",
          "c: orrect",
          "A: mazon RDS supports using Transparent Data Encryption (TDE) to encrypt stored data on your DB instances running Microsoft SQL Server. TDE",
          "a: utomatically encrypts data before it is written to storage, and automatically decrypts data when the data is read from storage.",
          "A: mazon RDS supports TDE for the following SQL Server versions and editions:",
          "a: ndard and Enterprise Editions",
          "d: ition",
          "c: nl Sarver 201R8 Enternrice Edition"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "AA 4 aN 4\n31. QUESTION |\nCategory: CDA - Security\nA web application is currently using an on-premises Microsoft SQL Server 2019 Enterprise Edition database. Your manager instructed you to\nmigrate the application to Elastic Beanstalk and the database to RDS for SQL Server. For additional security, you must configure your database\nto automatically encrypt the actual data before it is written to storage, and automatically decrypt data when the data is read from storage.\nWhich of the following services will you use to achieve this?\nEnable Transparent Data Encryption (TDE).\nUse Microsoft SQL Server Windows Authentication.\nEnable RDS Encryption.\n® Use IAM DB Authentication.\nIncorrect\nAmazon RDS supports using Transparent Data Encryption (TDE) to encrypt stored data on your DB instances running Microsoft SQL Server. TDE\nautomatically encrypts data before it is written to storage, and automatically decrypts data when the data is read from storage.\nAmazon RDS supports TDE for the following SQL Server versions and editions:\n— SQL Server 2019 Standard and Enterprise Editions\n— SQL Server 2017 Enterprise Edition\n—_cnl Sarver 201R8 Enternrice Edition"
      },
      "tags": {
        "services": [
          "Elastic Beanstalk",
          "RDS",
          "IAM",
          "Config",
          "ECR"
        ],
        "domains": [
          "Security",
          "Deployment"
        ],
        "keywords": [
          "security",
          "encryption",
          "authentication"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 124450.png",
      "parsed": {
        "question": "Category: CDA - Security\n\nA developer is managing an application hosted in EC2, which stores data in an S3 bucket. The application also uses HTTPS for secure B\n\ncommunication. To comply with the new security policy, the developer must ensure that the data is encrypted at rest using an encryption key\n\nthat is provided and managed by the company. The change should also provide AES-256 encryption to their data.\n\nWhich of the following actions could the developer take to achieve this? (Select TWO.)\nImplement Amazon S3 server-side encryption with customer-provided keys (SSE-C).\n\nUse SSL to encrypt the data while in transit to Amazon S3.\nImplement Amazon S3 server-side encryption with Amazon S3-Managed Encryption Keys.\nImplement Amazon S3 server-side encryption with AWS KMS Keys (SSE-KMS).\nEncrypt the data on the client-side before sending to Amazon S3 using their own master key.\nIncorrect\nData protection refers to protecting data while in transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3\ndata centers). You can protect data in transit by using SSL or by using client-side encryption.\nYou have the following options for protecting data at rest in Amazon S3:\nUse Server-Side Encryption — You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you\ndownload the objects.\n1. Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)\n2. Use Server-Side Encryption with AWS KMS Keys (SSE-KMS)\n| 2 llea Carvar-Cida Enmrruntinm with C1ietarmear-Dravided Kave (CCE_C)",
        "options": [
          "C: ategory: CDA - Security",
          "A: developer is managing an application hosted in EC2, which stores data in an S3 bucket. The application also uses HTTPS for secure B",
          "c: ommunication. To comply with the new security policy, the developer must ensure that the data is encrypted at rest using an encryption key",
          "a: t is provided and managed by the company. The change should also provide AES-256 encryption to their data.",
          "c: h of the following actions could the developer take to achieve this? (Select TWO.)",
          "A: mazon S3 server-side encryption with customer-provided keys (SSE-C).",
          "c: rypt the data while in transit to Amazon S3.",
          "A: mazon S3 server-side encryption with Amazon S3-Managed Encryption Keys.",
          "A: mazon S3 server-side encryption with AWS KMS Keys (SSE-KMS).",
          "c: rypt the data on the client-side before sending to Amazon S3 using their own master key.",
          "c: orrect",
          "D: ata protection refers to protecting data while in transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3",
          "d: ata centers). You can protect data in transit by using SSL or by using client-side encryption.",
          "a: ve the following options for protecting data at rest in Amazon S3:",
          "d: e Encryption — You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you",
          "d: ownload the objects.",
          "d: e Encryption with Amazon S3-Managed Keys (SSE-S3)",
          "d: e Encryption with AWS KMS Keys (SSE-KMS)",
          "a: Carvar-Cida Enmrruntinm with C1ietarmear-Dravided Kave (CCE_C)"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "33. QUESTION\n\nCategory: CDA - Security\n\nA developer is managing an application hosted in EC2, which stores data in an S3 bucket. The application also uses HTTPS for secure B\n\ncommunication. To comply with the new security policy, the developer must ensure that the data is encrypted at rest using an encryption key\n\nthat is provided and managed by the company. The change should also provide AES-256 encryption to their data.\n\nWhich of the following actions could the developer take to achieve this? (Select TWO.)\nImplement Amazon S3 server-side encryption with customer-provided keys (SSE-C).\n\nUse SSL to encrypt the data while in transit to Amazon S3.\nImplement Amazon S3 server-side encryption with Amazon S3-Managed Encryption Keys.\nImplement Amazon S3 server-side encryption with AWS KMS Keys (SSE-KMS).\nEncrypt the data on the client-side before sending to Amazon S3 using their own master key.\nIncorrect\nData protection refers to protecting data while in transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3\ndata centers). You can protect data in transit by using SSL or by using client-side encryption.\nYou have the following options for protecting data at rest in Amazon S3:\nUse Server-Side Encryption — You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you\ndownload the objects.\n1. Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)\n2. Use Server-Side Encryption with AWS KMS Keys (SSE-KMS)\n| 2 llea Carvar-Cida Enmrruntinm with C1ietarmear-Dravided Kave (CCE_C)"
      },
      "tags": {
        "services": [
          "EC2",
          "S3",
          "KMS",
          "ECR"
        ],
        "domains": [
          "Development with AWS Services",
          "Security"
        ],
        "keywords": [
          "security",
          "encryption",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 124509.png",
      "parsed": {
        "question": "uest Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you\ndownload the objects.\n1. Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)\n2. Use Server-Side Encryption with AWS KMS Keys (SSE-KMS)\n3. Use Server-Side Encryption with Customer-Provided Keys (SSE-C)\nUse Client-Side Encryption — You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the\nencryption process, the encryption keys, and related tools.\n1. Use Client-Side Encryption with AWS KMS Key\n2. Use Client-Side Encryption Using a Client-Side Master Key\nlg D. Configure CloudFront to deliver content over HTTPS\nwr using the custom domain name with an SSL certificate.\n‘Amazon CloudFront (a content delivery network)\nc Configure origin access identity to prevent Amazon S3 objects from\nbeing directly accessed publicly from an Amazon S3 URL.\nB.\nConfigure access permissions to the bucket with a bucket policy.\nAmazon $3 bucket\n; Server-side encryption via:\nEncrypted objects 1. Amazon S3-managed keys (SSE-53)\nA. 2. AWS KMS-managed keys (SSE-KMS)\n3. Customer-provided keys (SSE-C)\n£N AN Vo | Client-side encrvption via:",
        "options": [
          "c: orrect +",
          "D: ata protection refers to protecting data while in transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3",
          "d: ata centers). You can protect data in transit by using SSL or by using client-side encryption.",
          "a: ve the following options for protecting data at rest in Amazon S3:",
          "d: e Encryption — You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you",
          "d: ownload the objects.",
          "d: e Encryption with Amazon S3-Managed Keys (SSE-S3)",
          "d: e Encryption with AWS KMS Keys (SSE-KMS)",
          "d: e Encryption with Customer-Provided Keys (SSE-C)",
          "C: lient-Side Encryption — You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the",
          "c: ryption process, the encryption keys, and related tools.",
          "C: lient-Side Encryption with AWS KMS Key",
          "C: lient-Side Encryption Using a Client-Side Master Key",
          "D: Configure CloudFront to deliver content over HTTPS",
          "c: ustom domain name with an SSL certificate.",
          "A: mazon CloudFront (a content delivery network)",
          "c: Configure origin access identity to prevent Amazon S3 objects from",
          "b: eing directly accessed publicly from an Amazon S3 URL.",
          "B: Configure access permissions to the bucket with a bucket policy.",
          "A: mazon $3 bucket",
          "d: e encryption via:",
          "c: rypted objects 1. Amazon S3-managed keys (SSE-53)",
          "A: 2. AWS KMS-managed keys (SSE-KMS)",
          "C: ustomer-provided keys (SSE-C)",
          "A: N Vo | Client-side encrvption via:"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "Incorrect +\nData protection refers to protecting data while in transit (as it travels to and from Amazon S3) and at rest (while it is stored on disks in Amazon S3\ndata centers). You can protect data in transit by using SSL or by using client-side encryption.\nYou have the following options for protecting data at rest in Amazon S3:\nUse Server-Side Encryption — You request Amazon S3 to encrypt your object before saving it on disks in its data centers and decrypt it when you\ndownload the objects.\n1. Use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3)\n2. Use Server-Side Encryption with AWS KMS Keys (SSE-KMS)\n3. Use Server-Side Encryption with Customer-Provided Keys (SSE-C)\nUse Client-Side Encryption — You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the\nencryption process, the encryption keys, and related tools.\n1. Use Client-Side Encryption with AWS KMS Key\n2. Use Client-Side Encryption Using a Client-Side Master Key\nlg D. Configure CloudFront to deliver content over HTTPS\nwr using the custom domain name with an SSL certificate.\n‘Amazon CloudFront (a content delivery network)\nc Configure origin access identity to prevent Amazon S3 objects from\nbeing directly accessed publicly from an Amazon S3 URL.\nB.\nConfigure access permissions to the bucket with a bucket policy.\nAmazon $3 bucket\n; Server-side encryption via:\nEncrypted objects 1. Amazon S3-managed keys (SSE-53)\nA. 2. AWS KMS-managed keys (SSE-KMS)\n3. Customer-provided keys (SSE-C)\n£N AN Vo | Client-side encrvption via:"
      },
      "tags": {
        "services": [
          "S3",
          "CloudFront",
          "KMS",
          "Config",
          "ECR"
        ],
        "domains": [
          "Development with AWS Services",
          "Security"
        ],
        "keywords": [
          "encryption",
          "policy"
        ]
      },
      "isCorrect": true
    },
    {
      "file": "Screenshot 2026-01-17 124647.png",
      "parsed": {
        "question": "uirement is to only secure the data at rest and not data\n\nin transit. Hence, you have to use server-side encryption instead. Moreover, the scenario explicitly states that the application already uses HTTPS\n\nfor secure communication.\n\nImplementing Amazon S3 server-side encryption with AWS KMS Keys (SSE-KMS) is incorrect. Although you can upload the company’s KMS\n\nkeys (CMKs), the keys will be managed by KMS and not your company. This does not comply with the security policy mandated by the company.\n\nImplementing Amazon S3 server-side encryption with Amazon S3-Managed Encryption Keys is incorrect because the Amazon S3-Managed\n\nencryption does not comply with the policy mentioned in the given scenario since the keys are managed by AWS (through Amazon S3) and not by\n\nthe company. The suitable server-side encryption that you should use here is SSE-C.\n\nReferences:\n\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html\n\nCheck out this Amazon S3 Cheat Sheet:\n\nhttps://tutorialsdojo.com/amazon-s3/",
        "options": [
          "C: ARES MVS TENE EM EYE",
          "C: ustomer-supplied client-side master keys",
          "c: e, the valid actions that the developer can implement in this scenario are: B",
          "A: mazon S3 server-side encryption with customer-provided keys (SSE-C)",
          "c: rypt the data on the client-side before sending to Amazon S3 using their own master key.",
          "c: rypt the data while in transit to Amazon S3 is incorrect because the requirement is to only secure the data at rest and not data",
          "a: nsit. Hence, you have to use server-side encryption instead. Moreover, the scenario explicitly states that the application already uses HTTPS",
          "c: ure communication.",
          "A: mazon S3 server-side encryption with AWS KMS Keys (SSE-KMS) is incorrect. Although you can upload the company’s KMS",
          "C: MKs), the keys will be managed by KMS and not your company. This does not comply with the security policy mandated by the company.",
          "A: mazon S3 server-side encryption with Amazon S3-Managed Encryption Keys is incorrect because the Amazon S3-Managed",
          "c: ryption does not comply with the policy mentioned in the given scenario since the keys are managed by AWS (through Amazon S3) and not by",
          "c: ompany. The suitable server-side encryption that you should use here is SSE-C.",
          "c: es:",
          "d: ocs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html",
          "d: ocs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html",
          "C: heck out this Amazon S3 Cheat Sheet:",
          "a: lsdojo.com/amazon-s3/"
        ],
        "yourAnswer": "",
        "correctAnswer": "",
        "explanation": "",
        "rawText": "Ee EE he CARES MVS TENE EM EYE\n\\ ) 2. Customer-supplied client-side master keys\n\nHence, the valid actions that the developer can implement in this scenario are: B\n- Implement Amazon S3 server-side encryption with customer-provided keys (SSE-C)\n- Encrypt the data on the client-side before sending to Amazon S3 using their own master key.\n\nUsing SSL to encrypt the data while in transit to Amazon S3 is incorrect because the requirement is to only secure the data at rest and not data\n\nin transit. Hence, you have to use server-side encryption instead. Moreover, the scenario explicitly states that the application already uses HTTPS\n\nfor secure communication.\n\nImplementing Amazon S3 server-side encryption with AWS KMS Keys (SSE-KMS) is incorrect. Although you can upload the company’s KMS\n\nkeys (CMKs), the keys will be managed by KMS and not your company. This does not comply with the security policy mandated by the company.\n\nImplementing Amazon S3 server-side encryption with Amazon S3-Managed Encryption Keys is incorrect because the Amazon S3-Managed\n\nencryption does not comply with the policy mentioned in the given scenario since the keys are managed by AWS (through Amazon S3) and not by\n\nthe company. The suitable server-side encryption that you should use here is SSE-C.\n\nReferences:\n\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html\n\nhttps://docs.aws.amazon.com/AmazonS3/latest/dev/DataDurability.html\n\nCheck out this Amazon S3 Cheat Sheet:\n\nhttps://tutorialsdojo.com/amazon-s3/"
      },
      "tags": {
        "services": [
          "S3",
          "KMS"
        ],
        "domains": [
          "Development with AWS Services",
          "Security"
        ],
        "keywords": [
          "security",
          "encryption",
          "policy"
        ]
      },
      "isCorrect": true
    }
  ],
  "summary": {
    "totalQuestions": 44,
    "correctCount": 44,
    "incorrectCount": 0,
    "accuracy": 100,
    "serviceBreakdown": {
      "EC2": {
        "total": 12,
        "correct": 12,
        "incorrect": 0,
        "accuracy": 100
      },
      "Systems Manager": {
        "total": 4,
        "correct": 4,
        "incorrect": 0,
        "accuracy": 100
      },
      "Parameter Store": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudFormation": {
        "total": 6,
        "correct": 6,
        "incorrect": 0,
        "accuracy": 100
      },
      "Config": {
        "total": 20,
        "correct": 20,
        "incorrect": 0,
        "accuracy": 100
      },
      "Elastic Beanstalk": {
        "total": 7,
        "correct": 7,
        "incorrect": 0,
        "accuracy": 100
      },
      "DynamoDB": {
        "total": 7,
        "correct": 7,
        "incorrect": 0,
        "accuracy": 100
      },
      "Lambda": {
        "total": 10,
        "correct": 10,
        "incorrect": 0,
        "accuracy": 100
      },
      "SQS": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "SNS": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "SAM": {
        "total": 13,
        "correct": 13,
        "incorrect": 0,
        "accuracy": 100
      },
      "X-Ray": {
        "total": 6,
        "correct": 6,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudWatch": {
        "total": 6,
        "correct": 6,
        "incorrect": 0,
        "accuracy": 100
      },
      "S3": {
        "total": 9,
        "correct": 9,
        "incorrect": 0,
        "accuracy": 100
      },
      "RDS": {
        "total": 7,
        "correct": 7,
        "incorrect": 0,
        "accuracy": 100
      },
      "ECR": {
        "total": 6,
        "correct": 6,
        "incorrect": 0,
        "accuracy": 100
      },
      "IAM": {
        "total": 8,
        "correct": 8,
        "incorrect": 0,
        "accuracy": 100
      },
      "ElastiCache": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "API Gateway": {
        "total": 4,
        "correct": 4,
        "incorrect": 0,
        "accuracy": 100
      },
      "ELB": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "KMS": {
        "total": 6,
        "correct": 6,
        "incorrect": 0,
        "accuracy": 100
      },
      "EBS": {
        "total": 3,
        "correct": 3,
        "incorrect": 0,
        "accuracy": 100
      },
      "VPC": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "CloudFront": {
        "total": 3,
        "correct": 3,
        "incorrect": 0,
        "accuracy": 100
      },
      "Route 53": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "ECS": {
        "total": 5,
        "correct": 5,
        "incorrect": 0,
        "accuracy": 100
      },
      "CodeDeploy": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "Kinesis": {
        "total": 2,
        "correct": 2,
        "incorrect": 0,
        "accuracy": 100
      },
      "Direct Connect": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "Amplify": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "CodePipeline": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "ALB": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "WAF": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      },
      "Secrets Manager": {
        "total": 1,
        "correct": 1,
        "incorrect": 0,
        "accuracy": 100
      }
    },
    "domainBreakdown": {
      "Security": {
        "total": 17,
        "correct": 17,
        "incorrect": 0,
        "accuracy": 100
      },
      "Deployment": {
        "total": 23,
        "correct": 23,
        "incorrect": 0,
        "accuracy": 100
      },
      "Development with AWS Services": {
        "total": 24,
        "correct": 24,
        "incorrect": 0,
        "accuracy": 100
      },
      "Troubleshooting and Optimization": {
        "total": 20,
        "correct": 20,
        "incorrect": 0,
        "accuracy": 100
      }
    },
    "weakAreas": [],
    "strongAreas": [
      {
        "service": "EC2",
        "accuracy": 100,
        "questionsReviewed": 12
      },
      {
        "service": "Systems Manager",
        "accuracy": 100,
        "questionsReviewed": 4
      },
      {
        "service": "Parameter Store",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "CloudFormation",
        "accuracy": 100,
        "questionsReviewed": 6
      },
      {
        "service": "Config",
        "accuracy": 100,
        "questionsReviewed": 20
      },
      {
        "service": "Elastic Beanstalk",
        "accuracy": 100,
        "questionsReviewed": 7
      },
      {
        "service": "DynamoDB",
        "accuracy": 100,
        "questionsReviewed": 7
      },
      {
        "service": "Lambda",
        "accuracy": 100,
        "questionsReviewed": 10
      },
      {
        "service": "SQS",
        "accuracy": 100,
        "questionsReviewed": 2
      },
      {
        "service": "SAM",
        "accuracy": 100,
        "questionsReviewed": 13
      },
      {
        "service": "X-Ray",
        "accuracy": 100,
        "questionsReviewed": 6
      },
      {
        "service": "CloudWatch",
        "accuracy": 100,
        "questionsReviewed": 6
      },
      {
        "service": "S3",
        "accuracy": 100,
        "questionsReviewed": 9
      },
      {
        "service": "RDS",
        "accuracy": 100,
        "questionsReviewed": 7
      },
      {
        "service": "ECR",
        "accuracy": 100,
        "questionsReviewed": 6
      },
      {
        "service": "IAM",
        "accuracy": 100,
        "questionsReviewed": 8
      },
      {
        "service": "API Gateway",
        "accuracy": 100,
        "questionsReviewed": 4
      },
      {
        "service": "KMS",
        "accuracy": 100,
        "questionsReviewed": 6
      },
      {
        "service": "EBS",
        "accuracy": 100,
        "questionsReviewed": 3
      },
      {
        "service": "CloudFront",
        "accuracy": 100,
        "questionsReviewed": 3
      },
      {
        "service": "ECS",
        "accuracy": 100,
        "questionsReviewed": 5
      },
      {
        "service": "Kinesis",
        "accuracy": 100,
        "questionsReviewed": 2
      }
    ],
    "topKeywords": [
      {
        "keyword": "security",
        "count": 14
      },
      {
        "keyword": "policy",
        "count": 10
      },
      {
        "keyword": "deployment",
        "count": 9
      },
      {
        "keyword": "encryption",
        "count": 8
      },
      {
        "keyword": "serverless",
        "count": 7
      },
      {
        "keyword": "performance",
        "count": 5
      },
      {
        "keyword": "authentication",
        "count": 4
      },
      {
        "keyword": "scaling",
        "count": 4
      },
      {
        "keyword": "monitoring",
        "count": 3
      },
      {
        "keyword": "synchronous",
        "count": 2
      },
      {
        "keyword": "queue",
        "count": 2
      },
      {
        "keyword": "IAM role",
        "count": 2
      },
      {
        "keyword": "caching",
        "count": 2
      },
      {
        "keyword": "throttling",
        "count": 2
      },
      {
        "keyword": "container",
        "count": 2
      }
    ],
    "recommendations": [
      "🎯 Excellent performance! You're exam-ready. Focus on scenario-based practice"
    ]
  },
  "heatmap": [
    {
      "service": "EC2",
      "accuracy": 100,
      "total": 12,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Systems Manager",
      "accuracy": 100,
      "total": 4,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Parameter Store",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudFormation",
      "accuracy": 100,
      "total": 6,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Config",
      "accuracy": 100,
      "total": 20,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Elastic Beanstalk",
      "accuracy": 100,
      "total": 7,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "DynamoDB",
      "accuracy": 100,
      "total": 7,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Lambda",
      "accuracy": 100,
      "total": 10,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "SQS",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "SNS",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "SAM",
      "accuracy": 100,
      "total": 13,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "X-Ray",
      "accuracy": 100,
      "total": 6,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudWatch",
      "accuracy": 100,
      "total": 6,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "S3",
      "accuracy": 100,
      "total": 9,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "RDS",
      "accuracy": 100,
      "total": 7,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ECR",
      "accuracy": 100,
      "total": 6,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "IAM",
      "accuracy": 100,
      "total": 8,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ElastiCache",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "API Gateway",
      "accuracy": 100,
      "total": 4,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ELB",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "KMS",
      "accuracy": 100,
      "total": 6,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "EBS",
      "accuracy": 100,
      "total": 3,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "VPC",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CloudFront",
      "accuracy": 100,
      "total": 3,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Route 53",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ECS",
      "accuracy": 100,
      "total": 5,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CodeDeploy",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Kinesis",
      "accuracy": 100,
      "total": 2,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Direct Connect",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Amplify",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "CodePipeline",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "ALB",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "WAF",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    },
    {
      "service": "Secrets Manager",
      "accuracy": 100,
      "total": 1,
      "color": "#22c55e",
      "status": "Strong"
    }
  ],
  "quickStats": {
    "accuracy": "100.0%",
    "correct": 44,
    "incorrect": 0,
    "totalServices": 34,
    "weakAreaCount": 0
  }
}